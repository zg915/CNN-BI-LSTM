{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_preprocessing import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6618, 10, 8]) torch.Size([6618, 1]) torch.Size([490, 10, 8]) (490, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, y_scaler, train_loader, test_loader = get_data(\n",
    "            timestep = 10,\n",
    "            batch_size = 64,\n",
    "            y_name = 'Closing price',\n",
    "            train_size = 6627,\n",
    "            path = \"Data/SSE000001.csv\")\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    print (\"GPU device not found.\")\n",
    "\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer parameters from the table\n",
    "        self.conv1d = nn.Conv1d(in_channels= 8, out_channels= 32, kernel_size= 1, padding=\"same\")\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Pooling\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size= 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layer\n",
    "        x = self.conv1d(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64,  fc1_output_size = 16):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        # self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p = self.lstm_dropout)\n",
    "        # self.fc1 = nn.Linear(self.lstm_hidden_size, self.fc1_output_size)\n",
    "        # self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        tan_h_lstm = self.tanh(h_lstm)\n",
    "        # h_dropout = self.dropout(h_lstm)\n",
    "\n",
    "        # h_fc1 = self.fc1(h_lstm)\n",
    "        # h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        # h_fc2 = self.fc2(h_fc1)\n",
    "        # output = h_fc2[:, -1, :]\n",
    "\n",
    "        h_fc1 = self.fc1(tan_h_lstm)\n",
    "        output = h_fc1[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.lstm = LSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x) #[64, 32, 10]\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1) #[64, 10, 32]\n",
    "        x = self.lstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_LSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.7491185665130615\n",
      "Epoch 0, Batch 10, Loss: 0.7189062833786011\n",
      "Epoch 0, Batch 20, Loss: 0.6353232860565186\n",
      "Epoch 0, Batch 30, Loss: 0.6690065860748291\n",
      "Epoch 0, Batch 40, Loss: 0.6986151337623596\n",
      "Epoch 0, Batch 50, Loss: 0.6975069046020508\n",
      "Epoch 0, Batch 60, Loss: 0.731777548789978\n",
      "Epoch 0, Batch 70, Loss: 0.6237348318099976\n",
      "Epoch 0, Batch 80, Loss: 0.6206910610198975\n",
      "Epoch 0, Batch 90, Loss: 0.621310293674469\n",
      "Epoch 0, Batch 100, Loss: 0.535598874092102\n",
      "Epoch 1, Batch 0, Loss: 0.5707441568374634\n",
      "Epoch 1, Batch 10, Loss: 0.45867636799812317\n",
      "Epoch 1, Batch 20, Loss: 0.36037755012512207\n",
      "Epoch 1, Batch 30, Loss: 0.20988866686820984\n",
      "Epoch 1, Batch 40, Loss: 0.12761858105659485\n",
      "Epoch 1, Batch 50, Loss: 0.14389510452747345\n",
      "Epoch 1, Batch 60, Loss: 0.19735851883888245\n",
      "Epoch 1, Batch 70, Loss: 0.12763115763664246\n",
      "Epoch 1, Batch 80, Loss: 0.11140377074480057\n",
      "Epoch 1, Batch 90, Loss: 0.16423824429512024\n",
      "Epoch 1, Batch 100, Loss: 0.06517411023378372\n",
      "Epoch 2, Batch 0, Loss: 0.11661655455827713\n",
      "Epoch 2, Batch 10, Loss: 0.203014075756073\n",
      "Epoch 2, Batch 20, Loss: 0.12283065170049667\n",
      "Epoch 2, Batch 30, Loss: 0.08804082125425339\n",
      "Epoch 2, Batch 40, Loss: 0.07413022965192795\n",
      "Epoch 2, Batch 50, Loss: 0.11858700215816498\n",
      "Epoch 2, Batch 60, Loss: 0.12451457977294922\n",
      "Epoch 2, Batch 70, Loss: 0.09576191753149033\n",
      "Epoch 2, Batch 80, Loss: 0.056124310940504074\n",
      "Epoch 2, Batch 90, Loss: 0.0823059231042862\n",
      "Epoch 2, Batch 100, Loss: 0.0652439296245575\n",
      "Epoch 3, Batch 0, Loss: 0.07162386178970337\n",
      "Epoch 3, Batch 10, Loss: 0.0663694366812706\n",
      "Epoch 3, Batch 20, Loss: 0.047367919236421585\n",
      "Epoch 3, Batch 30, Loss: 0.1178482174873352\n",
      "Epoch 3, Batch 40, Loss: 0.12141828238964081\n",
      "Epoch 3, Batch 50, Loss: 0.07011788338422775\n",
      "Epoch 3, Batch 60, Loss: 0.08252579718828201\n",
      "Epoch 3, Batch 70, Loss: 0.12316065281629562\n",
      "Epoch 3, Batch 80, Loss: 0.08110266923904419\n",
      "Epoch 3, Batch 90, Loss: 0.07812411338090897\n",
      "Epoch 3, Batch 100, Loss: 0.06110691279172897\n",
      "Epoch 4, Batch 0, Loss: 0.07196086645126343\n",
      "Epoch 4, Batch 10, Loss: 0.04349030181765556\n",
      "Epoch 4, Batch 20, Loss: 0.10416749864816666\n",
      "Epoch 4, Batch 30, Loss: 0.054422467947006226\n",
      "Epoch 4, Batch 40, Loss: 0.1206948310136795\n",
      "Epoch 4, Batch 50, Loss: 0.08441530168056488\n",
      "Epoch 4, Batch 60, Loss: 0.04916823282837868\n",
      "Epoch 4, Batch 70, Loss: 0.06811882555484772\n",
      "Epoch 4, Batch 80, Loss: 0.13093014061450958\n",
      "Epoch 4, Batch 90, Loss: 0.05004943534731865\n",
      "Epoch 4, Batch 100, Loss: 0.06561282277107239\n",
      "Epoch 5, Batch 0, Loss: 0.10038424283266068\n",
      "Epoch 5, Batch 10, Loss: 0.09036725014448166\n",
      "Epoch 5, Batch 20, Loss: 0.06952425092458725\n",
      "Epoch 5, Batch 30, Loss: 0.07481235265731812\n",
      "Epoch 5, Batch 40, Loss: 0.06293556839227676\n",
      "Epoch 5, Batch 50, Loss: 0.076524518430233\n",
      "Epoch 5, Batch 60, Loss: 0.0769612044095993\n",
      "Epoch 5, Batch 70, Loss: 0.036339715123176575\n",
      "Epoch 5, Batch 80, Loss: 0.09022515267133713\n",
      "Epoch 5, Batch 90, Loss: 0.06600672751665115\n",
      "Epoch 5, Batch 100, Loss: 0.0629049688577652\n",
      "Epoch 6, Batch 0, Loss: 0.05317821353673935\n",
      "Epoch 6, Batch 10, Loss: 0.05419613793492317\n",
      "Epoch 6, Batch 20, Loss: 0.07357239723205566\n",
      "Epoch 6, Batch 30, Loss: 0.04223686829209328\n",
      "Epoch 6, Batch 40, Loss: 0.029041307047009468\n",
      "Epoch 6, Batch 50, Loss: 0.048683833330869675\n",
      "Epoch 6, Batch 60, Loss: 0.06264697015285492\n",
      "Epoch 6, Batch 70, Loss: 0.028667090460658073\n",
      "Epoch 6, Batch 80, Loss: 0.043768227100372314\n",
      "Epoch 6, Batch 90, Loss: 0.03236302733421326\n",
      "Epoch 6, Batch 100, Loss: 0.05045730248093605\n",
      "Epoch 7, Batch 0, Loss: 0.06269654631614685\n",
      "Epoch 7, Batch 10, Loss: 0.05221547186374664\n",
      "Epoch 7, Batch 20, Loss: 0.047688957303762436\n",
      "Epoch 7, Batch 30, Loss: 0.04812539368867874\n",
      "Epoch 7, Batch 40, Loss: 0.0339190736413002\n",
      "Epoch 7, Batch 50, Loss: 0.033990368247032166\n",
      "Epoch 7, Batch 60, Loss: 0.024210967123508453\n",
      "Epoch 7, Batch 70, Loss: 0.03113367035984993\n",
      "Epoch 7, Batch 80, Loss: 0.030381955206394196\n",
      "Epoch 7, Batch 90, Loss: 0.03469409421086311\n",
      "Epoch 7, Batch 100, Loss: 0.02743876725435257\n",
      "Epoch 8, Batch 0, Loss: 0.029518738389015198\n",
      "Epoch 8, Batch 10, Loss: 0.051914751529693604\n",
      "Epoch 8, Batch 20, Loss: 0.02490215189754963\n",
      "Epoch 8, Batch 30, Loss: 0.04682856798171997\n",
      "Epoch 8, Batch 40, Loss: 0.054890070110559464\n",
      "Epoch 8, Batch 50, Loss: 0.02790670469403267\n",
      "Epoch 8, Batch 60, Loss: 0.03311902657151222\n",
      "Epoch 8, Batch 70, Loss: 0.042502906173467636\n",
      "Epoch 8, Batch 80, Loss: 0.023286692798137665\n",
      "Epoch 8, Batch 90, Loss: 0.05421644449234009\n",
      "Epoch 8, Batch 100, Loss: 0.02618931047618389\n",
      "Epoch 9, Batch 0, Loss: 0.0391654446721077\n",
      "Epoch 9, Batch 10, Loss: 0.0257624089717865\n",
      "Epoch 9, Batch 20, Loss: 0.041635941714048386\n",
      "Epoch 9, Batch 30, Loss: 0.039117053151130676\n",
      "Epoch 9, Batch 40, Loss: 0.029253996908664703\n",
      "Epoch 9, Batch 50, Loss: 0.02850349061191082\n",
      "Epoch 9, Batch 60, Loss: 0.030205829069018364\n",
      "Epoch 9, Batch 70, Loss: 0.03738902881741524\n",
      "Epoch 9, Batch 80, Loss: 0.02523089572787285\n",
      "Epoch 9, Batch 90, Loss: 0.028890380635857582\n",
      "Epoch 9, Batch 100, Loss: 0.031001005321741104\n",
      "Epoch 10, Batch 0, Loss: 0.03453906252980232\n",
      "Epoch 10, Batch 10, Loss: 0.03132370859384537\n",
      "Epoch 10, Batch 20, Loss: 0.038120292127132416\n",
      "Epoch 10, Batch 30, Loss: 0.02467295154929161\n",
      "Epoch 10, Batch 40, Loss: 0.024594606831669807\n",
      "Epoch 10, Batch 50, Loss: 0.04885510355234146\n",
      "Epoch 10, Batch 60, Loss: 0.03342181816697121\n",
      "Epoch 10, Batch 70, Loss: 0.025783643126487732\n",
      "Epoch 10, Batch 80, Loss: 0.03619951382279396\n",
      "Epoch 10, Batch 90, Loss: 0.031584709882736206\n",
      "Epoch 10, Batch 100, Loss: 0.0353643037378788\n",
      "Epoch 11, Batch 0, Loss: 0.024220068007707596\n",
      "Epoch 11, Batch 10, Loss: 0.025566764175891876\n",
      "Epoch 11, Batch 20, Loss: 0.04034946858882904\n",
      "Epoch 11, Batch 30, Loss: 0.047182027250528336\n",
      "Epoch 11, Batch 40, Loss: 0.03308989480137825\n",
      "Epoch 11, Batch 50, Loss: 0.024867240339517593\n",
      "Epoch 11, Batch 60, Loss: 0.03075704723596573\n",
      "Epoch 11, Batch 70, Loss: 0.026965821161866188\n",
      "Epoch 11, Batch 80, Loss: 0.038293201476335526\n",
      "Epoch 11, Batch 90, Loss: 0.03457828611135483\n",
      "Epoch 11, Batch 100, Loss: 0.025830212980508804\n",
      "Epoch 12, Batch 0, Loss: 0.03497973829507828\n",
      "Epoch 12, Batch 10, Loss: 0.038361430168151855\n",
      "Epoch 12, Batch 20, Loss: 0.037171367555856705\n",
      "Epoch 12, Batch 30, Loss: 0.025622887536883354\n",
      "Epoch 12, Batch 40, Loss: 0.02650386095046997\n",
      "Epoch 12, Batch 50, Loss: 0.028293177485466003\n",
      "Epoch 12, Batch 60, Loss: 0.035078391432762146\n",
      "Epoch 12, Batch 70, Loss: 0.02657645009458065\n",
      "Epoch 12, Batch 80, Loss: 0.02500119060277939\n",
      "Epoch 12, Batch 90, Loss: 0.03182877227663994\n",
      "Epoch 12, Batch 100, Loss: 0.033235687762498856\n",
      "Epoch 13, Batch 0, Loss: 0.02630552090704441\n",
      "Epoch 13, Batch 10, Loss: 0.03716645389795303\n",
      "Epoch 13, Batch 20, Loss: 0.026385284960269928\n",
      "Epoch 13, Batch 30, Loss: 0.03586104139685631\n",
      "Epoch 13, Batch 40, Loss: 0.0274353064596653\n",
      "Epoch 13, Batch 50, Loss: 0.032167959958314896\n",
      "Epoch 13, Batch 60, Loss: 0.022768482565879822\n",
      "Epoch 13, Batch 70, Loss: 0.02551315538585186\n",
      "Epoch 13, Batch 80, Loss: 0.024910371750593185\n",
      "Epoch 13, Batch 90, Loss: 0.03359135985374451\n",
      "Epoch 13, Batch 100, Loss: 0.029317623004317284\n",
      "Epoch 14, Batch 0, Loss: 0.03479815647006035\n",
      "Epoch 14, Batch 10, Loss: 0.03877188265323639\n",
      "Epoch 14, Batch 20, Loss: 0.019020602107048035\n",
      "Epoch 14, Batch 30, Loss: 0.020940516144037247\n",
      "Epoch 14, Batch 40, Loss: 0.033216312527656555\n",
      "Epoch 14, Batch 50, Loss: 0.02438676916062832\n",
      "Epoch 14, Batch 60, Loss: 0.03826817870140076\n",
      "Epoch 14, Batch 70, Loss: 0.02696075290441513\n",
      "Epoch 14, Batch 80, Loss: 0.02219841629266739\n",
      "Epoch 14, Batch 90, Loss: 0.030079763382673264\n",
      "Epoch 14, Batch 100, Loss: 0.027522116899490356\n",
      "Epoch 15, Batch 0, Loss: 0.024317214265465736\n",
      "Epoch 15, Batch 10, Loss: 0.02554578147828579\n",
      "Epoch 15, Batch 20, Loss: 0.03072849102318287\n",
      "Epoch 15, Batch 30, Loss: 0.03277188166975975\n",
      "Epoch 15, Batch 40, Loss: 0.02696426585316658\n",
      "Epoch 15, Batch 50, Loss: 0.037645839154720306\n",
      "Epoch 15, Batch 60, Loss: 0.03601309657096863\n",
      "Epoch 15, Batch 70, Loss: 0.02531566470861435\n",
      "Epoch 15, Batch 80, Loss: 0.03012070804834366\n",
      "Epoch 15, Batch 90, Loss: 0.026239221915602684\n",
      "Epoch 15, Batch 100, Loss: 0.021219192072749138\n",
      "Epoch 16, Batch 0, Loss: 0.02510678768157959\n",
      "Epoch 16, Batch 10, Loss: 0.027993345633149147\n",
      "Epoch 16, Batch 20, Loss: 0.034482523798942566\n",
      "Epoch 16, Batch 30, Loss: 0.027957839891314507\n",
      "Epoch 16, Batch 40, Loss: 0.027293652296066284\n",
      "Epoch 16, Batch 50, Loss: 0.026578059419989586\n",
      "Epoch 16, Batch 60, Loss: 0.034741222858428955\n",
      "Epoch 16, Batch 70, Loss: 0.025437038391828537\n",
      "Epoch 16, Batch 80, Loss: 0.02593277208507061\n",
      "Epoch 16, Batch 90, Loss: 0.02349162846803665\n",
      "Epoch 16, Batch 100, Loss: 0.03664859011769295\n",
      "Epoch 17, Batch 0, Loss: 0.022182878106832504\n",
      "Epoch 17, Batch 10, Loss: 0.02896156534552574\n",
      "Epoch 17, Batch 20, Loss: 0.024568231776356697\n",
      "Epoch 17, Batch 30, Loss: 0.022927366197109222\n",
      "Epoch 17, Batch 40, Loss: 0.02805379219353199\n",
      "Epoch 17, Batch 50, Loss: 0.017168084159493446\n",
      "Epoch 17, Batch 60, Loss: 0.025104690343141556\n",
      "Epoch 17, Batch 70, Loss: 0.03753945976495743\n",
      "Epoch 17, Batch 80, Loss: 0.01840181089937687\n",
      "Epoch 17, Batch 90, Loss: 0.02493908628821373\n",
      "Epoch 17, Batch 100, Loss: 0.017714429646730423\n",
      "Epoch 18, Batch 0, Loss: 0.02915346994996071\n",
      "Epoch 18, Batch 10, Loss: 0.022723769769072533\n",
      "Epoch 18, Batch 20, Loss: 0.02598954364657402\n",
      "Epoch 18, Batch 30, Loss: 0.027384601533412933\n",
      "Epoch 18, Batch 40, Loss: 0.03509524464607239\n",
      "Epoch 18, Batch 50, Loss: 0.022770581766963005\n",
      "Epoch 18, Batch 60, Loss: 0.02462206967175007\n",
      "Epoch 18, Batch 70, Loss: 0.023885762318968773\n",
      "Epoch 18, Batch 80, Loss: 0.022485628724098206\n",
      "Epoch 18, Batch 90, Loss: 0.021447250619530678\n",
      "Epoch 18, Batch 100, Loss: 0.0296788327395916\n",
      "Epoch 19, Batch 0, Loss: 0.02691216766834259\n",
      "Epoch 19, Batch 10, Loss: 0.018241263926029205\n",
      "Epoch 19, Batch 20, Loss: 0.031390637159347534\n",
      "Epoch 19, Batch 30, Loss: 0.023815805092453957\n",
      "Epoch 19, Batch 40, Loss: 0.019067049026489258\n",
      "Epoch 19, Batch 50, Loss: 0.021435197442770004\n",
      "Epoch 19, Batch 60, Loss: 0.03129687160253525\n",
      "Epoch 19, Batch 70, Loss: 0.02518528141081333\n",
      "Epoch 19, Batch 80, Loss: 0.02216866798698902\n",
      "Epoch 19, Batch 90, Loss: 0.028991080820560455\n",
      "Epoch 19, Batch 100, Loss: 0.02092265896499157\n",
      "Epoch 20, Batch 0, Loss: 0.02440745197236538\n",
      "Epoch 20, Batch 10, Loss: 0.01937108486890793\n",
      "Epoch 20, Batch 20, Loss: 0.03124208189547062\n",
      "Epoch 20, Batch 30, Loss: 0.02734193205833435\n",
      "Epoch 20, Batch 40, Loss: 0.024319211021065712\n",
      "Epoch 20, Batch 50, Loss: 0.027132300660014153\n",
      "Epoch 20, Batch 60, Loss: 0.024177078157663345\n",
      "Epoch 20, Batch 70, Loss: 0.02288999781012535\n",
      "Epoch 20, Batch 80, Loss: 0.02417139522731304\n",
      "Epoch 20, Batch 90, Loss: 0.019861450418829918\n",
      "Epoch 20, Batch 100, Loss: 0.02332800067961216\n",
      "Epoch 21, Batch 0, Loss: 0.025872521102428436\n",
      "Epoch 21, Batch 10, Loss: 0.025691619142889977\n",
      "Epoch 21, Batch 20, Loss: 0.023741696029901505\n",
      "Epoch 21, Batch 30, Loss: 0.027030840516090393\n",
      "Epoch 21, Batch 40, Loss: 0.026857223361730576\n",
      "Epoch 21, Batch 50, Loss: 0.02759862318634987\n",
      "Epoch 21, Batch 60, Loss: 0.03619062155485153\n",
      "Epoch 21, Batch 70, Loss: 0.026876375079154968\n",
      "Epoch 21, Batch 80, Loss: 0.027701862156391144\n",
      "Epoch 21, Batch 90, Loss: 0.03005390614271164\n",
      "Epoch 21, Batch 100, Loss: 0.029371678829193115\n",
      "Epoch 22, Batch 0, Loss: 0.02184658870100975\n",
      "Epoch 22, Batch 10, Loss: 0.02242802456021309\n",
      "Epoch 22, Batch 20, Loss: 0.021027445793151855\n",
      "Epoch 22, Batch 30, Loss: 0.03025120683014393\n",
      "Epoch 22, Batch 40, Loss: 0.02932611294090748\n",
      "Epoch 22, Batch 50, Loss: 0.02863881178200245\n",
      "Epoch 22, Batch 60, Loss: 0.02069755271077156\n",
      "Epoch 22, Batch 70, Loss: 0.024641292169690132\n",
      "Epoch 22, Batch 80, Loss: 0.03145093470811844\n",
      "Epoch 22, Batch 90, Loss: 0.03581863269209862\n",
      "Epoch 22, Batch 100, Loss: 0.025061555206775665\n",
      "Epoch 23, Batch 0, Loss: 0.03198624402284622\n",
      "Epoch 23, Batch 10, Loss: 0.02636333368718624\n",
      "Epoch 23, Batch 20, Loss: 0.02081567794084549\n",
      "Epoch 23, Batch 30, Loss: 0.02098935842514038\n",
      "Epoch 23, Batch 40, Loss: 0.025760959833860397\n",
      "Epoch 23, Batch 50, Loss: 0.017251932993531227\n",
      "Epoch 23, Batch 60, Loss: 0.024114230647683144\n",
      "Epoch 23, Batch 70, Loss: 0.02801797352731228\n",
      "Epoch 23, Batch 80, Loss: 0.031080052256584167\n",
      "Epoch 23, Batch 90, Loss: 0.029591718688607216\n",
      "Epoch 23, Batch 100, Loss: 0.04081888124346733\n",
      "Epoch 24, Batch 0, Loss: 0.01971345953643322\n",
      "Epoch 24, Batch 10, Loss: 0.01815895363688469\n",
      "Epoch 24, Batch 20, Loss: 0.030689673498272896\n",
      "Epoch 24, Batch 30, Loss: 0.02731318399310112\n",
      "Epoch 24, Batch 40, Loss: 0.026719041168689728\n",
      "Epoch 24, Batch 50, Loss: 0.023443805053830147\n",
      "Epoch 24, Batch 60, Loss: 0.02508825808763504\n",
      "Epoch 24, Batch 70, Loss: 0.026093589141964912\n",
      "Epoch 24, Batch 80, Loss: 0.03397146612405777\n",
      "Epoch 24, Batch 90, Loss: 0.03425157815217972\n",
      "Epoch 24, Batch 100, Loss: 0.020255371928215027\n",
      "Epoch 25, Batch 0, Loss: 0.022623445838689804\n",
      "Epoch 25, Batch 10, Loss: 0.027448464184999466\n",
      "Epoch 25, Batch 20, Loss: 0.023370448499917984\n",
      "Epoch 25, Batch 30, Loss: 0.022605769336223602\n",
      "Epoch 25, Batch 40, Loss: 0.01959439367055893\n",
      "Epoch 25, Batch 50, Loss: 0.020792748779058456\n",
      "Epoch 25, Batch 60, Loss: 0.022313838824629784\n",
      "Epoch 25, Batch 70, Loss: 0.02583257667720318\n",
      "Epoch 25, Batch 80, Loss: 0.020313872024416924\n",
      "Epoch 25, Batch 90, Loss: 0.029901262372732162\n",
      "Epoch 25, Batch 100, Loss: 0.02230149134993553\n",
      "Epoch 26, Batch 0, Loss: 0.01761879399418831\n",
      "Epoch 26, Batch 10, Loss: 0.038687147200107574\n",
      "Epoch 26, Batch 20, Loss: 0.02237512171268463\n",
      "Epoch 26, Batch 30, Loss: 0.022887732833623886\n",
      "Epoch 26, Batch 40, Loss: 0.022804275155067444\n",
      "Epoch 26, Batch 50, Loss: 0.03067413531243801\n",
      "Epoch 26, Batch 60, Loss: 0.03108367696404457\n",
      "Epoch 26, Batch 70, Loss: 0.021135950461030006\n",
      "Epoch 26, Batch 80, Loss: 0.024288073182106018\n",
      "Epoch 26, Batch 90, Loss: 0.03482167422771454\n",
      "Epoch 26, Batch 100, Loss: 0.023829402402043343\n",
      "Epoch 27, Batch 0, Loss: 0.028327802196145058\n",
      "Epoch 27, Batch 10, Loss: 0.032168909907341\n",
      "Epoch 27, Batch 20, Loss: 0.028083419427275658\n",
      "Epoch 27, Batch 30, Loss: 0.0247005857527256\n",
      "Epoch 27, Batch 40, Loss: 0.022609325125813484\n",
      "Epoch 27, Batch 50, Loss: 0.023346034809947014\n",
      "Epoch 27, Batch 60, Loss: 0.019699856638908386\n",
      "Epoch 27, Batch 70, Loss: 0.020599626004695892\n",
      "Epoch 27, Batch 80, Loss: 0.020322896540164948\n",
      "Epoch 27, Batch 90, Loss: 0.02372794970870018\n",
      "Epoch 27, Batch 100, Loss: 0.02439054474234581\n",
      "Epoch 28, Batch 0, Loss: 0.020072035491466522\n",
      "Epoch 28, Batch 10, Loss: 0.03653542324900627\n",
      "Epoch 28, Batch 20, Loss: 0.021127326413989067\n",
      "Epoch 28, Batch 30, Loss: 0.02429923042654991\n",
      "Epoch 28, Batch 40, Loss: 0.01965533010661602\n",
      "Epoch 28, Batch 50, Loss: 0.03261956572532654\n",
      "Epoch 28, Batch 60, Loss: 0.02886166423559189\n",
      "Epoch 28, Batch 70, Loss: 0.02146085910499096\n",
      "Epoch 28, Batch 80, Loss: 0.027274735271930695\n",
      "Epoch 28, Batch 90, Loss: 0.023435955867171288\n",
      "Epoch 28, Batch 100, Loss: 0.02592279762029648\n",
      "Epoch 29, Batch 0, Loss: 0.0241861455142498\n",
      "Epoch 29, Batch 10, Loss: 0.03024931810796261\n",
      "Epoch 29, Batch 20, Loss: 0.02995838038623333\n",
      "Epoch 29, Batch 30, Loss: 0.02454155497252941\n",
      "Epoch 29, Batch 40, Loss: 0.020464081317186356\n",
      "Epoch 29, Batch 50, Loss: 0.02168084681034088\n",
      "Epoch 29, Batch 60, Loss: 0.023223180323839188\n",
      "Epoch 29, Batch 70, Loss: 0.022863337770104408\n",
      "Epoch 29, Batch 80, Loss: 0.01946962997317314\n",
      "Epoch 29, Batch 90, Loss: 0.02603956311941147\n",
      "Epoch 29, Batch 100, Loss: 0.02223491296172142\n",
      "Epoch 30, Batch 0, Loss: 0.03207205981016159\n",
      "Epoch 30, Batch 10, Loss: 0.019895782694220543\n",
      "Epoch 30, Batch 20, Loss: 0.015512646175920963\n",
      "Epoch 30, Batch 30, Loss: 0.022914472967386246\n",
      "Epoch 30, Batch 40, Loss: 0.027123376727104187\n",
      "Epoch 30, Batch 50, Loss: 0.02770371176302433\n",
      "Epoch 30, Batch 60, Loss: 0.031558744609355927\n",
      "Epoch 30, Batch 70, Loss: 0.02134224772453308\n",
      "Epoch 30, Batch 80, Loss: 0.019958101212978363\n",
      "Epoch 30, Batch 90, Loss: 0.023519592359662056\n",
      "Epoch 30, Batch 100, Loss: 0.031643033027648926\n",
      "Epoch 31, Batch 0, Loss: 0.022254573181271553\n",
      "Epoch 31, Batch 10, Loss: 0.01645488105714321\n",
      "Epoch 31, Batch 20, Loss: 0.033948421478271484\n",
      "Epoch 31, Batch 30, Loss: 0.022117840126156807\n",
      "Epoch 31, Batch 40, Loss: 0.031012725085020065\n",
      "Epoch 31, Batch 50, Loss: 0.02385687083005905\n",
      "Epoch 31, Batch 60, Loss: 0.03142542019486427\n",
      "Epoch 31, Batch 70, Loss: 0.02722635492682457\n",
      "Epoch 31, Batch 80, Loss: 0.029423847794532776\n",
      "Epoch 31, Batch 90, Loss: 0.027380608022212982\n",
      "Epoch 31, Batch 100, Loss: 0.01948566548526287\n",
      "Epoch 32, Batch 0, Loss: 0.025947289541363716\n",
      "Epoch 32, Batch 10, Loss: 0.028836097568273544\n",
      "Epoch 32, Batch 20, Loss: 0.02077142894268036\n",
      "Epoch 32, Batch 30, Loss: 0.024160616099834442\n",
      "Epoch 32, Batch 40, Loss: 0.0340551994740963\n",
      "Epoch 32, Batch 50, Loss: 0.02404872700572014\n",
      "Epoch 32, Batch 60, Loss: 0.022052302956581116\n",
      "Epoch 32, Batch 70, Loss: 0.02402731217443943\n",
      "Epoch 32, Batch 80, Loss: 0.02783597819507122\n",
      "Epoch 32, Batch 90, Loss: 0.02373618632555008\n",
      "Epoch 32, Batch 100, Loss: 0.020157352089881897\n",
      "Epoch 33, Batch 0, Loss: 0.017098836600780487\n",
      "Epoch 33, Batch 10, Loss: 0.028935283422470093\n",
      "Epoch 33, Batch 20, Loss: 0.030102413147687912\n",
      "Epoch 33, Batch 30, Loss: 0.02337849698960781\n",
      "Epoch 33, Batch 40, Loss: 0.035101573914289474\n",
      "Epoch 33, Batch 50, Loss: 0.024203451350331306\n",
      "Epoch 33, Batch 60, Loss: 0.022572804242372513\n",
      "Epoch 33, Batch 70, Loss: 0.021838825196027756\n",
      "Epoch 33, Batch 80, Loss: 0.020506907254457474\n",
      "Epoch 33, Batch 90, Loss: 0.021828720346093178\n",
      "Epoch 33, Batch 100, Loss: 0.022924821823835373\n",
      "Epoch 34, Batch 0, Loss: 0.027660079300403595\n",
      "Epoch 34, Batch 10, Loss: 0.01960396207869053\n",
      "Epoch 34, Batch 20, Loss: 0.024885283783078194\n",
      "Epoch 34, Batch 30, Loss: 0.027884792536497116\n",
      "Epoch 34, Batch 40, Loss: 0.03541151434183121\n",
      "Epoch 34, Batch 50, Loss: 0.02326715737581253\n",
      "Epoch 34, Batch 60, Loss: 0.01874695159494877\n",
      "Epoch 34, Batch 70, Loss: 0.022479120641946793\n",
      "Epoch 34, Batch 80, Loss: 0.024064235389232635\n",
      "Epoch 34, Batch 90, Loss: 0.01927194371819496\n",
      "Epoch 34, Batch 100, Loss: 0.023838618770241737\n",
      "Epoch 35, Batch 0, Loss: 0.024057280272245407\n",
      "Epoch 35, Batch 10, Loss: 0.02410479448735714\n",
      "Epoch 35, Batch 20, Loss: 0.023894675076007843\n",
      "Epoch 35, Batch 30, Loss: 0.02596440538764\n",
      "Epoch 35, Batch 40, Loss: 0.024868138134479523\n",
      "Epoch 35, Batch 50, Loss: 0.02977629192173481\n",
      "Epoch 35, Batch 60, Loss: 0.026028070598840714\n",
      "Epoch 35, Batch 70, Loss: 0.033845894038677216\n",
      "Epoch 35, Batch 80, Loss: 0.026636909693479538\n",
      "Epoch 35, Batch 90, Loss: 0.027326250448822975\n",
      "Epoch 35, Batch 100, Loss: 0.029016928747296333\n",
      "Epoch 36, Batch 0, Loss: 0.021216897293925285\n",
      "Epoch 36, Batch 10, Loss: 0.021756049245595932\n",
      "Epoch 36, Batch 20, Loss: 0.028913257643580437\n",
      "Epoch 36, Batch 30, Loss: 0.017829613760113716\n",
      "Epoch 36, Batch 40, Loss: 0.030297361314296722\n",
      "Epoch 36, Batch 50, Loss: 0.02648274227976799\n",
      "Epoch 36, Batch 60, Loss: 0.025219064205884933\n",
      "Epoch 36, Batch 70, Loss: 0.022842807695269585\n",
      "Epoch 36, Batch 80, Loss: 0.032957419753074646\n",
      "Epoch 36, Batch 90, Loss: 0.02225772850215435\n",
      "Epoch 36, Batch 100, Loss: 0.030163409188389778\n",
      "Epoch 37, Batch 0, Loss: 0.022881394252181053\n",
      "Epoch 37, Batch 10, Loss: 0.02196609228849411\n",
      "Epoch 37, Batch 20, Loss: 0.02337345853447914\n",
      "Epoch 37, Batch 30, Loss: 0.024822033941745758\n",
      "Epoch 37, Batch 40, Loss: 0.021407002583146095\n",
      "Epoch 37, Batch 50, Loss: 0.014936234802007675\n",
      "Epoch 37, Batch 60, Loss: 0.020113013684749603\n",
      "Epoch 37, Batch 70, Loss: 0.02192467823624611\n",
      "Epoch 37, Batch 80, Loss: 0.020846089348196983\n",
      "Epoch 37, Batch 90, Loss: 0.029315076768398285\n",
      "Epoch 37, Batch 100, Loss: 0.019279390573501587\n",
      "Epoch 38, Batch 0, Loss: 0.026702340692281723\n",
      "Epoch 38, Batch 10, Loss: 0.02234066277742386\n",
      "Epoch 38, Batch 20, Loss: 0.02129414863884449\n",
      "Epoch 38, Batch 30, Loss: 0.021994883194565773\n",
      "Epoch 38, Batch 40, Loss: 0.028030354529619217\n",
      "Epoch 38, Batch 50, Loss: 0.02334573306143284\n",
      "Epoch 38, Batch 60, Loss: 0.022107401862740517\n",
      "Epoch 38, Batch 70, Loss: 0.032967232167720795\n",
      "Epoch 38, Batch 80, Loss: 0.027732476592063904\n",
      "Epoch 38, Batch 90, Loss: 0.015603296458721161\n",
      "Epoch 38, Batch 100, Loss: 0.020515691488981247\n",
      "Epoch 39, Batch 0, Loss: 0.021236596629023552\n",
      "Epoch 39, Batch 10, Loss: 0.023247351869940758\n",
      "Epoch 39, Batch 20, Loss: 0.02282470092177391\n",
      "Epoch 39, Batch 30, Loss: 0.022023504599928856\n",
      "Epoch 39, Batch 40, Loss: 0.01886037178337574\n",
      "Epoch 39, Batch 50, Loss: 0.022335242480039597\n",
      "Epoch 39, Batch 60, Loss: 0.02695561572909355\n",
      "Epoch 39, Batch 70, Loss: 0.032783813774585724\n",
      "Epoch 39, Batch 80, Loss: 0.020870236679911613\n",
      "Epoch 39, Batch 90, Loss: 0.027228472754359245\n",
      "Epoch 39, Batch 100, Loss: 0.0240604467689991\n",
      "Epoch 40, Batch 0, Loss: 0.027064992114901543\n",
      "Epoch 40, Batch 10, Loss: 0.022472236305475235\n",
      "Epoch 40, Batch 20, Loss: 0.02550116926431656\n",
      "Epoch 40, Batch 30, Loss: 0.01902606710791588\n",
      "Epoch 40, Batch 40, Loss: 0.023906337097287178\n",
      "Epoch 40, Batch 50, Loss: 0.029511429369449615\n",
      "Epoch 40, Batch 60, Loss: 0.027466105297207832\n",
      "Epoch 40, Batch 70, Loss: 0.030587535351514816\n",
      "Epoch 40, Batch 80, Loss: 0.02414536103606224\n",
      "Epoch 40, Batch 90, Loss: 0.027757063508033752\n",
      "Epoch 40, Batch 100, Loss: 0.02489234134554863\n",
      "Epoch 41, Batch 0, Loss: 0.02285628207027912\n",
      "Epoch 41, Batch 10, Loss: 0.02523568645119667\n",
      "Epoch 41, Batch 20, Loss: 0.029658475890755653\n",
      "Epoch 41, Batch 30, Loss: 0.03086598962545395\n",
      "Epoch 41, Batch 40, Loss: 0.027936717495322227\n",
      "Epoch 41, Batch 50, Loss: 0.02700183540582657\n",
      "Epoch 41, Batch 60, Loss: 0.026949254795908928\n",
      "Epoch 41, Batch 70, Loss: 0.023922866210341454\n",
      "Epoch 41, Batch 80, Loss: 0.025165051221847534\n",
      "Epoch 41, Batch 90, Loss: 0.031143831089138985\n",
      "Epoch 41, Batch 100, Loss: 0.019040094688534737\n",
      "Epoch 42, Batch 0, Loss: 0.01690134033560753\n",
      "Epoch 42, Batch 10, Loss: 0.023944642394781113\n",
      "Epoch 42, Batch 20, Loss: 0.026114877313375473\n",
      "Epoch 42, Batch 30, Loss: 0.020042799413204193\n",
      "Epoch 42, Batch 40, Loss: 0.022363843396306038\n",
      "Epoch 42, Batch 50, Loss: 0.02897469326853752\n",
      "Epoch 42, Batch 60, Loss: 0.03197754546999931\n",
      "Epoch 42, Batch 70, Loss: 0.019348226487636566\n",
      "Epoch 42, Batch 80, Loss: 0.026768771931529045\n",
      "Epoch 42, Batch 90, Loss: 0.019615964964032173\n",
      "Epoch 42, Batch 100, Loss: 0.031199844554066658\n",
      "Epoch 43, Batch 0, Loss: 0.016685668379068375\n",
      "Epoch 43, Batch 10, Loss: 0.01761028729379177\n",
      "Epoch 43, Batch 20, Loss: 0.025376062840223312\n",
      "Epoch 43, Batch 30, Loss: 0.02296634018421173\n",
      "Epoch 43, Batch 40, Loss: 0.020920969545841217\n",
      "Epoch 43, Batch 50, Loss: 0.022864030674099922\n",
      "Epoch 43, Batch 60, Loss: 0.028160322457551956\n",
      "Epoch 43, Batch 70, Loss: 0.023136520758271217\n",
      "Epoch 43, Batch 80, Loss: 0.022111114114522934\n",
      "Epoch 43, Batch 90, Loss: 0.02143329568207264\n",
      "Epoch 43, Batch 100, Loss: 0.018593911081552505\n",
      "Epoch 44, Batch 0, Loss: 0.03571980446577072\n",
      "Epoch 44, Batch 10, Loss: 0.021176576614379883\n",
      "Epoch 44, Batch 20, Loss: 0.01913050189614296\n",
      "Epoch 44, Batch 30, Loss: 0.02890186756849289\n",
      "Epoch 44, Batch 40, Loss: 0.022918086498975754\n",
      "Epoch 44, Batch 50, Loss: 0.02794516831636429\n",
      "Epoch 44, Batch 60, Loss: 0.026440994814038277\n",
      "Epoch 44, Batch 70, Loss: 0.02217303402721882\n",
      "Epoch 44, Batch 80, Loss: 0.025266099721193314\n",
      "Epoch 44, Batch 90, Loss: 0.021007908508181572\n",
      "Epoch 44, Batch 100, Loss: 0.021209031343460083\n",
      "Epoch 45, Batch 0, Loss: 0.02840353175997734\n",
      "Epoch 45, Batch 10, Loss: 0.025340527296066284\n",
      "Epoch 45, Batch 20, Loss: 0.02669188752770424\n",
      "Epoch 45, Batch 30, Loss: 0.02984851412475109\n",
      "Epoch 45, Batch 40, Loss: 0.026803690940141678\n",
      "Epoch 45, Batch 50, Loss: 0.02955452725291252\n",
      "Epoch 45, Batch 60, Loss: 0.020535282790660858\n",
      "Epoch 45, Batch 70, Loss: 0.03161393478512764\n",
      "Epoch 45, Batch 80, Loss: 0.025425197556614876\n",
      "Epoch 45, Batch 90, Loss: 0.022113654762506485\n",
      "Epoch 45, Batch 100, Loss: 0.02750682458281517\n",
      "Epoch 46, Batch 0, Loss: 0.02460591122508049\n",
      "Epoch 46, Batch 10, Loss: 0.028658663854002953\n",
      "Epoch 46, Batch 20, Loss: 0.019827090203762054\n",
      "Epoch 46, Batch 30, Loss: 0.02210797183215618\n",
      "Epoch 46, Batch 40, Loss: 0.02210291661322117\n",
      "Epoch 46, Batch 50, Loss: 0.021732831373810768\n",
      "Epoch 46, Batch 60, Loss: 0.020494766533374786\n",
      "Epoch 46, Batch 70, Loss: 0.021514682099223137\n",
      "Epoch 46, Batch 80, Loss: 0.021133266389369965\n",
      "Epoch 46, Batch 90, Loss: 0.019890442490577698\n",
      "Epoch 46, Batch 100, Loss: 0.03021075576543808\n",
      "Epoch 47, Batch 0, Loss: 0.0231853686273098\n",
      "Epoch 47, Batch 10, Loss: 0.031401604413986206\n",
      "Epoch 47, Batch 20, Loss: 0.02910025231540203\n",
      "Epoch 47, Batch 30, Loss: 0.02394425868988037\n",
      "Epoch 47, Batch 40, Loss: 0.02618066780269146\n",
      "Epoch 47, Batch 50, Loss: 0.021909218281507492\n",
      "Epoch 47, Batch 60, Loss: 0.02133513055741787\n",
      "Epoch 47, Batch 70, Loss: 0.03431018069386482\n",
      "Epoch 47, Batch 80, Loss: 0.023646652698516846\n",
      "Epoch 47, Batch 90, Loss: 0.023610834032297134\n",
      "Epoch 47, Batch 100, Loss: 0.01933848299086094\n",
      "Epoch 48, Batch 0, Loss: 0.021447530016303062\n",
      "Epoch 48, Batch 10, Loss: 0.026673272252082825\n",
      "Epoch 48, Batch 20, Loss: 0.028111224994063377\n",
      "Epoch 48, Batch 30, Loss: 0.021130681037902832\n",
      "Epoch 48, Batch 40, Loss: 0.017718108370900154\n",
      "Epoch 48, Batch 50, Loss: 0.022379813715815544\n",
      "Epoch 48, Batch 60, Loss: 0.0256771519780159\n",
      "Epoch 48, Batch 70, Loss: 0.026440635323524475\n",
      "Epoch 48, Batch 80, Loss: 0.023605523630976677\n",
      "Epoch 48, Batch 90, Loss: 0.03143968805670738\n",
      "Epoch 48, Batch 100, Loss: 0.03037405014038086\n",
      "Epoch 49, Batch 0, Loss: 0.023674041032791138\n",
      "Epoch 49, Batch 10, Loss: 0.026922207325696945\n",
      "Epoch 49, Batch 20, Loss: 0.02920488826930523\n",
      "Epoch 49, Batch 30, Loss: 0.021645594388246536\n",
      "Epoch 49, Batch 40, Loss: 0.022157886996865273\n",
      "Epoch 49, Batch 50, Loss: 0.0328785739839077\n",
      "Epoch 49, Batch 60, Loss: 0.02370111644268036\n",
      "Epoch 49, Batch 70, Loss: 0.033713143318891525\n",
      "Epoch 49, Batch 80, Loss: 0.023307355120778084\n",
      "Epoch 49, Batch 90, Loss: 0.02372605726122856\n",
      "Epoch 49, Batch 100, Loss: 0.025600163266062737\n",
      "Epoch 50, Batch 0, Loss: 0.018000390380620956\n",
      "Epoch 50, Batch 10, Loss: 0.02270537056028843\n",
      "Epoch 50, Batch 20, Loss: 0.029733959585428238\n",
      "Epoch 50, Batch 30, Loss: 0.0299395564943552\n",
      "Epoch 50, Batch 40, Loss: 0.023511862382292747\n",
      "Epoch 50, Batch 50, Loss: 0.021098539233207703\n",
      "Epoch 50, Batch 60, Loss: 0.018195705488324165\n",
      "Epoch 50, Batch 70, Loss: 0.02999579906463623\n",
      "Epoch 50, Batch 80, Loss: 0.027154603973031044\n",
      "Epoch 50, Batch 90, Loss: 0.02350747585296631\n",
      "Epoch 50, Batch 100, Loss: 0.022739142179489136\n",
      "Epoch 51, Batch 0, Loss: 0.028920896351337433\n",
      "Epoch 51, Batch 10, Loss: 0.02485022135078907\n",
      "Epoch 51, Batch 20, Loss: 0.019810235127806664\n",
      "Epoch 51, Batch 30, Loss: 0.026201527565717697\n",
      "Epoch 51, Batch 40, Loss: 0.018431531265378\n",
      "Epoch 51, Batch 50, Loss: 0.02567412704229355\n",
      "Epoch 51, Batch 60, Loss: 0.022663716226816177\n",
      "Epoch 51, Batch 70, Loss: 0.022635959088802338\n",
      "Epoch 51, Batch 80, Loss: 0.017706401646137238\n",
      "Epoch 51, Batch 90, Loss: 0.027736250311136246\n",
      "Epoch 51, Batch 100, Loss: 0.01738208718597889\n",
      "Epoch 52, Batch 0, Loss: 0.02027214877307415\n",
      "Epoch 52, Batch 10, Loss: 0.026468656957149506\n",
      "Epoch 52, Batch 20, Loss: 0.023470299318432808\n",
      "Epoch 52, Batch 30, Loss: 0.018469279631972313\n",
      "Epoch 52, Batch 40, Loss: 0.02634832076728344\n",
      "Epoch 52, Batch 50, Loss: 0.028962304815649986\n",
      "Epoch 52, Batch 60, Loss: 0.02370157279074192\n",
      "Epoch 52, Batch 70, Loss: 0.026917587965726852\n",
      "Epoch 52, Batch 80, Loss: 0.025675280019640923\n",
      "Epoch 52, Batch 90, Loss: 0.021995311602950096\n",
      "Epoch 52, Batch 100, Loss: 0.033551670610904694\n",
      "Epoch 53, Batch 0, Loss: 0.027413304895162582\n",
      "Epoch 53, Batch 10, Loss: 0.016349459066987038\n",
      "Epoch 53, Batch 20, Loss: 0.022782359272241592\n",
      "Epoch 53, Batch 30, Loss: 0.02337866835296154\n",
      "Epoch 53, Batch 40, Loss: 0.021243710070848465\n",
      "Epoch 53, Batch 50, Loss: 0.023013489320874214\n",
      "Epoch 53, Batch 60, Loss: 0.020525313913822174\n",
      "Epoch 53, Batch 70, Loss: 0.020842887461185455\n",
      "Epoch 53, Batch 80, Loss: 0.02583657205104828\n",
      "Epoch 53, Batch 90, Loss: 0.02410932257771492\n",
      "Epoch 53, Batch 100, Loss: 0.021538158878684044\n",
      "Epoch 54, Batch 0, Loss: 0.023537809029221535\n",
      "Epoch 54, Batch 10, Loss: 0.02546972967684269\n",
      "Epoch 54, Batch 20, Loss: 0.023547638207674026\n",
      "Epoch 54, Batch 30, Loss: 0.02218102104961872\n",
      "Epoch 54, Batch 40, Loss: 0.02729819528758526\n",
      "Epoch 54, Batch 50, Loss: 0.027770940214395523\n",
      "Epoch 54, Batch 60, Loss: 0.01684066653251648\n",
      "Epoch 54, Batch 70, Loss: 0.020278962329030037\n",
      "Epoch 54, Batch 80, Loss: 0.02351359836757183\n",
      "Epoch 54, Batch 90, Loss: 0.02346840314567089\n",
      "Epoch 54, Batch 100, Loss: 0.01990179531276226\n",
      "Epoch 55, Batch 0, Loss: 0.020423781126737595\n",
      "Epoch 55, Batch 10, Loss: 0.023463159799575806\n",
      "Epoch 55, Batch 20, Loss: 0.019430365413427353\n",
      "Epoch 55, Batch 30, Loss: 0.0207957960665226\n",
      "Epoch 55, Batch 40, Loss: 0.02805536612868309\n",
      "Epoch 55, Batch 50, Loss: 0.024532705545425415\n",
      "Epoch 55, Batch 60, Loss: 0.02165418118238449\n",
      "Epoch 55, Batch 70, Loss: 0.019603306427598\n",
      "Epoch 55, Batch 80, Loss: 0.03243265300989151\n",
      "Epoch 55, Batch 90, Loss: 0.03853800147771835\n",
      "Epoch 55, Batch 100, Loss: 0.019246257841587067\n",
      "Epoch 56, Batch 0, Loss: 0.021261336281895638\n",
      "Epoch 56, Batch 10, Loss: 0.02603727951645851\n",
      "Epoch 56, Batch 20, Loss: 0.02423679269850254\n",
      "Epoch 56, Batch 30, Loss: 0.026210607960820198\n",
      "Epoch 56, Batch 40, Loss: 0.016333511099219322\n",
      "Epoch 56, Batch 50, Loss: 0.03255828097462654\n",
      "Epoch 56, Batch 60, Loss: 0.02599840611219406\n",
      "Epoch 56, Batch 70, Loss: 0.02526717446744442\n",
      "Epoch 56, Batch 80, Loss: 0.02810540422797203\n",
      "Epoch 56, Batch 90, Loss: 0.03502732142806053\n",
      "Epoch 56, Batch 100, Loss: 0.02350291609764099\n",
      "Epoch 57, Batch 0, Loss: 0.022753018885850906\n",
      "Epoch 57, Batch 10, Loss: 0.016419002786278725\n",
      "Epoch 57, Batch 20, Loss: 0.017839573323726654\n",
      "Epoch 57, Batch 30, Loss: 0.022464638575911522\n",
      "Epoch 57, Batch 40, Loss: 0.024248681962490082\n",
      "Epoch 57, Batch 50, Loss: 0.025738056749105453\n",
      "Epoch 57, Batch 60, Loss: 0.04090316593647003\n",
      "Epoch 57, Batch 70, Loss: 0.020571118220686913\n",
      "Epoch 57, Batch 80, Loss: 0.021258803084492683\n",
      "Epoch 57, Batch 90, Loss: 0.023507285863161087\n",
      "Epoch 57, Batch 100, Loss: 0.027668550610542297\n",
      "Epoch 58, Batch 0, Loss: 0.03049371764063835\n",
      "Epoch 58, Batch 10, Loss: 0.027872305363416672\n",
      "Epoch 58, Batch 20, Loss: 0.021700354292988777\n",
      "Epoch 58, Batch 30, Loss: 0.04267127811908722\n",
      "Epoch 58, Batch 40, Loss: 0.019501926377415657\n",
      "Epoch 58, Batch 50, Loss: 0.016223490238189697\n",
      "Epoch 58, Batch 60, Loss: 0.026191458106040955\n",
      "Epoch 58, Batch 70, Loss: 0.026854917407035828\n",
      "Epoch 58, Batch 80, Loss: 0.031725749373435974\n",
      "Epoch 58, Batch 90, Loss: 0.0314924456179142\n",
      "Epoch 58, Batch 100, Loss: 0.021054666489362717\n",
      "Epoch 59, Batch 0, Loss: 0.019922995939850807\n",
      "Epoch 59, Batch 10, Loss: 0.017603382468223572\n",
      "Epoch 59, Batch 20, Loss: 0.016903705894947052\n",
      "Epoch 59, Batch 30, Loss: 0.025484390556812286\n",
      "Epoch 59, Batch 40, Loss: 0.02360542304813862\n",
      "Epoch 59, Batch 50, Loss: 0.020965125411748886\n",
      "Epoch 59, Batch 60, Loss: 0.020458294078707695\n",
      "Epoch 59, Batch 70, Loss: 0.0314154289662838\n",
      "Epoch 59, Batch 80, Loss: 0.022254714742302895\n",
      "Epoch 59, Batch 90, Loss: 0.02098942920565605\n",
      "Epoch 59, Batch 100, Loss: 0.023616943508386612\n",
      "Epoch 60, Batch 0, Loss: 0.01795848086476326\n",
      "Epoch 60, Batch 10, Loss: 0.015854237601161003\n",
      "Epoch 60, Batch 20, Loss: 0.020600512623786926\n",
      "Epoch 60, Batch 30, Loss: 0.01883835159242153\n",
      "Epoch 60, Batch 40, Loss: 0.02852792479097843\n",
      "Epoch 60, Batch 50, Loss: 0.02481290139257908\n",
      "Epoch 60, Batch 60, Loss: 0.020657218992710114\n",
      "Epoch 60, Batch 70, Loss: 0.021091220900416374\n",
      "Epoch 60, Batch 80, Loss: 0.029100295156240463\n",
      "Epoch 60, Batch 90, Loss: 0.022351853549480438\n",
      "Epoch 60, Batch 100, Loss: 0.022124528884887695\n",
      "Epoch 61, Batch 0, Loss: 0.03183690458536148\n",
      "Epoch 61, Batch 10, Loss: 0.037471525371074677\n",
      "Epoch 61, Batch 20, Loss: 0.028756514191627502\n",
      "Epoch 61, Batch 30, Loss: 0.01858040690422058\n",
      "Epoch 61, Batch 40, Loss: 0.023478558287024498\n",
      "Epoch 61, Batch 50, Loss: 0.025041308254003525\n",
      "Epoch 61, Batch 60, Loss: 0.023372510448098183\n",
      "Epoch 61, Batch 70, Loss: 0.02443326637148857\n",
      "Epoch 61, Batch 80, Loss: 0.023124806582927704\n",
      "Epoch 61, Batch 90, Loss: 0.02556288056075573\n",
      "Epoch 61, Batch 100, Loss: 0.024852804839611053\n",
      "Epoch 62, Batch 0, Loss: 0.015891382470726967\n",
      "Epoch 62, Batch 10, Loss: 0.02291763387620449\n",
      "Epoch 62, Batch 20, Loss: 0.026023678481578827\n",
      "Epoch 62, Batch 30, Loss: 0.020842071622610092\n",
      "Epoch 62, Batch 40, Loss: 0.024685930460691452\n",
      "Epoch 62, Batch 50, Loss: 0.030202068388462067\n",
      "Epoch 62, Batch 60, Loss: 0.01991717517375946\n",
      "Epoch 62, Batch 70, Loss: 0.02676139399409294\n",
      "Epoch 62, Batch 80, Loss: 0.025438666343688965\n",
      "Epoch 62, Batch 90, Loss: 0.02002706006169319\n",
      "Epoch 62, Batch 100, Loss: 0.02128487452864647\n",
      "Epoch 63, Batch 0, Loss: 0.025307470932602882\n",
      "Epoch 63, Batch 10, Loss: 0.02532539889216423\n",
      "Epoch 63, Batch 20, Loss: 0.025334492325782776\n",
      "Epoch 63, Batch 30, Loss: 0.025948217138648033\n",
      "Epoch 63, Batch 40, Loss: 0.028935173526406288\n",
      "Epoch 63, Batch 50, Loss: 0.019982462748885155\n",
      "Epoch 63, Batch 60, Loss: 0.017284579575061798\n",
      "Epoch 63, Batch 70, Loss: 0.0209648460149765\n",
      "Epoch 63, Batch 80, Loss: 0.018897313624620438\n",
      "Epoch 63, Batch 90, Loss: 0.021683469414711\n",
      "Epoch 63, Batch 100, Loss: 0.02344703860580921\n",
      "Epoch 64, Batch 0, Loss: 0.02080930769443512\n",
      "Epoch 64, Batch 10, Loss: 0.02520802430808544\n",
      "Epoch 64, Batch 20, Loss: 0.02643599733710289\n",
      "Epoch 64, Batch 30, Loss: 0.020253339782357216\n",
      "Epoch 64, Batch 40, Loss: 0.032787442207336426\n",
      "Epoch 64, Batch 50, Loss: 0.026842691004276276\n",
      "Epoch 64, Batch 60, Loss: 0.016215186566114426\n",
      "Epoch 64, Batch 70, Loss: 0.017854411154985428\n",
      "Epoch 64, Batch 80, Loss: 0.02840062603354454\n",
      "Epoch 64, Batch 90, Loss: 0.01890813559293747\n",
      "Epoch 64, Batch 100, Loss: 0.018714340403676033\n",
      "Epoch 65, Batch 0, Loss: 0.022677090018987656\n",
      "Epoch 65, Batch 10, Loss: 0.018379898741841316\n",
      "Epoch 65, Batch 20, Loss: 0.03230627253651619\n",
      "Epoch 65, Batch 30, Loss: 0.020225536078214645\n",
      "Epoch 65, Batch 40, Loss: 0.017701812088489532\n",
      "Epoch 65, Batch 50, Loss: 0.020836157724261284\n",
      "Epoch 65, Batch 60, Loss: 0.022280871868133545\n",
      "Epoch 65, Batch 70, Loss: 0.019687335938215256\n",
      "Epoch 65, Batch 80, Loss: 0.029721073806285858\n",
      "Epoch 65, Batch 90, Loss: 0.020586317405104637\n",
      "Epoch 65, Batch 100, Loss: 0.017460448667407036\n",
      "Epoch 66, Batch 0, Loss: 0.020394325256347656\n",
      "Epoch 66, Batch 10, Loss: 0.024358931928873062\n",
      "Epoch 66, Batch 20, Loss: 0.022136474028229713\n",
      "Epoch 66, Batch 30, Loss: 0.02980443276464939\n",
      "Epoch 66, Batch 40, Loss: 0.02662649378180504\n",
      "Epoch 66, Batch 50, Loss: 0.01983540877699852\n",
      "Epoch 66, Batch 60, Loss: 0.021748840808868408\n",
      "Epoch 66, Batch 70, Loss: 0.022216636687517166\n",
      "Epoch 66, Batch 80, Loss: 0.021574443206191063\n",
      "Epoch 66, Batch 90, Loss: 0.027281295508146286\n",
      "Epoch 66, Batch 100, Loss: 0.018513906747102737\n",
      "Epoch 67, Batch 0, Loss: 0.019400227814912796\n",
      "Epoch 67, Batch 10, Loss: 0.021072475239634514\n",
      "Epoch 67, Batch 20, Loss: 0.02277846820652485\n",
      "Epoch 67, Batch 30, Loss: 0.021135911345481873\n",
      "Epoch 67, Batch 40, Loss: 0.029449893161654472\n",
      "Epoch 67, Batch 50, Loss: 0.020480813458561897\n",
      "Epoch 67, Batch 60, Loss: 0.026210201904177666\n",
      "Epoch 67, Batch 70, Loss: 0.024601347744464874\n",
      "Epoch 67, Batch 80, Loss: 0.021002240478992462\n",
      "Epoch 67, Batch 90, Loss: 0.022646095603704453\n",
      "Epoch 67, Batch 100, Loss: 0.021374661475419998\n",
      "Epoch 68, Batch 0, Loss: 0.018172405660152435\n",
      "Epoch 68, Batch 10, Loss: 0.01984255015850067\n",
      "Epoch 68, Batch 20, Loss: 0.02631305903196335\n",
      "Epoch 68, Batch 30, Loss: 0.02206926792860031\n",
      "Epoch 68, Batch 40, Loss: 0.021541280671954155\n",
      "Epoch 68, Batch 50, Loss: 0.020204078406095505\n",
      "Epoch 68, Batch 60, Loss: 0.019476423040032387\n",
      "Epoch 68, Batch 70, Loss: 0.02960675023496151\n",
      "Epoch 68, Batch 80, Loss: 0.024047963321208954\n",
      "Epoch 68, Batch 90, Loss: 0.02503795176744461\n",
      "Epoch 68, Batch 100, Loss: 0.03276098147034645\n",
      "Epoch 69, Batch 0, Loss: 0.021208753809332848\n",
      "Epoch 69, Batch 10, Loss: 0.021352455019950867\n",
      "Epoch 69, Batch 20, Loss: 0.02337072417140007\n",
      "Epoch 69, Batch 30, Loss: 0.026460837572813034\n",
      "Epoch 69, Batch 40, Loss: 0.022662745788693428\n",
      "Epoch 69, Batch 50, Loss: 0.02340730093419552\n",
      "Epoch 69, Batch 60, Loss: 0.018585728481411934\n",
      "Epoch 69, Batch 70, Loss: 0.023717856034636497\n",
      "Epoch 69, Batch 80, Loss: 0.020240675657987595\n",
      "Epoch 69, Batch 90, Loss: 0.01854727976024151\n",
      "Epoch 69, Batch 100, Loss: 0.023572610691189766\n",
      "Epoch 70, Batch 0, Loss: 0.028621502220630646\n",
      "Epoch 70, Batch 10, Loss: 0.01873999834060669\n",
      "Epoch 70, Batch 20, Loss: 0.021375399082899094\n",
      "Epoch 70, Batch 30, Loss: 0.019567066803574562\n",
      "Epoch 70, Batch 40, Loss: 0.018749825656414032\n",
      "Epoch 70, Batch 50, Loss: 0.032672908157110214\n",
      "Epoch 70, Batch 60, Loss: 0.01569240912795067\n",
      "Epoch 70, Batch 70, Loss: 0.021560553461313248\n",
      "Epoch 70, Batch 80, Loss: 0.022301917895674706\n",
      "Epoch 70, Batch 90, Loss: 0.022446680814027786\n",
      "Epoch 70, Batch 100, Loss: 0.019536854699254036\n",
      "Epoch 71, Batch 0, Loss: 0.020459014922380447\n",
      "Epoch 71, Batch 10, Loss: 0.028719265013933182\n",
      "Epoch 71, Batch 20, Loss: 0.022319458425045013\n",
      "Epoch 71, Batch 30, Loss: 0.027482856065034866\n",
      "Epoch 71, Batch 40, Loss: 0.02978067472577095\n",
      "Epoch 71, Batch 50, Loss: 0.025899702683091164\n",
      "Epoch 71, Batch 60, Loss: 0.029171504080295563\n",
      "Epoch 71, Batch 70, Loss: 0.023033564910292625\n",
      "Epoch 71, Batch 80, Loss: 0.0228557251393795\n",
      "Epoch 71, Batch 90, Loss: 0.017943285405635834\n",
      "Epoch 71, Batch 100, Loss: 0.02245902828872204\n",
      "Epoch 72, Batch 0, Loss: 0.01936115138232708\n",
      "Epoch 72, Batch 10, Loss: 0.027300134301185608\n",
      "Epoch 72, Batch 20, Loss: 0.02512665092945099\n",
      "Epoch 72, Batch 30, Loss: 0.029584048315882683\n",
      "Epoch 72, Batch 40, Loss: 0.02293522097170353\n",
      "Epoch 72, Batch 50, Loss: 0.028415178880095482\n",
      "Epoch 72, Batch 60, Loss: 0.03217250481247902\n",
      "Epoch 72, Batch 70, Loss: 0.020441874861717224\n",
      "Epoch 72, Batch 80, Loss: 0.02345435693860054\n",
      "Epoch 72, Batch 90, Loss: 0.02359975501894951\n",
      "Epoch 72, Batch 100, Loss: 0.03267382085323334\n",
      "Epoch 73, Batch 0, Loss: 0.029618993401527405\n",
      "Epoch 73, Batch 10, Loss: 0.019735537469387054\n",
      "Epoch 73, Batch 20, Loss: 0.020471984520554543\n",
      "Epoch 73, Batch 30, Loss: 0.023559853434562683\n",
      "Epoch 73, Batch 40, Loss: 0.036248378455638885\n",
      "Epoch 73, Batch 50, Loss: 0.022293588146567345\n",
      "Epoch 73, Batch 60, Loss: 0.02530847117304802\n",
      "Epoch 73, Batch 70, Loss: 0.024932516738772392\n",
      "Epoch 73, Batch 80, Loss: 0.03417922183871269\n",
      "Epoch 73, Batch 90, Loss: 0.022213032469153404\n",
      "Epoch 73, Batch 100, Loss: 0.01787675730884075\n",
      "Epoch 74, Batch 0, Loss: 0.020970715209841728\n",
      "Epoch 74, Batch 10, Loss: 0.026753142476081848\n",
      "Epoch 74, Batch 20, Loss: 0.02364741824567318\n",
      "Epoch 74, Batch 30, Loss: 0.0288890078663826\n",
      "Epoch 74, Batch 40, Loss: 0.021682029590010643\n",
      "Epoch 74, Batch 50, Loss: 0.02080293744802475\n",
      "Epoch 74, Batch 60, Loss: 0.02934851497411728\n",
      "Epoch 74, Batch 70, Loss: 0.022918954491615295\n",
      "Epoch 74, Batch 80, Loss: 0.026439905166625977\n",
      "Epoch 74, Batch 90, Loss: 0.019949592649936676\n",
      "Epoch 74, Batch 100, Loss: 0.02249804139137268\n",
      "Epoch 75, Batch 0, Loss: 0.025565342977643013\n",
      "Epoch 75, Batch 10, Loss: 0.024864554405212402\n",
      "Epoch 75, Batch 20, Loss: 0.028267692774534225\n",
      "Epoch 75, Batch 30, Loss: 0.025713590905070305\n",
      "Epoch 75, Batch 40, Loss: 0.020733578130602837\n",
      "Epoch 75, Batch 50, Loss: 0.018862120807170868\n",
      "Epoch 75, Batch 60, Loss: 0.015493353828787804\n",
      "Epoch 75, Batch 70, Loss: 0.020283441990613937\n",
      "Epoch 75, Batch 80, Loss: 0.029615070670843124\n",
      "Epoch 75, Batch 90, Loss: 0.030271748080849648\n",
      "Epoch 75, Batch 100, Loss: 0.02502317540347576\n",
      "Epoch 76, Batch 0, Loss: 0.02103683166205883\n",
      "Epoch 76, Batch 10, Loss: 0.01702950894832611\n",
      "Epoch 76, Batch 20, Loss: 0.02767261676490307\n",
      "Epoch 76, Batch 30, Loss: 0.025314534083008766\n",
      "Epoch 76, Batch 40, Loss: 0.019317861646413803\n",
      "Epoch 76, Batch 50, Loss: 0.02123287320137024\n",
      "Epoch 76, Batch 60, Loss: 0.021930117160081863\n",
      "Epoch 76, Batch 70, Loss: 0.023097451776266098\n",
      "Epoch 76, Batch 80, Loss: 0.027033643797039986\n",
      "Epoch 76, Batch 90, Loss: 0.022117288783192635\n",
      "Epoch 76, Batch 100, Loss: 0.025578495115041733\n",
      "Epoch 77, Batch 0, Loss: 0.029556531459093094\n",
      "Epoch 77, Batch 10, Loss: 0.023351550102233887\n",
      "Epoch 77, Batch 20, Loss: 0.028067758306860924\n",
      "Epoch 77, Batch 30, Loss: 0.02321300096809864\n",
      "Epoch 77, Batch 40, Loss: 0.02009626477956772\n",
      "Epoch 77, Batch 50, Loss: 0.017946327105164528\n",
      "Epoch 77, Batch 60, Loss: 0.021900517866015434\n",
      "Epoch 77, Batch 70, Loss: 0.038883596658706665\n",
      "Epoch 77, Batch 80, Loss: 0.022822320461273193\n",
      "Epoch 77, Batch 90, Loss: 0.026392336934804916\n",
      "Epoch 77, Batch 100, Loss: 0.02006051316857338\n",
      "Epoch 78, Batch 0, Loss: 0.025566481053829193\n",
      "Epoch 78, Batch 10, Loss: 0.025589901953935623\n",
      "Epoch 78, Batch 20, Loss: 0.017076699063181877\n",
      "Epoch 78, Batch 30, Loss: 0.026785455644130707\n",
      "Epoch 78, Batch 40, Loss: 0.02200794219970703\n",
      "Epoch 78, Batch 50, Loss: 0.024695098400115967\n",
      "Epoch 78, Batch 60, Loss: 0.028025835752487183\n",
      "Epoch 78, Batch 70, Loss: 0.017391204833984375\n",
      "Epoch 78, Batch 80, Loss: 0.02895316854119301\n",
      "Epoch 78, Batch 90, Loss: 0.031959641724824905\n",
      "Epoch 78, Batch 100, Loss: 0.018368501216173172\n",
      "Epoch 79, Batch 0, Loss: 0.027852678671479225\n",
      "Epoch 79, Batch 10, Loss: 0.021706320345401764\n",
      "Epoch 79, Batch 20, Loss: 0.016800321638584137\n",
      "Epoch 79, Batch 30, Loss: 0.029589826241135597\n",
      "Epoch 79, Batch 40, Loss: 0.031618840992450714\n",
      "Epoch 79, Batch 50, Loss: 0.01638285629451275\n",
      "Epoch 79, Batch 60, Loss: 0.019484858959913254\n",
      "Epoch 79, Batch 70, Loss: 0.023358620703220367\n",
      "Epoch 79, Batch 80, Loss: 0.02461938001215458\n",
      "Epoch 79, Batch 90, Loss: 0.026962555944919586\n",
      "Epoch 79, Batch 100, Loss: 0.0330047532916069\n",
      "Epoch 80, Batch 0, Loss: 0.026566144078969955\n",
      "Epoch 80, Batch 10, Loss: 0.024745263159275055\n",
      "Epoch 80, Batch 20, Loss: 0.022362295538187027\n",
      "Epoch 80, Batch 30, Loss: 0.019488990306854248\n",
      "Epoch 80, Batch 40, Loss: 0.023268822580575943\n",
      "Epoch 80, Batch 50, Loss: 0.01991993933916092\n",
      "Epoch 80, Batch 60, Loss: 0.022730085998773575\n",
      "Epoch 80, Batch 70, Loss: 0.01850811019539833\n",
      "Epoch 80, Batch 80, Loss: 0.02294359914958477\n",
      "Epoch 80, Batch 90, Loss: 0.021562140434980392\n",
      "Epoch 80, Batch 100, Loss: 0.02273375727236271\n",
      "Epoch 81, Batch 0, Loss: 0.02283044531941414\n",
      "Epoch 81, Batch 10, Loss: 0.02212538756430149\n",
      "Epoch 81, Batch 20, Loss: 0.02316652424633503\n",
      "Epoch 81, Batch 30, Loss: 0.02756374515593052\n",
      "Epoch 81, Batch 40, Loss: 0.017954032868146896\n",
      "Epoch 81, Batch 50, Loss: 0.027034064754843712\n",
      "Epoch 81, Batch 60, Loss: 0.024204997345805168\n",
      "Epoch 81, Batch 70, Loss: 0.026422204449772835\n",
      "Epoch 81, Batch 80, Loss: 0.026340872049331665\n",
      "Epoch 81, Batch 90, Loss: 0.02617425099015236\n",
      "Epoch 81, Batch 100, Loss: 0.027932478114962578\n",
      "Epoch 82, Batch 0, Loss: 0.025993840768933296\n",
      "Epoch 82, Batch 10, Loss: 0.03565085679292679\n",
      "Epoch 82, Batch 20, Loss: 0.023397376760840416\n",
      "Epoch 82, Batch 30, Loss: 0.027181459590792656\n",
      "Epoch 82, Batch 40, Loss: 0.019824542105197906\n",
      "Epoch 82, Batch 50, Loss: 0.027117440477013588\n",
      "Epoch 82, Batch 60, Loss: 0.019199447706341743\n",
      "Epoch 82, Batch 70, Loss: 0.0196352731436491\n",
      "Epoch 82, Batch 80, Loss: 0.020986609160900116\n",
      "Epoch 82, Batch 90, Loss: 0.02923191338777542\n",
      "Epoch 82, Batch 100, Loss: 0.018864061683416367\n",
      "Epoch 83, Batch 0, Loss: 0.030625183135271072\n",
      "Epoch 83, Batch 10, Loss: 0.018797798082232475\n",
      "Epoch 83, Batch 20, Loss: 0.02801644429564476\n",
      "Epoch 83, Batch 30, Loss: 0.02644187957048416\n",
      "Epoch 83, Batch 40, Loss: 0.02413000911474228\n",
      "Epoch 83, Batch 50, Loss: 0.028352797031402588\n",
      "Epoch 83, Batch 60, Loss: 0.02249850705265999\n",
      "Epoch 83, Batch 70, Loss: 0.025252047926187515\n",
      "Epoch 83, Batch 80, Loss: 0.018789511173963547\n",
      "Epoch 83, Batch 90, Loss: 0.022542618215084076\n",
      "Epoch 83, Batch 100, Loss: 0.020407240837812424\n",
      "Epoch 84, Batch 0, Loss: 0.021912438794970512\n",
      "Epoch 84, Batch 10, Loss: 0.021087830886244774\n",
      "Epoch 84, Batch 20, Loss: 0.023711420595645905\n",
      "Epoch 84, Batch 30, Loss: 0.019176391884684563\n",
      "Epoch 84, Batch 40, Loss: 0.02363898605108261\n",
      "Epoch 84, Batch 50, Loss: 0.030177265405654907\n",
      "Epoch 84, Batch 60, Loss: 0.017582381144165993\n",
      "Epoch 84, Batch 70, Loss: 0.023681536316871643\n",
      "Epoch 84, Batch 80, Loss: 0.023139219731092453\n",
      "Epoch 84, Batch 90, Loss: 0.023287368938326836\n",
      "Epoch 84, Batch 100, Loss: 0.02294241078197956\n",
      "Epoch 85, Batch 0, Loss: 0.01808260940015316\n",
      "Epoch 85, Batch 10, Loss: 0.018893437460064888\n",
      "Epoch 85, Batch 20, Loss: 0.020215235650539398\n",
      "Epoch 85, Batch 30, Loss: 0.023829862475395203\n",
      "Epoch 85, Batch 40, Loss: 0.030722947791218758\n",
      "Epoch 85, Batch 50, Loss: 0.02575768530368805\n",
      "Epoch 85, Batch 60, Loss: 0.023476406931877136\n",
      "Epoch 85, Batch 70, Loss: 0.023880554363131523\n",
      "Epoch 85, Batch 80, Loss: 0.029731348156929016\n",
      "Epoch 85, Batch 90, Loss: 0.017585396766662598\n",
      "Epoch 85, Batch 100, Loss: 0.022944465279579163\n",
      "Epoch 86, Batch 0, Loss: 0.022576652467250824\n",
      "Epoch 86, Batch 10, Loss: 0.016821762546896935\n",
      "Epoch 86, Batch 20, Loss: 0.021159956231713295\n",
      "Epoch 86, Batch 30, Loss: 0.024494929239153862\n",
      "Epoch 86, Batch 40, Loss: 0.017453238368034363\n",
      "Epoch 86, Batch 50, Loss: 0.02585681900382042\n",
      "Epoch 86, Batch 60, Loss: 0.025156350806355476\n",
      "Epoch 86, Batch 70, Loss: 0.024154987186193466\n",
      "Epoch 86, Batch 80, Loss: 0.03029654175043106\n",
      "Epoch 86, Batch 90, Loss: 0.020029932260513306\n",
      "Epoch 86, Batch 100, Loss: 0.022874489426612854\n",
      "Epoch 87, Batch 0, Loss: 0.034879520535469055\n",
      "Epoch 87, Batch 10, Loss: 0.019665604457259178\n",
      "Epoch 87, Batch 20, Loss: 0.017592869699001312\n",
      "Epoch 87, Batch 30, Loss: 0.023823050782084465\n",
      "Epoch 87, Batch 40, Loss: 0.024316132068634033\n",
      "Epoch 87, Batch 50, Loss: 0.022094931453466415\n",
      "Epoch 87, Batch 60, Loss: 0.021405816078186035\n",
      "Epoch 87, Batch 70, Loss: 0.020942533388733864\n",
      "Epoch 87, Batch 80, Loss: 0.025729086250066757\n",
      "Epoch 87, Batch 90, Loss: 0.021266838535666466\n",
      "Epoch 87, Batch 100, Loss: 0.024785423651337624\n",
      "Epoch 88, Batch 0, Loss: 0.022711699828505516\n",
      "Epoch 88, Batch 10, Loss: 0.021005799993872643\n",
      "Epoch 88, Batch 20, Loss: 0.020564628764986992\n",
      "Epoch 88, Batch 30, Loss: 0.029109971597790718\n",
      "Epoch 88, Batch 40, Loss: 0.025900395587086678\n",
      "Epoch 88, Batch 50, Loss: 0.02231329120695591\n",
      "Epoch 88, Batch 60, Loss: 0.018919331952929497\n",
      "Epoch 88, Batch 70, Loss: 0.017753416672348976\n",
      "Epoch 88, Batch 80, Loss: 0.022020533680915833\n",
      "Epoch 88, Batch 90, Loss: 0.020003851503133774\n",
      "Epoch 88, Batch 100, Loss: 0.019073696807026863\n",
      "Epoch 89, Batch 0, Loss: 0.02154330350458622\n",
      "Epoch 89, Batch 10, Loss: 0.026421042159199715\n",
      "Epoch 89, Batch 20, Loss: 0.024187803268432617\n",
      "Epoch 89, Batch 30, Loss: 0.02361818216741085\n",
      "Epoch 89, Batch 40, Loss: 0.026424720883369446\n",
      "Epoch 89, Batch 50, Loss: 0.02273235283792019\n",
      "Epoch 89, Batch 60, Loss: 0.019884593784809113\n",
      "Epoch 89, Batch 70, Loss: 0.022025998681783676\n",
      "Epoch 89, Batch 80, Loss: 0.022136377170681953\n",
      "Epoch 89, Batch 90, Loss: 0.01954108290374279\n",
      "Epoch 89, Batch 100, Loss: 0.017835967242717743\n",
      "Epoch 90, Batch 0, Loss: 0.02442050166428089\n",
      "Epoch 90, Batch 10, Loss: 0.021540911868214607\n",
      "Epoch 90, Batch 20, Loss: 0.020202776417136192\n",
      "Epoch 90, Batch 30, Loss: 0.030222905799746513\n",
      "Epoch 90, Batch 40, Loss: 0.024510376155376434\n",
      "Epoch 90, Batch 50, Loss: 0.022627435624599457\n",
      "Epoch 90, Batch 60, Loss: 0.0303837601095438\n",
      "Epoch 90, Batch 70, Loss: 0.024436404928565025\n",
      "Epoch 90, Batch 80, Loss: 0.022818729281425476\n",
      "Epoch 90, Batch 90, Loss: 0.024605078622698784\n",
      "Epoch 90, Batch 100, Loss: 0.019702069461345673\n",
      "Epoch 91, Batch 0, Loss: 0.02013956755399704\n",
      "Epoch 91, Batch 10, Loss: 0.025177158415317535\n",
      "Epoch 91, Batch 20, Loss: 0.02698579803109169\n",
      "Epoch 91, Batch 30, Loss: 0.026568925008177757\n",
      "Epoch 91, Batch 40, Loss: 0.016790444031357765\n",
      "Epoch 91, Batch 50, Loss: 0.033129073679447174\n",
      "Epoch 91, Batch 60, Loss: 0.02296333573758602\n",
      "Epoch 91, Batch 70, Loss: 0.026129920035600662\n",
      "Epoch 91, Batch 80, Loss: 0.034769631922245026\n",
      "Epoch 91, Batch 90, Loss: 0.027744611725211143\n",
      "Epoch 91, Batch 100, Loss: 0.020510882139205933\n",
      "Epoch 92, Batch 0, Loss: 0.021188046783208847\n",
      "Epoch 92, Batch 10, Loss: 0.02408994361758232\n",
      "Epoch 92, Batch 20, Loss: 0.022463195025920868\n",
      "Epoch 92, Batch 30, Loss: 0.023827292025089264\n",
      "Epoch 92, Batch 40, Loss: 0.02154427021741867\n",
      "Epoch 92, Batch 50, Loss: 0.020521551370620728\n",
      "Epoch 92, Batch 60, Loss: 0.026623379439115524\n",
      "Epoch 92, Batch 70, Loss: 0.028211822733283043\n",
      "Epoch 92, Batch 80, Loss: 0.033110879361629486\n",
      "Epoch 92, Batch 90, Loss: 0.021585147827863693\n",
      "Epoch 92, Batch 100, Loss: 0.02696082927286625\n",
      "Epoch 93, Batch 0, Loss: 0.023029325529932976\n",
      "Epoch 93, Batch 10, Loss: 0.021232647821307182\n",
      "Epoch 93, Batch 20, Loss: 0.024217840284109116\n",
      "Epoch 93, Batch 30, Loss: 0.019829099997878075\n",
      "Epoch 93, Batch 40, Loss: 0.024401450529694557\n",
      "Epoch 93, Batch 50, Loss: 0.024412499740719795\n",
      "Epoch 93, Batch 60, Loss: 0.024353066459298134\n",
      "Epoch 93, Batch 70, Loss: 0.020269764587283134\n",
      "Epoch 93, Batch 80, Loss: 0.025027649477124214\n",
      "Epoch 93, Batch 90, Loss: 0.014282265678048134\n",
      "Epoch 93, Batch 100, Loss: 0.018317919224500656\n",
      "Epoch 94, Batch 0, Loss: 0.026308976113796234\n",
      "Epoch 94, Batch 10, Loss: 0.014507339335978031\n",
      "Epoch 94, Batch 20, Loss: 0.01858101785182953\n",
      "Epoch 94, Batch 30, Loss: 0.021406065672636032\n",
      "Epoch 94, Batch 40, Loss: 0.02079886943101883\n",
      "Epoch 94, Batch 50, Loss: 0.02402576059103012\n",
      "Epoch 94, Batch 60, Loss: 0.021697329357266426\n",
      "Epoch 94, Batch 70, Loss: 0.025659676641225815\n",
      "Epoch 94, Batch 80, Loss: 0.025679592043161392\n",
      "Epoch 94, Batch 90, Loss: 0.01736048422753811\n",
      "Epoch 94, Batch 100, Loss: 0.02077675238251686\n",
      "Epoch 95, Batch 0, Loss: 0.027883708477020264\n",
      "Epoch 95, Batch 10, Loss: 0.03810287266969681\n",
      "Epoch 95, Batch 20, Loss: 0.021917816251516342\n",
      "Epoch 95, Batch 30, Loss: 0.01950845681130886\n",
      "Epoch 95, Batch 40, Loss: 0.027137717232108116\n",
      "Epoch 95, Batch 50, Loss: 0.025335349142551422\n",
      "Epoch 95, Batch 60, Loss: 0.021976061165332794\n",
      "Epoch 95, Batch 70, Loss: 0.01913198083639145\n",
      "Epoch 95, Batch 80, Loss: 0.020036468282341957\n",
      "Epoch 95, Batch 90, Loss: 0.02106257900595665\n",
      "Epoch 95, Batch 100, Loss: 0.023785846307873726\n",
      "Epoch 96, Batch 0, Loss: 0.025575648993253708\n",
      "Epoch 96, Batch 10, Loss: 0.029197335243225098\n",
      "Epoch 96, Batch 20, Loss: 0.021984562277793884\n",
      "Epoch 96, Batch 30, Loss: 0.024105284363031387\n",
      "Epoch 96, Batch 40, Loss: 0.035114835947752\n",
      "Epoch 96, Batch 50, Loss: 0.023436959832906723\n",
      "Epoch 96, Batch 60, Loss: 0.02897072583436966\n",
      "Epoch 96, Batch 70, Loss: 0.021662432700395584\n",
      "Epoch 96, Batch 80, Loss: 0.019431082531809807\n",
      "Epoch 96, Batch 90, Loss: 0.022284023463726044\n",
      "Epoch 96, Batch 100, Loss: 0.02581377699971199\n",
      "Epoch 97, Batch 0, Loss: 0.017460038885474205\n",
      "Epoch 97, Batch 10, Loss: 0.02618097886443138\n",
      "Epoch 97, Batch 20, Loss: 0.029966112226247787\n",
      "Epoch 97, Batch 30, Loss: 0.02142202854156494\n",
      "Epoch 97, Batch 40, Loss: 0.028129778802394867\n",
      "Epoch 97, Batch 50, Loss: 0.02696034498512745\n",
      "Epoch 97, Batch 60, Loss: 0.03152617812156677\n",
      "Epoch 97, Batch 70, Loss: 0.025358637794852257\n",
      "Epoch 97, Batch 80, Loss: 0.019729752093553543\n",
      "Epoch 97, Batch 90, Loss: 0.022975489497184753\n",
      "Epoch 97, Batch 100, Loss: 0.02479509264230728\n",
      "Epoch 98, Batch 0, Loss: 0.01657121255993843\n",
      "Epoch 98, Batch 10, Loss: 0.025047961622476578\n",
      "Epoch 98, Batch 20, Loss: 0.026147594675421715\n",
      "Epoch 98, Batch 30, Loss: 0.027399418875575066\n",
      "Epoch 98, Batch 40, Loss: 0.02287915349006653\n",
      "Epoch 98, Batch 50, Loss: 0.021319642663002014\n",
      "Epoch 98, Batch 60, Loss: 0.021489322185516357\n",
      "Epoch 98, Batch 70, Loss: 0.03006128780543804\n",
      "Epoch 98, Batch 80, Loss: 0.03124113567173481\n",
      "Epoch 98, Batch 90, Loss: 0.025557588785886765\n",
      "Epoch 98, Batch 100, Loss: 0.026859598234295845\n",
      "Epoch 99, Batch 0, Loss: 0.01894897222518921\n",
      "Epoch 99, Batch 10, Loss: 0.02630896493792534\n",
      "Epoch 99, Batch 20, Loss: 0.03433636203408241\n",
      "Epoch 99, Batch 30, Loss: 0.017580151557922363\n",
      "Epoch 99, Batch 40, Loss: 0.02517980895936489\n",
      "Epoch 99, Batch 50, Loss: 0.022061433643102646\n",
      "Epoch 99, Batch 60, Loss: 0.02670038491487503\n",
      "Epoch 99, Batch 70, Loss: 0.020937543362379074\n",
      "Epoch 99, Batch 80, Loss: 0.027034467086195946\n",
      "Epoch 99, Batch 90, Loss: 0.0320831835269928\n",
      "Epoch 99, Batch 100, Loss: 0.0206682737916708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_LSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True)\n",
       "    (fc1): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.039533; Test RMSE 41.517510\n",
      "\n",
      "Train  MAE: 0.023022; Test  MAE 28.970661\n",
      "\n",
      "Train  R^2: 0.998437; Test  R^2 0.962160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_num_layers = 1\n",
    "lstm_hidden_size = 64\n",
    "path = \"models/CNN_LSTM_{0}Epoch_{1}Lr_{2}Layer_{3}Size.pt\".format(num_epoch, learning_rate, lstm_num_layers, lstm_hidden_size)\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-BILSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64, lstm_dropout = 0.2, fc1_output_size = 16):\n",
    "        super(BILSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True,\n",
    "                             bidirectional = True)\n",
    "        \n",
    "        # self.fc1 = nn.Linear(self.lstm_hidden_size * 2, self.fc1_output_size)\n",
    "        # self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size * 2, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # h_fc1 = self.fc1(h_lstm)\n",
    "        # h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        # h_fc2 = self.fc2(h_fc1)\n",
    "        # output = h_fc2[:, -1, :]\n",
    "\n",
    "        tan_h_lstm = self.tanh(h_lstm)\n",
    "        h_fc1 = self.fc1(tan_h_lstm)\n",
    "        output = h_fc1[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BILSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BILSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.bilstm = BILSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x)\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Reshape x to fit BiLSTM input requirements\n",
    "        # x = x.contiguous().view(batch_size, seq_len, -1)\n",
    "        x = self.bilstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_BILSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.8165919780731201\n",
      "Epoch 0, Batch 10, Loss: 0.8068506717681885\n",
      "Epoch 0, Batch 20, Loss: 0.7826277017593384\n",
      "Epoch 0, Batch 30, Loss: 0.7410628199577332\n",
      "Epoch 0, Batch 40, Loss: 0.7587448954582214\n",
      "Epoch 0, Batch 50, Loss: 0.6637872457504272\n",
      "Epoch 0, Batch 60, Loss: 0.7033519148826599\n",
      "Epoch 0, Batch 70, Loss: 0.718801736831665\n",
      "Epoch 0, Batch 80, Loss: 0.7241650819778442\n",
      "Epoch 0, Batch 90, Loss: 0.6135603785514832\n",
      "Epoch 0, Batch 100, Loss: 0.5913267731666565\n",
      "Epoch 1, Batch 0, Loss: 0.597760021686554\n",
      "Epoch 1, Batch 10, Loss: 0.5409653186798096\n",
      "Epoch 1, Batch 20, Loss: 0.4647952616214752\n",
      "Epoch 1, Batch 30, Loss: 0.3569648563861847\n",
      "Epoch 1, Batch 40, Loss: 0.24569526314735413\n",
      "Epoch 1, Batch 50, Loss: 0.17313966155052185\n",
      "Epoch 1, Batch 60, Loss: 0.1711079627275467\n",
      "Epoch 1, Batch 70, Loss: 0.1007409617304802\n",
      "Epoch 1, Batch 80, Loss: 0.1458434909582138\n",
      "Epoch 1, Batch 90, Loss: 0.14248357713222504\n",
      "Epoch 1, Batch 100, Loss: 0.12932683527469635\n",
      "Epoch 2, Batch 0, Loss: 0.09442010521888733\n",
      "Epoch 2, Batch 10, Loss: 0.06375542283058167\n",
      "Epoch 2, Batch 20, Loss: 0.07202178239822388\n",
      "Epoch 2, Batch 30, Loss: 0.10061093419790268\n",
      "Epoch 2, Batch 40, Loss: 0.10051952302455902\n",
      "Epoch 2, Batch 50, Loss: 0.12678804993629456\n",
      "Epoch 2, Batch 60, Loss: 0.08306818455457687\n",
      "Epoch 2, Batch 70, Loss: 0.08700599521398544\n",
      "Epoch 2, Batch 80, Loss: 0.10865693539381027\n",
      "Epoch 2, Batch 90, Loss: 0.04102121666073799\n",
      "Epoch 2, Batch 100, Loss: 0.0682973563671112\n",
      "Epoch 3, Batch 0, Loss: 0.12728501856327057\n",
      "Epoch 3, Batch 10, Loss: 0.0613216832280159\n",
      "Epoch 3, Batch 20, Loss: 0.06509872525930405\n",
      "Epoch 3, Batch 30, Loss: 0.08577192574739456\n",
      "Epoch 3, Batch 40, Loss: 0.057319607585668564\n",
      "Epoch 3, Batch 50, Loss: 0.07757549732923508\n",
      "Epoch 3, Batch 60, Loss: 0.06320565193891525\n",
      "Epoch 3, Batch 70, Loss: 0.06861177831888199\n",
      "Epoch 3, Batch 80, Loss: 0.0747789517045021\n",
      "Epoch 3, Batch 90, Loss: 0.0688747987151146\n",
      "Epoch 3, Batch 100, Loss: 0.05435637757182121\n",
      "Epoch 4, Batch 0, Loss: 0.04298980161547661\n",
      "Epoch 4, Batch 10, Loss: 0.0434865839779377\n",
      "Epoch 4, Batch 20, Loss: 0.07267718762159348\n",
      "Epoch 4, Batch 30, Loss: 0.04241251200437546\n",
      "Epoch 4, Batch 40, Loss: 0.06432949751615524\n",
      "Epoch 4, Batch 50, Loss: 0.03737780824303627\n",
      "Epoch 4, Batch 60, Loss: 0.051729463040828705\n",
      "Epoch 4, Batch 70, Loss: 0.043049924075603485\n",
      "Epoch 4, Batch 80, Loss: 0.0772809311747551\n",
      "Epoch 4, Batch 90, Loss: 0.03670923411846161\n",
      "Epoch 4, Batch 100, Loss: 0.03825981169939041\n",
      "Epoch 5, Batch 0, Loss: 0.04127961024641991\n",
      "Epoch 5, Batch 10, Loss: 0.04583398625254631\n",
      "Epoch 5, Batch 20, Loss: 0.037872254848480225\n",
      "Epoch 5, Batch 30, Loss: 0.05796639621257782\n",
      "Epoch 5, Batch 40, Loss: 0.045319605618715286\n",
      "Epoch 5, Batch 50, Loss: 0.05121197924017906\n",
      "Epoch 5, Batch 60, Loss: 0.034682270139455795\n",
      "Epoch 5, Batch 70, Loss: 0.04290899634361267\n",
      "Epoch 5, Batch 80, Loss: 0.043648988008499146\n",
      "Epoch 5, Batch 90, Loss: 0.05506078898906708\n",
      "Epoch 5, Batch 100, Loss: 0.03076085075736046\n",
      "Epoch 6, Batch 0, Loss: 0.04652083292603493\n",
      "Epoch 6, Batch 10, Loss: 0.053973957896232605\n",
      "Epoch 6, Batch 20, Loss: 0.04130229726433754\n",
      "Epoch 6, Batch 30, Loss: 0.041597794741392136\n",
      "Epoch 6, Batch 40, Loss: 0.025564715266227722\n",
      "Epoch 6, Batch 50, Loss: 0.041527725756168365\n",
      "Epoch 6, Batch 60, Loss: 0.030888456851243973\n",
      "Epoch 6, Batch 70, Loss: 0.03445228189229965\n",
      "Epoch 6, Batch 80, Loss: 0.042725447565317154\n",
      "Epoch 6, Batch 90, Loss: 0.0382593609392643\n",
      "Epoch 6, Batch 100, Loss: 0.05765395611524582\n",
      "Epoch 7, Batch 0, Loss: 0.04299745708703995\n",
      "Epoch 7, Batch 10, Loss: 0.03271230310201645\n",
      "Epoch 7, Batch 20, Loss: 0.05502529814839363\n",
      "Epoch 7, Batch 30, Loss: 0.03340308740735054\n",
      "Epoch 7, Batch 40, Loss: 0.034230317920446396\n",
      "Epoch 7, Batch 50, Loss: 0.04013428837060928\n",
      "Epoch 7, Batch 60, Loss: 0.03129342943429947\n",
      "Epoch 7, Batch 70, Loss: 0.02484312653541565\n",
      "Epoch 7, Batch 80, Loss: 0.040753573179244995\n",
      "Epoch 7, Batch 90, Loss: 0.03246421739459038\n",
      "Epoch 7, Batch 100, Loss: 0.03882822394371033\n",
      "Epoch 8, Batch 0, Loss: 0.03606242313981056\n",
      "Epoch 8, Batch 10, Loss: 0.030068423599004745\n",
      "Epoch 8, Batch 20, Loss: 0.027260106056928635\n",
      "Epoch 8, Batch 30, Loss: 0.039551690220832825\n",
      "Epoch 8, Batch 40, Loss: 0.03383645787835121\n",
      "Epoch 8, Batch 50, Loss: 0.04463183134794235\n",
      "Epoch 8, Batch 60, Loss: 0.04334700107574463\n",
      "Epoch 8, Batch 70, Loss: 0.0258611012250185\n",
      "Epoch 8, Batch 80, Loss: 0.05576470494270325\n",
      "Epoch 8, Batch 90, Loss: 0.03685382008552551\n",
      "Epoch 8, Batch 100, Loss: 0.031010806560516357\n",
      "Epoch 9, Batch 0, Loss: 0.03379826247692108\n",
      "Epoch 9, Batch 10, Loss: 0.04725779965519905\n",
      "Epoch 9, Batch 20, Loss: 0.02558812126517296\n",
      "Epoch 9, Batch 30, Loss: 0.028613988310098648\n",
      "Epoch 9, Batch 40, Loss: 0.039822615683078766\n",
      "Epoch 9, Batch 50, Loss: 0.0466570220887661\n",
      "Epoch 9, Batch 60, Loss: 0.03525684028863907\n",
      "Epoch 9, Batch 70, Loss: 0.022617686539888382\n",
      "Epoch 9, Batch 80, Loss: 0.023731011897325516\n",
      "Epoch 9, Batch 90, Loss: 0.033071279525756836\n",
      "Epoch 9, Batch 100, Loss: 0.027598783373832703\n",
      "Epoch 10, Batch 0, Loss: 0.038640547543764114\n",
      "Epoch 10, Batch 10, Loss: 0.032008249312639236\n",
      "Epoch 10, Batch 20, Loss: 0.031052984297275543\n",
      "Epoch 10, Batch 30, Loss: 0.036851342767477036\n",
      "Epoch 10, Batch 40, Loss: 0.026874013245105743\n",
      "Epoch 10, Batch 50, Loss: 0.034059926867485046\n",
      "Epoch 10, Batch 60, Loss: 0.031405046582221985\n",
      "Epoch 10, Batch 70, Loss: 0.025514762848615646\n",
      "Epoch 10, Batch 80, Loss: 0.029651764780282974\n",
      "Epoch 10, Batch 90, Loss: 0.03191707655787468\n",
      "Epoch 10, Batch 100, Loss: 0.03582274913787842\n",
      "Epoch 11, Batch 0, Loss: 0.032282814383506775\n",
      "Epoch 11, Batch 10, Loss: 0.03607737272977829\n",
      "Epoch 11, Batch 20, Loss: 0.026009945198893547\n",
      "Epoch 11, Batch 30, Loss: 0.031470295041799545\n",
      "Epoch 11, Batch 40, Loss: 0.02727707102894783\n",
      "Epoch 11, Batch 50, Loss: 0.030999019742012024\n",
      "Epoch 11, Batch 60, Loss: 0.028614329174160957\n",
      "Epoch 11, Batch 70, Loss: 0.02189655601978302\n",
      "Epoch 11, Batch 80, Loss: 0.03726779296994209\n",
      "Epoch 11, Batch 90, Loss: 0.02833804115653038\n",
      "Epoch 11, Batch 100, Loss: 0.056094251573085785\n",
      "Epoch 12, Batch 0, Loss: 0.028889968991279602\n",
      "Epoch 12, Batch 10, Loss: 0.029750511050224304\n",
      "Epoch 12, Batch 20, Loss: 0.031832437962293625\n",
      "Epoch 12, Batch 30, Loss: 0.033858902752399445\n",
      "Epoch 12, Batch 40, Loss: 0.025934556499123573\n",
      "Epoch 12, Batch 50, Loss: 0.0306997112929821\n",
      "Epoch 12, Batch 60, Loss: 0.02598506212234497\n",
      "Epoch 12, Batch 70, Loss: 0.027604924514889717\n",
      "Epoch 12, Batch 80, Loss: 0.040995195508003235\n",
      "Epoch 12, Batch 90, Loss: 0.02664860710501671\n",
      "Epoch 12, Batch 100, Loss: 0.023133207112550735\n",
      "Epoch 13, Batch 0, Loss: 0.027491847053170204\n",
      "Epoch 13, Batch 10, Loss: 0.0239095501601696\n",
      "Epoch 13, Batch 20, Loss: 0.03567216545343399\n",
      "Epoch 13, Batch 30, Loss: 0.02568083256483078\n",
      "Epoch 13, Batch 40, Loss: 0.03976675868034363\n",
      "Epoch 13, Batch 50, Loss: 0.030618606135249138\n",
      "Epoch 13, Batch 60, Loss: 0.03701933100819588\n",
      "Epoch 13, Batch 70, Loss: 0.024999964982271194\n",
      "Epoch 13, Batch 80, Loss: 0.03303280845284462\n",
      "Epoch 13, Batch 90, Loss: 0.02918347343802452\n",
      "Epoch 13, Batch 100, Loss: 0.02967512235045433\n",
      "Epoch 14, Batch 0, Loss: 0.02880023419857025\n",
      "Epoch 14, Batch 10, Loss: 0.026154812425374985\n",
      "Epoch 14, Batch 20, Loss: 0.031343258917331696\n",
      "Epoch 14, Batch 30, Loss: 0.02669772505760193\n",
      "Epoch 14, Batch 40, Loss: 0.019737381488084793\n",
      "Epoch 14, Batch 50, Loss: 0.026097506284713745\n",
      "Epoch 14, Batch 60, Loss: 0.020704643800854683\n",
      "Epoch 14, Batch 70, Loss: 0.03179292008280754\n",
      "Epoch 14, Batch 80, Loss: 0.027968840673565865\n",
      "Epoch 14, Batch 90, Loss: 0.028522368520498276\n",
      "Epoch 14, Batch 100, Loss: 0.02105274423956871\n",
      "Epoch 15, Batch 0, Loss: 0.019732315093278885\n",
      "Epoch 15, Batch 10, Loss: 0.027867969125509262\n",
      "Epoch 15, Batch 20, Loss: 0.02438417449593544\n",
      "Epoch 15, Batch 30, Loss: 0.029899362474679947\n",
      "Epoch 15, Batch 40, Loss: 0.02767392247915268\n",
      "Epoch 15, Batch 50, Loss: 0.02946097031235695\n",
      "Epoch 15, Batch 60, Loss: 0.021848103031516075\n",
      "Epoch 15, Batch 70, Loss: 0.018435576930642128\n",
      "Epoch 15, Batch 80, Loss: 0.02843661606311798\n",
      "Epoch 15, Batch 90, Loss: 0.02330324426293373\n",
      "Epoch 15, Batch 100, Loss: 0.02541184052824974\n",
      "Epoch 16, Batch 0, Loss: 0.025983070954680443\n",
      "Epoch 16, Batch 10, Loss: 0.02435368113219738\n",
      "Epoch 16, Batch 20, Loss: 0.01916491985321045\n",
      "Epoch 16, Batch 30, Loss: 0.029478348791599274\n",
      "Epoch 16, Batch 40, Loss: 0.02422010339796543\n",
      "Epoch 16, Batch 50, Loss: 0.034334804862737656\n",
      "Epoch 16, Batch 60, Loss: 0.02836460806429386\n",
      "Epoch 16, Batch 70, Loss: 0.026117999106645584\n",
      "Epoch 16, Batch 80, Loss: 0.03519650548696518\n",
      "Epoch 16, Batch 90, Loss: 0.02140693925321102\n",
      "Epoch 16, Batch 100, Loss: 0.02667160890996456\n",
      "Epoch 17, Batch 0, Loss: 0.022996686398983\n",
      "Epoch 17, Batch 10, Loss: 0.02606312185525894\n",
      "Epoch 17, Batch 20, Loss: 0.027059786021709442\n",
      "Epoch 17, Batch 30, Loss: 0.017633438110351562\n",
      "Epoch 17, Batch 40, Loss: 0.026007022708654404\n",
      "Epoch 17, Batch 50, Loss: 0.026826445013284683\n",
      "Epoch 17, Batch 60, Loss: 0.02702074497938156\n",
      "Epoch 17, Batch 70, Loss: 0.03157281503081322\n",
      "Epoch 17, Batch 80, Loss: 0.020449865609407425\n",
      "Epoch 17, Batch 90, Loss: 0.02631905861198902\n",
      "Epoch 17, Batch 100, Loss: 0.03242594376206398\n",
      "Epoch 18, Batch 0, Loss: 0.024871937930583954\n",
      "Epoch 18, Batch 10, Loss: 0.02129506878554821\n",
      "Epoch 18, Batch 20, Loss: 0.023123839870095253\n",
      "Epoch 18, Batch 30, Loss: 0.024407900869846344\n",
      "Epoch 18, Batch 40, Loss: 0.019546959549188614\n",
      "Epoch 18, Batch 50, Loss: 0.028961122035980225\n",
      "Epoch 18, Batch 60, Loss: 0.03370184451341629\n",
      "Epoch 18, Batch 70, Loss: 0.03170577436685562\n",
      "Epoch 18, Batch 80, Loss: 0.030160101130604744\n",
      "Epoch 18, Batch 90, Loss: 0.04446256160736084\n",
      "Epoch 18, Batch 100, Loss: 0.02674022689461708\n",
      "Epoch 19, Batch 0, Loss: 0.022135797888040543\n",
      "Epoch 19, Batch 10, Loss: 0.02365940250456333\n",
      "Epoch 19, Batch 20, Loss: 0.024882664903998375\n",
      "Epoch 19, Batch 30, Loss: 0.02851274237036705\n",
      "Epoch 19, Batch 40, Loss: 0.03203502297401428\n",
      "Epoch 19, Batch 50, Loss: 0.024953175336122513\n",
      "Epoch 19, Batch 60, Loss: 0.0192768182605505\n",
      "Epoch 19, Batch 70, Loss: 0.021482067182660103\n",
      "Epoch 19, Batch 80, Loss: 0.02833130955696106\n",
      "Epoch 19, Batch 90, Loss: 0.025075538083910942\n",
      "Epoch 19, Batch 100, Loss: 0.03025864250957966\n",
      "Epoch 20, Batch 0, Loss: 0.033914193511009216\n",
      "Epoch 20, Batch 10, Loss: 0.03719320893287659\n",
      "Epoch 20, Batch 20, Loss: 0.02600480243563652\n",
      "Epoch 20, Batch 30, Loss: 0.027346722781658173\n",
      "Epoch 20, Batch 40, Loss: 0.027971897274255753\n",
      "Epoch 20, Batch 50, Loss: 0.02248818799853325\n",
      "Epoch 20, Batch 60, Loss: 0.03917418792843819\n",
      "Epoch 20, Batch 70, Loss: 0.018103664740920067\n",
      "Epoch 20, Batch 80, Loss: 0.023686964064836502\n",
      "Epoch 20, Batch 90, Loss: 0.025529049336910248\n",
      "Epoch 20, Batch 100, Loss: 0.030724897980690002\n",
      "Epoch 21, Batch 0, Loss: 0.03295554965734482\n",
      "Epoch 21, Batch 10, Loss: 0.031318679451942444\n",
      "Epoch 21, Batch 20, Loss: 0.03059377893805504\n",
      "Epoch 21, Batch 30, Loss: 0.0262653399258852\n",
      "Epoch 21, Batch 40, Loss: 0.030894659459590912\n",
      "Epoch 21, Batch 50, Loss: 0.034584324806928635\n",
      "Epoch 21, Batch 60, Loss: 0.03066823072731495\n",
      "Epoch 21, Batch 70, Loss: 0.02763965353369713\n",
      "Epoch 21, Batch 80, Loss: 0.030646272003650665\n",
      "Epoch 21, Batch 90, Loss: 0.028230564668774605\n",
      "Epoch 21, Batch 100, Loss: 0.023147262632846832\n",
      "Epoch 22, Batch 0, Loss: 0.01933375932276249\n",
      "Epoch 22, Batch 10, Loss: 0.02745332568883896\n",
      "Epoch 22, Batch 20, Loss: 0.021301820874214172\n",
      "Epoch 22, Batch 30, Loss: 0.01657409593462944\n",
      "Epoch 22, Batch 40, Loss: 0.03267960622906685\n",
      "Epoch 22, Batch 50, Loss: 0.027384620159864426\n",
      "Epoch 22, Batch 60, Loss: 0.02628335729241371\n",
      "Epoch 22, Batch 70, Loss: 0.02775348164141178\n",
      "Epoch 22, Batch 80, Loss: 0.024451877921819687\n",
      "Epoch 22, Batch 90, Loss: 0.02469906583428383\n",
      "Epoch 22, Batch 100, Loss: 0.024526303634047508\n",
      "Epoch 23, Batch 0, Loss: 0.01823488064110279\n",
      "Epoch 23, Batch 10, Loss: 0.027670076116919518\n",
      "Epoch 23, Batch 20, Loss: 0.02300415001809597\n",
      "Epoch 23, Batch 30, Loss: 0.01978387124836445\n",
      "Epoch 23, Batch 40, Loss: 0.03342629224061966\n",
      "Epoch 23, Batch 50, Loss: 0.027148980647325516\n",
      "Epoch 23, Batch 60, Loss: 0.0271822027862072\n",
      "Epoch 23, Batch 70, Loss: 0.034238867461681366\n",
      "Epoch 23, Batch 80, Loss: 0.03076576441526413\n",
      "Epoch 23, Batch 90, Loss: 0.030903790146112442\n",
      "Epoch 23, Batch 100, Loss: 0.031446803361177444\n",
      "Epoch 24, Batch 0, Loss: 0.023912418633699417\n",
      "Epoch 24, Batch 10, Loss: 0.017798874527215958\n",
      "Epoch 24, Batch 20, Loss: 0.029505101963877678\n",
      "Epoch 24, Batch 30, Loss: 0.028086137026548386\n",
      "Epoch 24, Batch 40, Loss: 0.01885918714106083\n",
      "Epoch 24, Batch 50, Loss: 0.02217998169362545\n",
      "Epoch 24, Batch 60, Loss: 0.030810341238975525\n",
      "Epoch 24, Batch 70, Loss: 0.035108782351017\n",
      "Epoch 24, Batch 80, Loss: 0.02496480382978916\n",
      "Epoch 24, Batch 90, Loss: 0.02377137541770935\n",
      "Epoch 24, Batch 100, Loss: 0.02336680144071579\n",
      "Epoch 25, Batch 0, Loss: 0.030551228672266006\n",
      "Epoch 25, Batch 10, Loss: 0.02189234271645546\n",
      "Epoch 25, Batch 20, Loss: 0.021453116089105606\n",
      "Epoch 25, Batch 30, Loss: 0.02334432676434517\n",
      "Epoch 25, Batch 40, Loss: 0.02614106424152851\n",
      "Epoch 25, Batch 50, Loss: 0.02494126744568348\n",
      "Epoch 25, Batch 60, Loss: 0.022202488034963608\n",
      "Epoch 25, Batch 70, Loss: 0.03560913726687431\n",
      "Epoch 25, Batch 80, Loss: 0.025208694860339165\n",
      "Epoch 25, Batch 90, Loss: 0.028439635410904884\n",
      "Epoch 25, Batch 100, Loss: 0.02398880198597908\n",
      "Epoch 26, Batch 0, Loss: 0.02547086961567402\n",
      "Epoch 26, Batch 10, Loss: 0.022106459364295006\n",
      "Epoch 26, Batch 20, Loss: 0.026687070727348328\n",
      "Epoch 26, Batch 30, Loss: 0.030692128464579582\n",
      "Epoch 26, Batch 40, Loss: 0.020929815247654915\n",
      "Epoch 26, Batch 50, Loss: 0.029010677710175514\n",
      "Epoch 26, Batch 60, Loss: 0.022524764761328697\n",
      "Epoch 26, Batch 70, Loss: 0.035712432116270065\n",
      "Epoch 26, Batch 80, Loss: 0.02337220124900341\n",
      "Epoch 26, Batch 90, Loss: 0.025421200320124626\n",
      "Epoch 26, Batch 100, Loss: 0.02069590613245964\n",
      "Epoch 27, Batch 0, Loss: 0.028834665194153786\n",
      "Epoch 27, Batch 10, Loss: 0.018034417182207108\n",
      "Epoch 27, Batch 20, Loss: 0.02034907042980194\n",
      "Epoch 27, Batch 30, Loss: 0.020110543817281723\n",
      "Epoch 27, Batch 40, Loss: 0.028563305735588074\n",
      "Epoch 27, Batch 50, Loss: 0.022068660706281662\n",
      "Epoch 27, Batch 60, Loss: 0.01952953077852726\n",
      "Epoch 27, Batch 70, Loss: 0.022743836045265198\n",
      "Epoch 27, Batch 80, Loss: 0.02546905353665352\n",
      "Epoch 27, Batch 90, Loss: 0.028115976601839066\n",
      "Epoch 27, Batch 100, Loss: 0.02520470879971981\n",
      "Epoch 28, Batch 0, Loss: 0.02017257548868656\n",
      "Epoch 28, Batch 10, Loss: 0.027342885732650757\n",
      "Epoch 28, Batch 20, Loss: 0.02448694035410881\n",
      "Epoch 28, Batch 30, Loss: 0.026211101561784744\n",
      "Epoch 28, Batch 40, Loss: 0.03146642819046974\n",
      "Epoch 28, Batch 50, Loss: 0.026185527443885803\n",
      "Epoch 28, Batch 60, Loss: 0.023978710174560547\n",
      "Epoch 28, Batch 70, Loss: 0.026974504813551903\n",
      "Epoch 28, Batch 80, Loss: 0.02135728858411312\n",
      "Epoch 28, Batch 90, Loss: 0.031289536505937576\n",
      "Epoch 28, Batch 100, Loss: 0.027185577899217606\n",
      "Epoch 29, Batch 0, Loss: 0.02647026628255844\n",
      "Epoch 29, Batch 10, Loss: 0.024164550006389618\n",
      "Epoch 29, Batch 20, Loss: 0.026165783405303955\n",
      "Epoch 29, Batch 30, Loss: 0.02889372408390045\n",
      "Epoch 29, Batch 40, Loss: 0.027710851281881332\n",
      "Epoch 29, Batch 50, Loss: 0.023056916892528534\n",
      "Epoch 29, Batch 60, Loss: 0.029328910633921623\n",
      "Epoch 29, Batch 70, Loss: 0.02930419147014618\n",
      "Epoch 29, Batch 80, Loss: 0.02769182249903679\n",
      "Epoch 29, Batch 90, Loss: 0.023836037144064903\n",
      "Epoch 29, Batch 100, Loss: 0.025614427402615547\n",
      "Epoch 30, Batch 0, Loss: 0.02441512979567051\n",
      "Epoch 30, Batch 10, Loss: 0.021397609263658524\n",
      "Epoch 30, Batch 20, Loss: 0.030796382576227188\n",
      "Epoch 30, Batch 30, Loss: 0.025376711040735245\n",
      "Epoch 30, Batch 40, Loss: 0.02168886736035347\n",
      "Epoch 30, Batch 50, Loss: 0.024631928652524948\n",
      "Epoch 30, Batch 60, Loss: 0.021455179899930954\n",
      "Epoch 30, Batch 70, Loss: 0.020929094403982162\n",
      "Epoch 30, Batch 80, Loss: 0.02406877465546131\n",
      "Epoch 30, Batch 90, Loss: 0.031091777607798576\n",
      "Epoch 30, Batch 100, Loss: 0.02107931300997734\n",
      "Epoch 31, Batch 0, Loss: 0.026102324947714806\n",
      "Epoch 31, Batch 10, Loss: 0.02333071455359459\n",
      "Epoch 31, Batch 20, Loss: 0.029726754873991013\n",
      "Epoch 31, Batch 30, Loss: 0.028168799355626106\n",
      "Epoch 31, Batch 40, Loss: 0.020500268787145615\n",
      "Epoch 31, Batch 50, Loss: 0.027367208153009415\n",
      "Epoch 31, Batch 60, Loss: 0.023970242589712143\n",
      "Epoch 31, Batch 70, Loss: 0.029743719846010208\n",
      "Epoch 31, Batch 80, Loss: 0.02591656520962715\n",
      "Epoch 31, Batch 90, Loss: 0.032646842300891876\n",
      "Epoch 31, Batch 100, Loss: 0.023717021569609642\n",
      "Epoch 32, Batch 0, Loss: 0.023229144513607025\n",
      "Epoch 32, Batch 10, Loss: 0.033830635249614716\n",
      "Epoch 32, Batch 20, Loss: 0.02774546667933464\n",
      "Epoch 32, Batch 30, Loss: 0.02879764325916767\n",
      "Epoch 32, Batch 40, Loss: 0.027631305158138275\n",
      "Epoch 32, Batch 50, Loss: 0.0183895044028759\n",
      "Epoch 32, Batch 60, Loss: 0.025849346071481705\n",
      "Epoch 32, Batch 70, Loss: 0.02336648665368557\n",
      "Epoch 32, Batch 80, Loss: 0.02612483501434326\n",
      "Epoch 32, Batch 90, Loss: 0.02204643003642559\n",
      "Epoch 32, Batch 100, Loss: 0.022429361939430237\n",
      "Epoch 33, Batch 0, Loss: 0.026977580040693283\n",
      "Epoch 33, Batch 10, Loss: 0.025969844311475754\n",
      "Epoch 33, Batch 20, Loss: 0.0162773709744215\n",
      "Epoch 33, Batch 30, Loss: 0.027816273272037506\n",
      "Epoch 33, Batch 40, Loss: 0.02088187448680401\n",
      "Epoch 33, Batch 50, Loss: 0.02491355501115322\n",
      "Epoch 33, Batch 60, Loss: 0.03160463273525238\n",
      "Epoch 33, Batch 70, Loss: 0.016876565292477608\n",
      "Epoch 33, Batch 80, Loss: 0.024265659973025322\n",
      "Epoch 33, Batch 90, Loss: 0.02745828405022621\n",
      "Epoch 33, Batch 100, Loss: 0.025442907586693764\n",
      "Epoch 34, Batch 0, Loss: 0.0212760791182518\n",
      "Epoch 34, Batch 10, Loss: 0.02420322224497795\n",
      "Epoch 34, Batch 20, Loss: 0.027073616161942482\n",
      "Epoch 34, Batch 30, Loss: 0.029342250898480415\n",
      "Epoch 34, Batch 40, Loss: 0.031465694308280945\n",
      "Epoch 34, Batch 50, Loss: 0.029117204248905182\n",
      "Epoch 34, Batch 60, Loss: 0.025490382686257362\n",
      "Epoch 34, Batch 70, Loss: 0.02885652892291546\n",
      "Epoch 34, Batch 80, Loss: 0.020140182226896286\n",
      "Epoch 34, Batch 90, Loss: 0.02644071914255619\n",
      "Epoch 34, Batch 100, Loss: 0.01900395192205906\n",
      "Epoch 35, Batch 0, Loss: 0.023026714101433754\n",
      "Epoch 35, Batch 10, Loss: 0.0183979831635952\n",
      "Epoch 35, Batch 20, Loss: 0.022271133959293365\n",
      "Epoch 35, Batch 30, Loss: 0.022047344595193863\n",
      "Epoch 35, Batch 40, Loss: 0.024662505835294724\n",
      "Epoch 35, Batch 50, Loss: 0.025991275906562805\n",
      "Epoch 35, Batch 60, Loss: 0.025799550116062164\n",
      "Epoch 35, Batch 70, Loss: 0.02873322181403637\n",
      "Epoch 35, Batch 80, Loss: 0.032129667699337006\n",
      "Epoch 35, Batch 90, Loss: 0.027221141383051872\n",
      "Epoch 35, Batch 100, Loss: 0.025059469044208527\n",
      "Epoch 36, Batch 0, Loss: 0.028754770755767822\n",
      "Epoch 36, Batch 10, Loss: 0.02656475082039833\n",
      "Epoch 36, Batch 20, Loss: 0.028135670349001884\n",
      "Epoch 36, Batch 30, Loss: 0.026836274191737175\n",
      "Epoch 36, Batch 40, Loss: 0.028568293899297714\n",
      "Epoch 36, Batch 50, Loss: 0.020945891737937927\n",
      "Epoch 36, Batch 60, Loss: 0.020867295563220978\n",
      "Epoch 36, Batch 70, Loss: 0.027117745950818062\n",
      "Epoch 36, Batch 80, Loss: 0.025774691253900528\n",
      "Epoch 36, Batch 90, Loss: 0.030393484979867935\n",
      "Epoch 36, Batch 100, Loss: 0.021496281027793884\n",
      "Epoch 37, Batch 0, Loss: 0.0232568196952343\n",
      "Epoch 37, Batch 10, Loss: 0.024207651615142822\n",
      "Epoch 37, Batch 20, Loss: 0.024380210787057877\n",
      "Epoch 37, Batch 30, Loss: 0.019922403618693352\n",
      "Epoch 37, Batch 40, Loss: 0.028608499094843864\n",
      "Epoch 37, Batch 50, Loss: 0.017185986042022705\n",
      "Epoch 37, Batch 60, Loss: 0.0304702278226614\n",
      "Epoch 37, Batch 70, Loss: 0.023884985595941544\n",
      "Epoch 37, Batch 80, Loss: 0.02405574545264244\n",
      "Epoch 37, Batch 90, Loss: 0.027237117290496826\n",
      "Epoch 37, Batch 100, Loss: 0.024018805474042892\n",
      "Epoch 38, Batch 0, Loss: 0.019381970167160034\n",
      "Epoch 38, Batch 10, Loss: 0.021783264353871346\n",
      "Epoch 38, Batch 20, Loss: 0.02182900160551071\n",
      "Epoch 38, Batch 30, Loss: 0.027635706588625908\n",
      "Epoch 38, Batch 40, Loss: 0.020052369683980942\n",
      "Epoch 38, Batch 50, Loss: 0.024189447984099388\n",
      "Epoch 38, Batch 60, Loss: 0.02459479309618473\n",
      "Epoch 38, Batch 70, Loss: 0.02692544274032116\n",
      "Epoch 38, Batch 80, Loss: 0.019737625494599342\n",
      "Epoch 38, Batch 90, Loss: 0.025249844416975975\n",
      "Epoch 38, Batch 100, Loss: 0.020283211022615433\n",
      "Epoch 39, Batch 0, Loss: 0.028521953150629997\n",
      "Epoch 39, Batch 10, Loss: 0.023010941222310066\n",
      "Epoch 39, Batch 20, Loss: 0.02493423782289028\n",
      "Epoch 39, Batch 30, Loss: 0.02052181586623192\n",
      "Epoch 39, Batch 40, Loss: 0.021885287016630173\n",
      "Epoch 39, Batch 50, Loss: 0.022439422085881233\n",
      "Epoch 39, Batch 60, Loss: 0.02452920749783516\n",
      "Epoch 39, Batch 70, Loss: 0.019690554589033127\n",
      "Epoch 39, Batch 80, Loss: 0.03501225262880325\n",
      "Epoch 39, Batch 90, Loss: 0.021148791536688805\n",
      "Epoch 39, Batch 100, Loss: 0.027056830003857613\n",
      "Epoch 40, Batch 0, Loss: 0.025907466188073158\n",
      "Epoch 40, Batch 10, Loss: 0.03283243253827095\n",
      "Epoch 40, Batch 20, Loss: 0.02310383878648281\n",
      "Epoch 40, Batch 30, Loss: 0.024369237944483757\n",
      "Epoch 40, Batch 40, Loss: 0.026631589978933334\n",
      "Epoch 40, Batch 50, Loss: 0.02049773745238781\n",
      "Epoch 40, Batch 60, Loss: 0.026738572865724564\n",
      "Epoch 40, Batch 70, Loss: 0.024290133267641068\n",
      "Epoch 40, Batch 80, Loss: 0.022135302424430847\n",
      "Epoch 40, Batch 90, Loss: 0.023334607481956482\n",
      "Epoch 40, Batch 100, Loss: 0.027576036751270294\n",
      "Epoch 41, Batch 0, Loss: 0.025153178721666336\n",
      "Epoch 41, Batch 10, Loss: 0.01872832328081131\n",
      "Epoch 41, Batch 20, Loss: 0.02601855807006359\n",
      "Epoch 41, Batch 30, Loss: 0.023217596113681793\n",
      "Epoch 41, Batch 40, Loss: 0.022878993302583694\n",
      "Epoch 41, Batch 50, Loss: 0.02116629108786583\n",
      "Epoch 41, Batch 60, Loss: 0.023890968412160873\n",
      "Epoch 41, Batch 70, Loss: 0.017256086692214012\n",
      "Epoch 41, Batch 80, Loss: 0.019085997715592384\n",
      "Epoch 41, Batch 90, Loss: 0.024449728429317474\n",
      "Epoch 41, Batch 100, Loss: 0.018346138298511505\n",
      "Epoch 42, Batch 0, Loss: 0.02334974706172943\n",
      "Epoch 42, Batch 10, Loss: 0.020954346284270287\n",
      "Epoch 42, Batch 20, Loss: 0.028767142444849014\n",
      "Epoch 42, Batch 30, Loss: 0.02636364847421646\n",
      "Epoch 42, Batch 40, Loss: 0.023934410884976387\n",
      "Epoch 42, Batch 50, Loss: 0.03259219229221344\n",
      "Epoch 42, Batch 60, Loss: 0.022381626069545746\n",
      "Epoch 42, Batch 70, Loss: 0.031231578439474106\n",
      "Epoch 42, Batch 80, Loss: 0.0278244037181139\n",
      "Epoch 42, Batch 90, Loss: 0.02425703965127468\n",
      "Epoch 42, Batch 100, Loss: 0.02071392349898815\n",
      "Epoch 43, Batch 0, Loss: 0.023802967742085457\n",
      "Epoch 43, Batch 10, Loss: 0.028434783220291138\n",
      "Epoch 43, Batch 20, Loss: 0.02255893684923649\n",
      "Epoch 43, Batch 30, Loss: 0.033871110528707504\n",
      "Epoch 43, Batch 40, Loss: 0.017023436725139618\n",
      "Epoch 43, Batch 50, Loss: 0.02421722188591957\n",
      "Epoch 43, Batch 60, Loss: 0.026985131204128265\n",
      "Epoch 43, Batch 70, Loss: 0.02222577854990959\n",
      "Epoch 43, Batch 80, Loss: 0.022431015968322754\n",
      "Epoch 43, Batch 90, Loss: 0.01979834958910942\n",
      "Epoch 43, Batch 100, Loss: 0.027000604197382927\n",
      "Epoch 44, Batch 0, Loss: 0.025822384282946587\n",
      "Epoch 44, Batch 10, Loss: 0.022115454077720642\n",
      "Epoch 44, Batch 20, Loss: 0.02359442040324211\n",
      "Epoch 44, Batch 30, Loss: 0.016145091503858566\n",
      "Epoch 44, Batch 40, Loss: 0.021570250391960144\n",
      "Epoch 44, Batch 50, Loss: 0.031729232519865036\n",
      "Epoch 44, Batch 60, Loss: 0.020979948341846466\n",
      "Epoch 44, Batch 70, Loss: 0.03106362745165825\n",
      "Epoch 44, Batch 80, Loss: 0.031751759350299835\n",
      "Epoch 44, Batch 90, Loss: 0.026902170851826668\n",
      "Epoch 44, Batch 100, Loss: 0.024026239290833473\n",
      "Epoch 45, Batch 0, Loss: 0.033651214092969894\n",
      "Epoch 45, Batch 10, Loss: 0.026948334649205208\n",
      "Epoch 45, Batch 20, Loss: 0.02853996679186821\n",
      "Epoch 45, Batch 30, Loss: 0.03449510782957077\n",
      "Epoch 45, Batch 40, Loss: 0.02421370893716812\n",
      "Epoch 45, Batch 50, Loss: 0.021029921248555183\n",
      "Epoch 45, Batch 60, Loss: 0.022403623908758163\n",
      "Epoch 45, Batch 70, Loss: 0.01842489093542099\n",
      "Epoch 45, Batch 80, Loss: 0.01945369876921177\n",
      "Epoch 45, Batch 90, Loss: 0.02277209796011448\n",
      "Epoch 45, Batch 100, Loss: 0.02760450914502144\n",
      "Epoch 46, Batch 0, Loss: 0.027392884716391563\n",
      "Epoch 46, Batch 10, Loss: 0.017415866255760193\n",
      "Epoch 46, Batch 20, Loss: 0.023108433932065964\n",
      "Epoch 46, Batch 30, Loss: 0.02524171583354473\n",
      "Epoch 46, Batch 40, Loss: 0.027898166328668594\n",
      "Epoch 46, Batch 50, Loss: 0.02702762559056282\n",
      "Epoch 46, Batch 60, Loss: 0.017144933342933655\n",
      "Epoch 46, Batch 70, Loss: 0.024801529943943024\n",
      "Epoch 46, Batch 80, Loss: 0.017618892714381218\n",
      "Epoch 46, Batch 90, Loss: 0.021691668778657913\n",
      "Epoch 46, Batch 100, Loss: 0.029880158603191376\n",
      "Epoch 47, Batch 0, Loss: 0.022423777729272842\n",
      "Epoch 47, Batch 10, Loss: 0.022001663222908974\n",
      "Epoch 47, Batch 20, Loss: 0.02520272508263588\n",
      "Epoch 47, Batch 30, Loss: 0.01917363330721855\n",
      "Epoch 47, Batch 40, Loss: 0.0228574201464653\n",
      "Epoch 47, Batch 50, Loss: 0.019198916852474213\n",
      "Epoch 47, Batch 60, Loss: 0.02136535570025444\n",
      "Epoch 47, Batch 70, Loss: 0.022177567705512047\n",
      "Epoch 47, Batch 80, Loss: 0.026207031682133675\n",
      "Epoch 47, Batch 90, Loss: 0.022697091102600098\n",
      "Epoch 47, Batch 100, Loss: 0.026461387053132057\n",
      "Epoch 48, Batch 0, Loss: 0.029348742216825485\n",
      "Epoch 48, Batch 10, Loss: 0.02592182159423828\n",
      "Epoch 48, Batch 20, Loss: 0.029065459966659546\n",
      "Epoch 48, Batch 30, Loss: 0.02060299925506115\n",
      "Epoch 48, Batch 40, Loss: 0.025668496266007423\n",
      "Epoch 48, Batch 50, Loss: 0.022678717970848083\n",
      "Epoch 48, Batch 60, Loss: 0.02507040649652481\n",
      "Epoch 48, Batch 70, Loss: 0.01971832476556301\n",
      "Epoch 48, Batch 80, Loss: 0.02340538799762726\n",
      "Epoch 48, Batch 90, Loss: 0.022215425968170166\n",
      "Epoch 48, Batch 100, Loss: 0.02306506037712097\n",
      "Epoch 49, Batch 0, Loss: 0.031053664162755013\n",
      "Epoch 49, Batch 10, Loss: 0.02059168554842472\n",
      "Epoch 49, Batch 20, Loss: 0.030726321041584015\n",
      "Epoch 49, Batch 30, Loss: 0.024954039603471756\n",
      "Epoch 49, Batch 40, Loss: 0.025409962981939316\n",
      "Epoch 49, Batch 50, Loss: 0.033479832112789154\n",
      "Epoch 49, Batch 60, Loss: 0.02623654715716839\n",
      "Epoch 49, Batch 70, Loss: 0.02564399316906929\n",
      "Epoch 49, Batch 80, Loss: 0.02078704535961151\n",
      "Epoch 49, Batch 90, Loss: 0.031171198934316635\n",
      "Epoch 49, Batch 100, Loss: 0.02358068898320198\n",
      "Epoch 50, Batch 0, Loss: 0.021236056461930275\n",
      "Epoch 50, Batch 10, Loss: 0.021033506840467453\n",
      "Epoch 50, Batch 20, Loss: 0.023251043632626534\n",
      "Epoch 50, Batch 30, Loss: 0.027059093117713928\n",
      "Epoch 50, Batch 40, Loss: 0.031478263437747955\n",
      "Epoch 50, Batch 50, Loss: 0.015076588839292526\n",
      "Epoch 50, Batch 60, Loss: 0.020027149468660355\n",
      "Epoch 50, Batch 70, Loss: 0.02161996439099312\n",
      "Epoch 50, Batch 80, Loss: 0.022877998650074005\n",
      "Epoch 50, Batch 90, Loss: 0.026202984154224396\n",
      "Epoch 50, Batch 100, Loss: 0.019584184512495995\n",
      "Epoch 51, Batch 0, Loss: 0.022059837356209755\n",
      "Epoch 51, Batch 10, Loss: 0.015872158110141754\n",
      "Epoch 51, Batch 20, Loss: 0.019839778542518616\n",
      "Epoch 51, Batch 30, Loss: 0.02890644408762455\n",
      "Epoch 51, Batch 40, Loss: 0.025507913902401924\n",
      "Epoch 51, Batch 50, Loss: 0.02101704478263855\n",
      "Epoch 51, Batch 60, Loss: 0.01987544633448124\n",
      "Epoch 51, Batch 70, Loss: 0.021292669698596\n",
      "Epoch 51, Batch 80, Loss: 0.035008978098630905\n",
      "Epoch 51, Batch 90, Loss: 0.02671976387500763\n",
      "Epoch 51, Batch 100, Loss: 0.028669454157352448\n",
      "Epoch 52, Batch 0, Loss: 0.020120471715927124\n",
      "Epoch 52, Batch 10, Loss: 0.026883326470851898\n",
      "Epoch 52, Batch 20, Loss: 0.02053539827466011\n",
      "Epoch 52, Batch 30, Loss: 0.03130055218935013\n",
      "Epoch 52, Batch 40, Loss: 0.022430291399359703\n",
      "Epoch 52, Batch 50, Loss: 0.026343615725636482\n",
      "Epoch 52, Batch 60, Loss: 0.018354399129748344\n",
      "Epoch 52, Batch 70, Loss: 0.02390117011964321\n",
      "Epoch 52, Batch 80, Loss: 0.030117210000753403\n",
      "Epoch 52, Batch 90, Loss: 0.020487293601036072\n",
      "Epoch 52, Batch 100, Loss: 0.022597262635827065\n",
      "Epoch 53, Batch 0, Loss: 0.030268725007772446\n",
      "Epoch 53, Batch 10, Loss: 0.023605000227689743\n",
      "Epoch 53, Batch 20, Loss: 0.018748974427580833\n",
      "Epoch 53, Batch 30, Loss: 0.023259539157152176\n",
      "Epoch 53, Batch 40, Loss: 0.020681720227003098\n",
      "Epoch 53, Batch 50, Loss: 0.0296466127038002\n",
      "Epoch 53, Batch 60, Loss: 0.02014905959367752\n",
      "Epoch 53, Batch 70, Loss: 0.030215926468372345\n",
      "Epoch 53, Batch 80, Loss: 0.019752616062760353\n",
      "Epoch 53, Batch 90, Loss: 0.02437494322657585\n",
      "Epoch 53, Batch 100, Loss: 0.026643402874469757\n",
      "Epoch 54, Batch 0, Loss: 0.02320023812353611\n",
      "Epoch 54, Batch 10, Loss: 0.03781675174832344\n",
      "Epoch 54, Batch 20, Loss: 0.033625874668359756\n",
      "Epoch 54, Batch 30, Loss: 0.03135792911052704\n",
      "Epoch 54, Batch 40, Loss: 0.02196904644370079\n",
      "Epoch 54, Batch 50, Loss: 0.030109720304608345\n",
      "Epoch 54, Batch 60, Loss: 0.02911403216421604\n",
      "Epoch 54, Batch 70, Loss: 0.022563109174370766\n",
      "Epoch 54, Batch 80, Loss: 0.021065620705485344\n",
      "Epoch 54, Batch 90, Loss: 0.02427477017045021\n",
      "Epoch 54, Batch 100, Loss: 0.018677491694688797\n",
      "Epoch 55, Batch 0, Loss: 0.02196671813726425\n",
      "Epoch 55, Batch 10, Loss: 0.02748623676598072\n",
      "Epoch 55, Batch 20, Loss: 0.026977187022566795\n",
      "Epoch 55, Batch 30, Loss: 0.02114538848400116\n",
      "Epoch 55, Batch 40, Loss: 0.020590536296367645\n",
      "Epoch 55, Batch 50, Loss: 0.019847657531499863\n",
      "Epoch 55, Batch 60, Loss: 0.024465881288051605\n",
      "Epoch 55, Batch 70, Loss: 0.023719601333141327\n",
      "Epoch 55, Batch 80, Loss: 0.019550254568457603\n",
      "Epoch 55, Batch 90, Loss: 0.022059213370084763\n",
      "Epoch 55, Batch 100, Loss: 0.02510840818285942\n",
      "Epoch 56, Batch 0, Loss: 0.02121679112315178\n",
      "Epoch 56, Batch 10, Loss: 0.024242158979177475\n",
      "Epoch 56, Batch 20, Loss: 0.024211905896663666\n",
      "Epoch 56, Batch 30, Loss: 0.024277985095977783\n",
      "Epoch 56, Batch 40, Loss: 0.023998817428946495\n",
      "Epoch 56, Batch 50, Loss: 0.020513290539383888\n",
      "Epoch 56, Batch 60, Loss: 0.014944609254598618\n",
      "Epoch 56, Batch 70, Loss: 0.024131594225764275\n",
      "Epoch 56, Batch 80, Loss: 0.026216648519039154\n",
      "Epoch 56, Batch 90, Loss: 0.027377421036362648\n",
      "Epoch 56, Batch 100, Loss: 0.024270614609122276\n",
      "Epoch 57, Batch 0, Loss: 0.02320774272084236\n",
      "Epoch 57, Batch 10, Loss: 0.028890622779726982\n",
      "Epoch 57, Batch 20, Loss: 0.024005062878131866\n",
      "Epoch 57, Batch 30, Loss: 0.019018404185771942\n",
      "Epoch 57, Batch 40, Loss: 0.016973650082945824\n",
      "Epoch 57, Batch 50, Loss: 0.025750916451215744\n",
      "Epoch 57, Batch 60, Loss: 0.017537599429488182\n",
      "Epoch 57, Batch 70, Loss: 0.018081601709127426\n",
      "Epoch 57, Batch 80, Loss: 0.02353859692811966\n",
      "Epoch 57, Batch 90, Loss: 0.033623598515987396\n",
      "Epoch 57, Batch 100, Loss: 0.024098945781588554\n",
      "Epoch 58, Batch 0, Loss: 0.019649697467684746\n",
      "Epoch 58, Batch 10, Loss: 0.022979576140642166\n",
      "Epoch 58, Batch 20, Loss: 0.021026303991675377\n",
      "Epoch 58, Batch 30, Loss: 0.021820537745952606\n",
      "Epoch 58, Batch 40, Loss: 0.0190306156873703\n",
      "Epoch 58, Batch 50, Loss: 0.026749689131975174\n",
      "Epoch 58, Batch 60, Loss: 0.027641238644719124\n",
      "Epoch 58, Batch 70, Loss: 0.01886695995926857\n",
      "Epoch 58, Batch 80, Loss: 0.0252671055495739\n",
      "Epoch 58, Batch 90, Loss: 0.02153792418539524\n",
      "Epoch 58, Batch 100, Loss: 0.020601607859134674\n",
      "Epoch 59, Batch 0, Loss: 0.028854085132479668\n",
      "Epoch 59, Batch 10, Loss: 0.028055571019649506\n",
      "Epoch 59, Batch 20, Loss: 0.020504353567957878\n",
      "Epoch 59, Batch 30, Loss: 0.023215893656015396\n",
      "Epoch 59, Batch 40, Loss: 0.02217317745089531\n",
      "Epoch 59, Batch 50, Loss: 0.02389346994459629\n",
      "Epoch 59, Batch 60, Loss: 0.022968506440520287\n",
      "Epoch 59, Batch 70, Loss: 0.02665085345506668\n",
      "Epoch 59, Batch 80, Loss: 0.031610697507858276\n",
      "Epoch 59, Batch 90, Loss: 0.023178543895483017\n",
      "Epoch 59, Batch 100, Loss: 0.021158896386623383\n",
      "Epoch 60, Batch 0, Loss: 0.022416234016418457\n",
      "Epoch 60, Batch 10, Loss: 0.03151456266641617\n",
      "Epoch 60, Batch 20, Loss: 0.02322147786617279\n",
      "Epoch 60, Batch 30, Loss: 0.025103624910116196\n",
      "Epoch 60, Batch 40, Loss: 0.025985904037952423\n",
      "Epoch 60, Batch 50, Loss: 0.027234183624386787\n",
      "Epoch 60, Batch 60, Loss: 0.023669712245464325\n",
      "Epoch 60, Batch 70, Loss: 0.024651672691106796\n",
      "Epoch 60, Batch 80, Loss: 0.023647308349609375\n",
      "Epoch 60, Batch 90, Loss: 0.025716129690408707\n",
      "Epoch 60, Batch 100, Loss: 0.024547826498746872\n",
      "Epoch 61, Batch 0, Loss: 0.01754630357027054\n",
      "Epoch 61, Batch 10, Loss: 0.028672248125076294\n",
      "Epoch 61, Batch 20, Loss: 0.02411133237183094\n",
      "Epoch 61, Batch 30, Loss: 0.02269604057073593\n",
      "Epoch 61, Batch 40, Loss: 0.03764999285340309\n",
      "Epoch 61, Batch 50, Loss: 0.02611774206161499\n",
      "Epoch 61, Batch 60, Loss: 0.01935333013534546\n",
      "Epoch 61, Batch 70, Loss: 0.021319381892681122\n",
      "Epoch 61, Batch 80, Loss: 0.02986161969602108\n",
      "Epoch 61, Batch 90, Loss: 0.019158590584993362\n",
      "Epoch 61, Batch 100, Loss: 0.02724129892885685\n",
      "Epoch 62, Batch 0, Loss: 0.01901313103735447\n",
      "Epoch 62, Batch 10, Loss: 0.021879326552152634\n",
      "Epoch 62, Batch 20, Loss: 0.02477184310555458\n",
      "Epoch 62, Batch 30, Loss: 0.019467270001769066\n",
      "Epoch 62, Batch 40, Loss: 0.02821989171206951\n",
      "Epoch 62, Batch 50, Loss: 0.03171755373477936\n",
      "Epoch 62, Batch 60, Loss: 0.022876467555761337\n",
      "Epoch 62, Batch 70, Loss: 0.024086162447929382\n",
      "Epoch 62, Batch 80, Loss: 0.022902416065335274\n",
      "Epoch 62, Batch 90, Loss: 0.024868736043572426\n",
      "Epoch 62, Batch 100, Loss: 0.021225761622190475\n",
      "Epoch 63, Batch 0, Loss: 0.03342405706644058\n",
      "Epoch 63, Batch 10, Loss: 0.019974444061517715\n",
      "Epoch 63, Batch 20, Loss: 0.023036904633045197\n",
      "Epoch 63, Batch 30, Loss: 0.02846686728298664\n",
      "Epoch 63, Batch 40, Loss: 0.022073034197092056\n",
      "Epoch 63, Batch 50, Loss: 0.02572057768702507\n",
      "Epoch 63, Batch 60, Loss: 0.017721692100167274\n",
      "Epoch 63, Batch 70, Loss: 0.01918528601527214\n",
      "Epoch 63, Batch 80, Loss: 0.019393056631088257\n",
      "Epoch 63, Batch 90, Loss: 0.022294724360108376\n",
      "Epoch 63, Batch 100, Loss: 0.020057883113622665\n",
      "Epoch 64, Batch 0, Loss: 0.018319960683584213\n",
      "Epoch 64, Batch 10, Loss: 0.022442281246185303\n",
      "Epoch 64, Batch 20, Loss: 0.02355661615729332\n",
      "Epoch 64, Batch 30, Loss: 0.026769375428557396\n",
      "Epoch 64, Batch 40, Loss: 0.02552018128335476\n",
      "Epoch 64, Batch 50, Loss: 0.04094654321670532\n",
      "Epoch 64, Batch 60, Loss: 0.026046736165881157\n",
      "Epoch 64, Batch 70, Loss: 0.01988602988421917\n",
      "Epoch 64, Batch 80, Loss: 0.019635522738099098\n",
      "Epoch 64, Batch 90, Loss: 0.02346520498394966\n",
      "Epoch 64, Batch 100, Loss: 0.027567587792873383\n",
      "Epoch 65, Batch 0, Loss: 0.029277168214321136\n",
      "Epoch 65, Batch 10, Loss: 0.019289975985884666\n",
      "Epoch 65, Batch 20, Loss: 0.029276961460709572\n",
      "Epoch 65, Batch 30, Loss: 0.02573845535516739\n",
      "Epoch 65, Batch 40, Loss: 0.019033856689929962\n",
      "Epoch 65, Batch 50, Loss: 0.024565400555729866\n",
      "Epoch 65, Batch 60, Loss: 0.028287433087825775\n",
      "Epoch 65, Batch 70, Loss: 0.02331271767616272\n",
      "Epoch 65, Batch 80, Loss: 0.019521042704582214\n",
      "Epoch 65, Batch 90, Loss: 0.02718290686607361\n",
      "Epoch 65, Batch 100, Loss: 0.030616655945777893\n",
      "Epoch 66, Batch 0, Loss: 0.016419915482401848\n",
      "Epoch 66, Batch 10, Loss: 0.017426423728466034\n",
      "Epoch 66, Batch 20, Loss: 0.02479216828942299\n",
      "Epoch 66, Batch 30, Loss: 0.020945217460393906\n",
      "Epoch 66, Batch 40, Loss: 0.021520163863897324\n",
      "Epoch 66, Batch 50, Loss: 0.02809753827750683\n",
      "Epoch 66, Batch 60, Loss: 0.03504878282546997\n",
      "Epoch 66, Batch 70, Loss: 0.025180064141750336\n",
      "Epoch 66, Batch 80, Loss: 0.022003566846251488\n",
      "Epoch 66, Batch 90, Loss: 0.022002317011356354\n",
      "Epoch 66, Batch 100, Loss: 0.023569155484437943\n",
      "Epoch 67, Batch 0, Loss: 0.027941759675741196\n",
      "Epoch 67, Batch 10, Loss: 0.032611262053251266\n",
      "Epoch 67, Batch 20, Loss: 0.016504600644111633\n",
      "Epoch 67, Batch 30, Loss: 0.0192891713231802\n",
      "Epoch 67, Batch 40, Loss: 0.021948056295514107\n",
      "Epoch 67, Batch 50, Loss: 0.02120230160653591\n",
      "Epoch 67, Batch 60, Loss: 0.023709183558821678\n",
      "Epoch 67, Batch 70, Loss: 0.025568880140781403\n",
      "Epoch 67, Batch 80, Loss: 0.017668845131993294\n",
      "Epoch 67, Batch 90, Loss: 0.024365130811929703\n",
      "Epoch 67, Batch 100, Loss: 0.02678106538951397\n",
      "Epoch 68, Batch 0, Loss: 0.02559574507176876\n",
      "Epoch 68, Batch 10, Loss: 0.02915400080382824\n",
      "Epoch 68, Batch 20, Loss: 0.02015218697488308\n",
      "Epoch 68, Batch 30, Loss: 0.02357301488518715\n",
      "Epoch 68, Batch 40, Loss: 0.02146664448082447\n",
      "Epoch 68, Batch 50, Loss: 0.02649959921836853\n",
      "Epoch 68, Batch 60, Loss: 0.03104046732187271\n",
      "Epoch 68, Batch 70, Loss: 0.028101757168769836\n",
      "Epoch 68, Batch 80, Loss: 0.02370305359363556\n",
      "Epoch 68, Batch 90, Loss: 0.024835940450429916\n",
      "Epoch 68, Batch 100, Loss: 0.021967114880681038\n",
      "Epoch 69, Batch 0, Loss: 0.03583049401640892\n",
      "Epoch 69, Batch 10, Loss: 0.02238350547850132\n",
      "Epoch 69, Batch 20, Loss: 0.02093658782541752\n",
      "Epoch 69, Batch 30, Loss: 0.020490821450948715\n",
      "Epoch 69, Batch 40, Loss: 0.028687428683042526\n",
      "Epoch 69, Batch 50, Loss: 0.022638237103819847\n",
      "Epoch 69, Batch 60, Loss: 0.01684170588850975\n",
      "Epoch 69, Batch 70, Loss: 0.023785006254911423\n",
      "Epoch 69, Batch 80, Loss: 0.02073105424642563\n",
      "Epoch 69, Batch 90, Loss: 0.021850107237696648\n",
      "Epoch 69, Batch 100, Loss: 0.02613561600446701\n",
      "Epoch 70, Batch 0, Loss: 0.028391029685735703\n",
      "Epoch 70, Batch 10, Loss: 0.023300349712371826\n",
      "Epoch 70, Batch 20, Loss: 0.026809755712747574\n",
      "Epoch 70, Batch 30, Loss: 0.026187395676970482\n",
      "Epoch 70, Batch 40, Loss: 0.022045154124498367\n",
      "Epoch 70, Batch 50, Loss: 0.023808009922504425\n",
      "Epoch 70, Batch 60, Loss: 0.028793899342417717\n",
      "Epoch 70, Batch 70, Loss: 0.016399450600147247\n",
      "Epoch 70, Batch 80, Loss: 0.018293021246790886\n",
      "Epoch 70, Batch 90, Loss: 0.016279323026537895\n",
      "Epoch 70, Batch 100, Loss: 0.030029568821191788\n",
      "Epoch 71, Batch 0, Loss: 0.01959608867764473\n",
      "Epoch 71, Batch 10, Loss: 0.019498281180858612\n",
      "Epoch 71, Batch 20, Loss: 0.024217287078499794\n",
      "Epoch 71, Batch 30, Loss: 0.028893131762742996\n",
      "Epoch 71, Batch 40, Loss: 0.016702847555279732\n",
      "Epoch 71, Batch 50, Loss: 0.02667248621582985\n",
      "Epoch 71, Batch 60, Loss: 0.02394658699631691\n",
      "Epoch 71, Batch 70, Loss: 0.028976205736398697\n",
      "Epoch 71, Batch 80, Loss: 0.016093773767352104\n",
      "Epoch 71, Batch 90, Loss: 0.02753487229347229\n",
      "Epoch 71, Batch 100, Loss: 0.028820039704442024\n",
      "Epoch 72, Batch 0, Loss: 0.026430994272232056\n",
      "Epoch 72, Batch 10, Loss: 0.01775854267179966\n",
      "Epoch 72, Batch 20, Loss: 0.0198139026761055\n",
      "Epoch 72, Batch 30, Loss: 0.01900181546807289\n",
      "Epoch 72, Batch 40, Loss: 0.024276502430438995\n",
      "Epoch 72, Batch 50, Loss: 0.021703297272324562\n",
      "Epoch 72, Batch 60, Loss: 0.03766639530658722\n",
      "Epoch 72, Batch 70, Loss: 0.022747810930013657\n",
      "Epoch 72, Batch 80, Loss: 0.019566096365451813\n",
      "Epoch 72, Batch 90, Loss: 0.02889264188706875\n",
      "Epoch 72, Batch 100, Loss: 0.020742760971188545\n",
      "Epoch 73, Batch 0, Loss: 0.022030161693692207\n",
      "Epoch 73, Batch 10, Loss: 0.021230924874544144\n",
      "Epoch 73, Batch 20, Loss: 0.025726495310664177\n",
      "Epoch 73, Batch 30, Loss: 0.031900033354759216\n",
      "Epoch 73, Batch 40, Loss: 0.019668936729431152\n",
      "Epoch 73, Batch 50, Loss: 0.023152191191911697\n",
      "Epoch 73, Batch 60, Loss: 0.02092876471579075\n",
      "Epoch 73, Batch 70, Loss: 0.02592029981315136\n",
      "Epoch 73, Batch 80, Loss: 0.020647039636969566\n",
      "Epoch 73, Batch 90, Loss: 0.02284889668226242\n",
      "Epoch 73, Batch 100, Loss: 0.02334117516875267\n",
      "Epoch 74, Batch 0, Loss: 0.021388543769717216\n",
      "Epoch 74, Batch 10, Loss: 0.027939625084400177\n",
      "Epoch 74, Batch 20, Loss: 0.019136546179652214\n",
      "Epoch 74, Batch 30, Loss: 0.019462035968899727\n",
      "Epoch 74, Batch 40, Loss: 0.03335133194923401\n",
      "Epoch 74, Batch 50, Loss: 0.01970086619257927\n",
      "Epoch 74, Batch 60, Loss: 0.02309367060661316\n",
      "Epoch 74, Batch 70, Loss: 0.02685515210032463\n",
      "Epoch 74, Batch 80, Loss: 0.024253137409687042\n",
      "Epoch 74, Batch 90, Loss: 0.034002527594566345\n",
      "Epoch 74, Batch 100, Loss: 0.020107321441173553\n",
      "Epoch 75, Batch 0, Loss: 0.021014872938394547\n",
      "Epoch 75, Batch 10, Loss: 0.03122866526246071\n",
      "Epoch 75, Batch 20, Loss: 0.026690110564231873\n",
      "Epoch 75, Batch 30, Loss: 0.023662565276026726\n",
      "Epoch 75, Batch 40, Loss: 0.024565767496824265\n",
      "Epoch 75, Batch 50, Loss: 0.01925761066377163\n",
      "Epoch 75, Batch 60, Loss: 0.03299844264984131\n",
      "Epoch 75, Batch 70, Loss: 0.02140435017645359\n",
      "Epoch 75, Batch 80, Loss: 0.026847966015338898\n",
      "Epoch 75, Batch 90, Loss: 0.017047923058271408\n",
      "Epoch 75, Batch 100, Loss: 0.030848216265439987\n",
      "Epoch 76, Batch 0, Loss: 0.020497465506196022\n",
      "Epoch 76, Batch 10, Loss: 0.02790963090956211\n",
      "Epoch 76, Batch 20, Loss: 0.02802206017076969\n",
      "Epoch 76, Batch 30, Loss: 0.019867299124598503\n",
      "Epoch 76, Batch 40, Loss: 0.02546466514468193\n",
      "Epoch 76, Batch 50, Loss: 0.0253312848508358\n",
      "Epoch 76, Batch 60, Loss: 0.01609865389764309\n",
      "Epoch 76, Batch 70, Loss: 0.02833985723555088\n",
      "Epoch 76, Batch 80, Loss: 0.01869363710284233\n",
      "Epoch 76, Batch 90, Loss: 0.019894156605005264\n",
      "Epoch 76, Batch 100, Loss: 0.02381557784974575\n",
      "Epoch 77, Batch 0, Loss: 0.021681537851691246\n",
      "Epoch 77, Batch 10, Loss: 0.019920825958251953\n",
      "Epoch 77, Batch 20, Loss: 0.025844665244221687\n",
      "Epoch 77, Batch 30, Loss: 0.022553743794560432\n",
      "Epoch 77, Batch 40, Loss: 0.026030585169792175\n",
      "Epoch 77, Batch 50, Loss: 0.02097262069582939\n",
      "Epoch 77, Batch 60, Loss: 0.023473719134926796\n",
      "Epoch 77, Batch 70, Loss: 0.024801727384328842\n",
      "Epoch 77, Batch 80, Loss: 0.027181118726730347\n",
      "Epoch 77, Batch 90, Loss: 0.018882926553487778\n",
      "Epoch 77, Batch 100, Loss: 0.022943660616874695\n",
      "Epoch 78, Batch 0, Loss: 0.02741413563489914\n",
      "Epoch 78, Batch 10, Loss: 0.02597665600478649\n",
      "Epoch 78, Batch 20, Loss: 0.02712186612188816\n",
      "Epoch 78, Batch 30, Loss: 0.0192423053085804\n",
      "Epoch 78, Batch 40, Loss: 0.03072349540889263\n",
      "Epoch 78, Batch 50, Loss: 0.02274121530354023\n",
      "Epoch 78, Batch 60, Loss: 0.0241074301302433\n",
      "Epoch 78, Batch 70, Loss: 0.025016041472554207\n",
      "Epoch 78, Batch 80, Loss: 0.025641508400440216\n",
      "Epoch 78, Batch 90, Loss: 0.02338322252035141\n",
      "Epoch 78, Batch 100, Loss: 0.021564282476902008\n",
      "Epoch 79, Batch 0, Loss: 0.03037998080253601\n",
      "Epoch 79, Batch 10, Loss: 0.024188591167330742\n",
      "Epoch 79, Batch 20, Loss: 0.024623477831482887\n",
      "Epoch 79, Batch 30, Loss: 0.0256599523127079\n",
      "Epoch 79, Batch 40, Loss: 0.027084827423095703\n",
      "Epoch 79, Batch 50, Loss: 0.019143719226121902\n",
      "Epoch 79, Batch 60, Loss: 0.029699603095650673\n",
      "Epoch 79, Batch 70, Loss: 0.027383040636777878\n",
      "Epoch 79, Batch 80, Loss: 0.025103436782956123\n",
      "Epoch 79, Batch 90, Loss: 0.025509601458907127\n",
      "Epoch 79, Batch 100, Loss: 0.019486913457512856\n",
      "Epoch 80, Batch 0, Loss: 0.02520783618092537\n",
      "Epoch 80, Batch 10, Loss: 0.02612879127264023\n",
      "Epoch 80, Batch 20, Loss: 0.02443399652838707\n",
      "Epoch 80, Batch 30, Loss: 0.017901429906487465\n",
      "Epoch 80, Batch 40, Loss: 0.03012312762439251\n",
      "Epoch 80, Batch 50, Loss: 0.016866998746991158\n",
      "Epoch 80, Batch 60, Loss: 0.027457047253847122\n",
      "Epoch 80, Batch 70, Loss: 0.02218419499695301\n",
      "Epoch 80, Batch 80, Loss: 0.02911856397986412\n",
      "Epoch 80, Batch 90, Loss: 0.028154592961072922\n",
      "Epoch 80, Batch 100, Loss: 0.020558802410960197\n",
      "Epoch 81, Batch 0, Loss: 0.027345119044184685\n",
      "Epoch 81, Batch 10, Loss: 0.024848386645317078\n",
      "Epoch 81, Batch 20, Loss: 0.017732711508870125\n",
      "Epoch 81, Batch 30, Loss: 0.03099486045539379\n",
      "Epoch 81, Batch 40, Loss: 0.015836002305150032\n",
      "Epoch 81, Batch 50, Loss: 0.023677032440900803\n",
      "Epoch 81, Batch 60, Loss: 0.019199397414922714\n",
      "Epoch 81, Batch 70, Loss: 0.03401971235871315\n",
      "Epoch 81, Batch 80, Loss: 0.027649328112602234\n",
      "Epoch 81, Batch 90, Loss: 0.02237938903272152\n",
      "Epoch 81, Batch 100, Loss: 0.01941807195544243\n",
      "Epoch 82, Batch 0, Loss: 0.01713922806084156\n",
      "Epoch 82, Batch 10, Loss: 0.027367064729332924\n",
      "Epoch 82, Batch 20, Loss: 0.028553035110235214\n",
      "Epoch 82, Batch 30, Loss: 0.023333795368671417\n",
      "Epoch 82, Batch 40, Loss: 0.022409377619624138\n",
      "Epoch 82, Batch 50, Loss: 0.021494723856449127\n",
      "Epoch 82, Batch 60, Loss: 0.0187336727976799\n",
      "Epoch 82, Batch 70, Loss: 0.026424270123243332\n",
      "Epoch 82, Batch 80, Loss: 0.02214839495718479\n",
      "Epoch 82, Batch 90, Loss: 0.017876647412776947\n",
      "Epoch 82, Batch 100, Loss: 0.020909316837787628\n",
      "Epoch 83, Batch 0, Loss: 0.03432042896747589\n",
      "Epoch 83, Batch 10, Loss: 0.021301496773958206\n",
      "Epoch 83, Batch 20, Loss: 0.031104441732168198\n",
      "Epoch 83, Batch 30, Loss: 0.024406233802437782\n",
      "Epoch 83, Batch 40, Loss: 0.02440553531050682\n",
      "Epoch 83, Batch 50, Loss: 0.02487741969525814\n",
      "Epoch 83, Batch 60, Loss: 0.024357743561267853\n",
      "Epoch 83, Batch 70, Loss: 0.023182038217782974\n",
      "Epoch 83, Batch 80, Loss: 0.027006154879927635\n",
      "Epoch 83, Batch 90, Loss: 0.020000632852315903\n",
      "Epoch 83, Batch 100, Loss: 0.028658827766776085\n",
      "Epoch 84, Batch 0, Loss: 0.02941550873219967\n",
      "Epoch 84, Batch 10, Loss: 0.033294931054115295\n",
      "Epoch 84, Batch 20, Loss: 0.026481453329324722\n",
      "Epoch 84, Batch 30, Loss: 0.02890995517373085\n",
      "Epoch 84, Batch 40, Loss: 0.0282946415245533\n",
      "Epoch 84, Batch 50, Loss: 0.015833061188459396\n",
      "Epoch 84, Batch 60, Loss: 0.02680724300444126\n",
      "Epoch 84, Batch 70, Loss: 0.029610756784677505\n",
      "Epoch 84, Batch 80, Loss: 0.025408407673239708\n",
      "Epoch 84, Batch 90, Loss: 0.02302139438688755\n",
      "Epoch 84, Batch 100, Loss: 0.028177956119179726\n",
      "Epoch 85, Batch 0, Loss: 0.02452462539076805\n",
      "Epoch 85, Batch 10, Loss: 0.025723736733198166\n",
      "Epoch 85, Batch 20, Loss: 0.0207301527261734\n",
      "Epoch 85, Batch 30, Loss: 0.02702212706208229\n",
      "Epoch 85, Batch 40, Loss: 0.024237439036369324\n",
      "Epoch 85, Batch 50, Loss: 0.020256832242012024\n",
      "Epoch 85, Batch 60, Loss: 0.025869060307741165\n",
      "Epoch 85, Batch 70, Loss: 0.025537922978401184\n",
      "Epoch 85, Batch 80, Loss: 0.024390187114477158\n",
      "Epoch 85, Batch 90, Loss: 0.026981737464666367\n",
      "Epoch 85, Batch 100, Loss: 0.023488909006118774\n",
      "Epoch 86, Batch 0, Loss: 0.026136038824915886\n",
      "Epoch 86, Batch 10, Loss: 0.027977777644991875\n",
      "Epoch 86, Batch 20, Loss: 0.02194514311850071\n",
      "Epoch 86, Batch 30, Loss: 0.02614264003932476\n",
      "Epoch 86, Batch 40, Loss: 0.023287655785679817\n",
      "Epoch 86, Batch 50, Loss: 0.02463054470717907\n",
      "Epoch 86, Batch 60, Loss: 0.024936703965067863\n",
      "Epoch 86, Batch 70, Loss: 0.020759159699082375\n",
      "Epoch 86, Batch 80, Loss: 0.02989690564572811\n",
      "Epoch 86, Batch 90, Loss: 0.02386474609375\n",
      "Epoch 86, Batch 100, Loss: 0.02314765378832817\n",
      "Epoch 87, Batch 0, Loss: 0.02608223631978035\n",
      "Epoch 87, Batch 10, Loss: 0.03135541453957558\n",
      "Epoch 87, Batch 20, Loss: 0.023808710277080536\n",
      "Epoch 87, Batch 30, Loss: 0.018600400537252426\n",
      "Epoch 87, Batch 40, Loss: 0.025860188528895378\n",
      "Epoch 87, Batch 50, Loss: 0.022418860346078873\n",
      "Epoch 87, Batch 60, Loss: 0.023423170670866966\n",
      "Epoch 87, Batch 70, Loss: 0.02271440625190735\n",
      "Epoch 87, Batch 80, Loss: 0.020569290965795517\n",
      "Epoch 87, Batch 90, Loss: 0.028870083391666412\n",
      "Epoch 87, Batch 100, Loss: 0.015136651694774628\n",
      "Epoch 88, Batch 0, Loss: 0.03320670127868652\n",
      "Epoch 88, Batch 10, Loss: 0.029455488547682762\n",
      "Epoch 88, Batch 20, Loss: 0.024020085111260414\n",
      "Epoch 88, Batch 30, Loss: 0.023019716143608093\n",
      "Epoch 88, Batch 40, Loss: 0.019402146339416504\n",
      "Epoch 88, Batch 50, Loss: 0.020611396059393883\n",
      "Epoch 88, Batch 60, Loss: 0.021093636751174927\n",
      "Epoch 88, Batch 70, Loss: 0.027840331196784973\n",
      "Epoch 88, Batch 80, Loss: 0.03291407227516174\n",
      "Epoch 88, Batch 90, Loss: 0.021366527304053307\n",
      "Epoch 88, Batch 100, Loss: 0.02153702825307846\n",
      "Epoch 89, Batch 0, Loss: 0.026122311130166054\n",
      "Epoch 89, Batch 10, Loss: 0.0338476188480854\n",
      "Epoch 89, Batch 20, Loss: 0.02117670699954033\n",
      "Epoch 89, Batch 30, Loss: 0.0279674232006073\n",
      "Epoch 89, Batch 40, Loss: 0.023216182366013527\n",
      "Epoch 89, Batch 50, Loss: 0.024589968845248222\n",
      "Epoch 89, Batch 60, Loss: 0.0219972413033247\n",
      "Epoch 89, Batch 70, Loss: 0.024987399578094482\n",
      "Epoch 89, Batch 80, Loss: 0.028276225551962852\n",
      "Epoch 89, Batch 90, Loss: 0.01922784000635147\n",
      "Epoch 89, Batch 100, Loss: 0.018026193603873253\n",
      "Epoch 90, Batch 0, Loss: 0.021893072873353958\n",
      "Epoch 90, Batch 10, Loss: 0.03425661474466324\n",
      "Epoch 90, Batch 20, Loss: 0.018347682431340218\n",
      "Epoch 90, Batch 30, Loss: 0.024392081424593925\n",
      "Epoch 90, Batch 40, Loss: 0.018814973533153534\n",
      "Epoch 90, Batch 50, Loss: 0.024863211438059807\n",
      "Epoch 90, Batch 60, Loss: 0.022440636530518532\n",
      "Epoch 90, Batch 70, Loss: 0.030088134109973907\n",
      "Epoch 90, Batch 80, Loss: 0.025463342666625977\n",
      "Epoch 90, Batch 90, Loss: 0.02498764358460903\n",
      "Epoch 90, Batch 100, Loss: 0.015428929589688778\n",
      "Epoch 91, Batch 0, Loss: 0.030715567991137505\n",
      "Epoch 91, Batch 10, Loss: 0.02579125389456749\n",
      "Epoch 91, Batch 20, Loss: 0.023334907367825508\n",
      "Epoch 91, Batch 30, Loss: 0.0215945765376091\n",
      "Epoch 91, Batch 40, Loss: 0.04031158611178398\n",
      "Epoch 91, Batch 50, Loss: 0.024864142760634422\n",
      "Epoch 91, Batch 60, Loss: 0.02457498013973236\n",
      "Epoch 91, Batch 70, Loss: 0.024792401120066643\n",
      "Epoch 91, Batch 80, Loss: 0.021637002006173134\n",
      "Epoch 91, Batch 90, Loss: 0.027924275025725365\n",
      "Epoch 91, Batch 100, Loss: 0.021279528737068176\n",
      "Epoch 92, Batch 0, Loss: 0.018644675612449646\n",
      "Epoch 92, Batch 10, Loss: 0.017580028623342514\n",
      "Epoch 92, Batch 20, Loss: 0.01815180853009224\n",
      "Epoch 92, Batch 30, Loss: 0.019013818353414536\n",
      "Epoch 92, Batch 40, Loss: 0.017299819737672806\n",
      "Epoch 92, Batch 50, Loss: 0.021247193217277527\n",
      "Epoch 92, Batch 60, Loss: 0.020890096202492714\n",
      "Epoch 92, Batch 70, Loss: 0.027507934719324112\n",
      "Epoch 92, Batch 80, Loss: 0.02247701585292816\n",
      "Epoch 92, Batch 90, Loss: 0.029101068153977394\n",
      "Epoch 92, Batch 100, Loss: 0.01771613024175167\n",
      "Epoch 93, Batch 0, Loss: 0.028041882440447807\n",
      "Epoch 93, Batch 10, Loss: 0.026348324492573738\n",
      "Epoch 93, Batch 20, Loss: 0.028135502710938454\n",
      "Epoch 93, Batch 30, Loss: 0.02159820683300495\n",
      "Epoch 93, Batch 40, Loss: 0.020605875179171562\n",
      "Epoch 93, Batch 50, Loss: 0.021954862400889397\n",
      "Epoch 93, Batch 60, Loss: 0.0306992270052433\n",
      "Epoch 93, Batch 70, Loss: 0.019999703392386436\n",
      "Epoch 93, Batch 80, Loss: 0.02864885702729225\n",
      "Epoch 93, Batch 90, Loss: 0.022351838648319244\n",
      "Epoch 93, Batch 100, Loss: 0.032901689410209656\n",
      "Epoch 94, Batch 0, Loss: 0.02561991661787033\n",
      "Epoch 94, Batch 10, Loss: 0.032016802579164505\n",
      "Epoch 94, Batch 20, Loss: 0.019085921347141266\n",
      "Epoch 94, Batch 30, Loss: 0.02298586443066597\n",
      "Epoch 94, Batch 40, Loss: 0.021939020603895187\n",
      "Epoch 94, Batch 50, Loss: 0.014899910427629948\n",
      "Epoch 94, Batch 60, Loss: 0.019798574969172478\n",
      "Epoch 94, Batch 70, Loss: 0.021640947088599205\n",
      "Epoch 94, Batch 80, Loss: 0.017766883596777916\n",
      "Epoch 94, Batch 90, Loss: 0.022656841203570366\n",
      "Epoch 94, Batch 100, Loss: 0.0243435800075531\n",
      "Epoch 95, Batch 0, Loss: 0.021659748628735542\n",
      "Epoch 95, Batch 10, Loss: 0.022988535463809967\n",
      "Epoch 95, Batch 20, Loss: 0.020244386047124863\n",
      "Epoch 95, Batch 30, Loss: 0.025975441560149193\n",
      "Epoch 95, Batch 40, Loss: 0.02858399599790573\n",
      "Epoch 95, Batch 50, Loss: 0.02712365798652172\n",
      "Epoch 95, Batch 60, Loss: 0.025351859629154205\n",
      "Epoch 95, Batch 70, Loss: 0.022228257730603218\n",
      "Epoch 95, Batch 80, Loss: 0.025717925280332565\n",
      "Epoch 95, Batch 90, Loss: 0.020105993375182152\n",
      "Epoch 95, Batch 100, Loss: 0.032812558114528656\n",
      "Epoch 96, Batch 0, Loss: 0.01929151453077793\n",
      "Epoch 96, Batch 10, Loss: 0.02577722817659378\n",
      "Epoch 96, Batch 20, Loss: 0.018558036535978317\n",
      "Epoch 96, Batch 30, Loss: 0.02833884209394455\n",
      "Epoch 96, Batch 40, Loss: 0.0284720566123724\n",
      "Epoch 96, Batch 50, Loss: 0.02966511808335781\n",
      "Epoch 96, Batch 60, Loss: 0.026968959718942642\n",
      "Epoch 96, Batch 70, Loss: 0.023341014981269836\n",
      "Epoch 96, Batch 80, Loss: 0.02428930252790451\n",
      "Epoch 96, Batch 90, Loss: 0.023981362581253052\n",
      "Epoch 96, Batch 100, Loss: 0.023373236879706383\n",
      "Epoch 97, Batch 0, Loss: 0.027941446751356125\n",
      "Epoch 97, Batch 10, Loss: 0.026879703626036644\n",
      "Epoch 97, Batch 20, Loss: 0.02382485568523407\n",
      "Epoch 97, Batch 30, Loss: 0.030664365738630295\n",
      "Epoch 97, Batch 40, Loss: 0.02355394884943962\n",
      "Epoch 97, Batch 50, Loss: 0.01820448227226734\n",
      "Epoch 97, Batch 60, Loss: 0.025531865656375885\n",
      "Epoch 97, Batch 70, Loss: 0.016985734924674034\n",
      "Epoch 97, Batch 80, Loss: 0.020302092656493187\n",
      "Epoch 97, Batch 90, Loss: 0.023708507418632507\n",
      "Epoch 97, Batch 100, Loss: 0.021481402218341827\n",
      "Epoch 98, Batch 0, Loss: 0.020736927166581154\n",
      "Epoch 98, Batch 10, Loss: 0.0175850261002779\n",
      "Epoch 98, Batch 20, Loss: 0.02375483326613903\n",
      "Epoch 98, Batch 30, Loss: 0.025396322831511497\n",
      "Epoch 98, Batch 40, Loss: 0.026655517518520355\n",
      "Epoch 98, Batch 50, Loss: 0.023547183722257614\n",
      "Epoch 98, Batch 60, Loss: 0.02551358938217163\n",
      "Epoch 98, Batch 70, Loss: 0.01950923725962639\n",
      "Epoch 98, Batch 80, Loss: 0.026324059814214706\n",
      "Epoch 98, Batch 90, Loss: 0.02159661054611206\n",
      "Epoch 98, Batch 100, Loss: 0.02360219694674015\n",
      "Epoch 99, Batch 0, Loss: 0.03074546717107296\n",
      "Epoch 99, Batch 10, Loss: 0.022124681621789932\n",
      "Epoch 99, Batch 20, Loss: 0.022191843017935753\n",
      "Epoch 99, Batch 30, Loss: 0.017744002863764763\n",
      "Epoch 99, Batch 40, Loss: 0.02321210503578186\n",
      "Epoch 99, Batch 50, Loss: 0.027354899793863297\n",
      "Epoch 99, Batch 60, Loss: 0.015174446627497673\n",
      "Epoch 99, Batch 70, Loss: 0.024161644279956818\n",
      "Epoch 99, Batch 80, Loss: 0.021843211725354195\n",
      "Epoch 99, Batch 90, Loss: 0.020319480448961258\n",
      "Epoch 99, Batch 100, Loss: 0.026399146765470505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_BILSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (bilstm): BILSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True, bidirectional=True)\n",
       "    (fc1): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.039768; Test RMSE 44.994372\n",
      "\n",
      "Train  MAE: 0.023019; Test  MAE 31.383991\n",
      "\n",
      "Train  R^2: 0.998418; Test  R^2 0.959002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
