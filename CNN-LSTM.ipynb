{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_preprocessing import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6618, 10, 8]) torch.Size([6618, 1]) torch.Size([490, 10, 8]) (490, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, y_scaler, train_loader, test_loader = get_data(\n",
    "            timestep = 10,\n",
    "            batch_size = 64,\n",
    "            y_name = 'Closing price',\n",
    "            train_size = 6627,\n",
    "            path = \"Data/SSE000001.csv\")\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    print (\"GPU device not found.\")\n",
    "\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer parameters from the table\n",
    "        self.conv1d = nn.Conv1d(in_channels= 8, out_channels= 32, kernel_size= 2, padding=\"same\")\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Pooling\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size= 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layer\n",
    "        x = self.conv1d(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64,  fc1_output_size = 16):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        # self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p = self.lstm_dropout)\n",
    "        # self.fc1 = nn.Linear(self.lstm_hidden_size, self.fc1_output_size)\n",
    "        # self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        tan_h_lstm = self.tanh(h_lstm)\n",
    "        # h_dropout = self.dropout(h_lstm)\n",
    "\n",
    "        # h_fc1 = self.fc1(h_lstm)\n",
    "        # h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        # h_fc2 = self.fc2(h_fc1)\n",
    "        # output = h_fc2[:, -1, :]\n",
    "\n",
    "        h_fc1 = self.fc1(tan_h_lstm)\n",
    "        output = h_fc1[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.lstm = LSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x) #[64, 32, 10]\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1) #[64, 10, 32]\n",
    "        x = self.lstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_LSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_LSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(5,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True)\n",
       "    (fc1): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.040136; Test RMSE 52.653338\n",
      "\n",
      "Train  MAE: 0.023155; Test  MAE 36.589050\n",
      "\n",
      "Train  R^2: 0.998406; Test  R^2 0.946165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_num_layers = 1\n",
    "lstm_hidden_size = 64\n",
    "path = \"models/CNN_LSTM_{0}Epoch_{1}Lr_{2}Layer_{3}Size.pt\".format(num_epoch, learning_rate, lstm_num_layers, lstm_hidden_size)\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-BILSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64, lstm_dropout = 0.2, fc1_output_size = 16):\n",
    "        super(BILSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True,\n",
    "                             bidirectional = True)\n",
    "        \n",
    "        # self.fc1 = nn.Linear(self.lstm_hidden_size * 2, self.fc1_output_size)\n",
    "        # self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size * 2, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # h_fc1 = self.fc1(h_lstm)\n",
    "        # h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        # h_fc2 = self.fc2(h_fc1)\n",
    "        # output = h_fc2[:, -1, :]\n",
    "\n",
    "        tan_h_lstm = self.tanh(h_lstm)\n",
    "        h_fc1 = self.fc1(tan_h_lstm)\n",
    "        output = h_fc1[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BILSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BILSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.bilstm = BILSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x)\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Reshape x to fit BiLSTM input requirements\n",
    "        # x = x.contiguous().view(batch_size, seq_len, -1)\n",
    "        x = self.bilstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_BILSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.8165919780731201\n",
      "Epoch 0, Batch 10, Loss: 0.8068506717681885\n",
      "Epoch 0, Batch 20, Loss: 0.7826277017593384\n",
      "Epoch 0, Batch 30, Loss: 0.7410628199577332\n",
      "Epoch 0, Batch 40, Loss: 0.7587448954582214\n",
      "Epoch 0, Batch 50, Loss: 0.6637872457504272\n",
      "Epoch 0, Batch 60, Loss: 0.7033519148826599\n",
      "Epoch 0, Batch 70, Loss: 0.718801736831665\n",
      "Epoch 0, Batch 80, Loss: 0.7241650819778442\n",
      "Epoch 0, Batch 90, Loss: 0.6135603785514832\n",
      "Epoch 0, Batch 100, Loss: 0.5913267731666565\n",
      "Epoch 1, Batch 0, Loss: 0.597760021686554\n",
      "Epoch 1, Batch 10, Loss: 0.5409653186798096\n",
      "Epoch 1, Batch 20, Loss: 0.4647952616214752\n",
      "Epoch 1, Batch 30, Loss: 0.3569648563861847\n",
      "Epoch 1, Batch 40, Loss: 0.24569526314735413\n",
      "Epoch 1, Batch 50, Loss: 0.17313966155052185\n",
      "Epoch 1, Batch 60, Loss: 0.1711079627275467\n",
      "Epoch 1, Batch 70, Loss: 0.1007409617304802\n",
      "Epoch 1, Batch 80, Loss: 0.1458434909582138\n",
      "Epoch 1, Batch 90, Loss: 0.14248357713222504\n",
      "Epoch 1, Batch 100, Loss: 0.12932683527469635\n",
      "Epoch 2, Batch 0, Loss: 0.09442010521888733\n",
      "Epoch 2, Batch 10, Loss: 0.06375542283058167\n",
      "Epoch 2, Batch 20, Loss: 0.07202178239822388\n",
      "Epoch 2, Batch 30, Loss: 0.10061093419790268\n",
      "Epoch 2, Batch 40, Loss: 0.10051952302455902\n",
      "Epoch 2, Batch 50, Loss: 0.12678804993629456\n",
      "Epoch 2, Batch 60, Loss: 0.08306818455457687\n",
      "Epoch 2, Batch 70, Loss: 0.08700599521398544\n",
      "Epoch 2, Batch 80, Loss: 0.10865693539381027\n",
      "Epoch 2, Batch 90, Loss: 0.04102121666073799\n",
      "Epoch 2, Batch 100, Loss: 0.0682973563671112\n",
      "Epoch 3, Batch 0, Loss: 0.12728501856327057\n",
      "Epoch 3, Batch 10, Loss: 0.0613216832280159\n",
      "Epoch 3, Batch 20, Loss: 0.06509872525930405\n",
      "Epoch 3, Batch 30, Loss: 0.08577192574739456\n",
      "Epoch 3, Batch 40, Loss: 0.057319607585668564\n",
      "Epoch 3, Batch 50, Loss: 0.07757549732923508\n",
      "Epoch 3, Batch 60, Loss: 0.06320565193891525\n",
      "Epoch 3, Batch 70, Loss: 0.06861177831888199\n",
      "Epoch 3, Batch 80, Loss: 0.0747789517045021\n",
      "Epoch 3, Batch 90, Loss: 0.0688747987151146\n",
      "Epoch 3, Batch 100, Loss: 0.05435637757182121\n",
      "Epoch 4, Batch 0, Loss: 0.04298980161547661\n",
      "Epoch 4, Batch 10, Loss: 0.0434865839779377\n",
      "Epoch 4, Batch 20, Loss: 0.07267718762159348\n",
      "Epoch 4, Batch 30, Loss: 0.04241251200437546\n",
      "Epoch 4, Batch 40, Loss: 0.06432949751615524\n",
      "Epoch 4, Batch 50, Loss: 0.03737780824303627\n",
      "Epoch 4, Batch 60, Loss: 0.051729463040828705\n",
      "Epoch 4, Batch 70, Loss: 0.043049924075603485\n",
      "Epoch 4, Batch 80, Loss: 0.0772809311747551\n",
      "Epoch 4, Batch 90, Loss: 0.03670923411846161\n",
      "Epoch 4, Batch 100, Loss: 0.03825981169939041\n",
      "Epoch 5, Batch 0, Loss: 0.04127961024641991\n",
      "Epoch 5, Batch 10, Loss: 0.04583398625254631\n",
      "Epoch 5, Batch 20, Loss: 0.037872254848480225\n",
      "Epoch 5, Batch 30, Loss: 0.05796639621257782\n",
      "Epoch 5, Batch 40, Loss: 0.045319605618715286\n",
      "Epoch 5, Batch 50, Loss: 0.05121197924017906\n",
      "Epoch 5, Batch 60, Loss: 0.034682270139455795\n",
      "Epoch 5, Batch 70, Loss: 0.04290899634361267\n",
      "Epoch 5, Batch 80, Loss: 0.043648988008499146\n",
      "Epoch 5, Batch 90, Loss: 0.05506078898906708\n",
      "Epoch 5, Batch 100, Loss: 0.03076085075736046\n",
      "Epoch 6, Batch 0, Loss: 0.04652083292603493\n",
      "Epoch 6, Batch 10, Loss: 0.053973957896232605\n",
      "Epoch 6, Batch 20, Loss: 0.04130229726433754\n",
      "Epoch 6, Batch 30, Loss: 0.041597794741392136\n",
      "Epoch 6, Batch 40, Loss: 0.025564715266227722\n",
      "Epoch 6, Batch 50, Loss: 0.041527725756168365\n",
      "Epoch 6, Batch 60, Loss: 0.030888456851243973\n",
      "Epoch 6, Batch 70, Loss: 0.03445228189229965\n",
      "Epoch 6, Batch 80, Loss: 0.042725447565317154\n",
      "Epoch 6, Batch 90, Loss: 0.0382593609392643\n",
      "Epoch 6, Batch 100, Loss: 0.05765395611524582\n",
      "Epoch 7, Batch 0, Loss: 0.04299745708703995\n",
      "Epoch 7, Batch 10, Loss: 0.03271230310201645\n",
      "Epoch 7, Batch 20, Loss: 0.05502529814839363\n",
      "Epoch 7, Batch 30, Loss: 0.03340308740735054\n",
      "Epoch 7, Batch 40, Loss: 0.034230317920446396\n",
      "Epoch 7, Batch 50, Loss: 0.04013428837060928\n",
      "Epoch 7, Batch 60, Loss: 0.03129342943429947\n",
      "Epoch 7, Batch 70, Loss: 0.02484312653541565\n",
      "Epoch 7, Batch 80, Loss: 0.040753573179244995\n",
      "Epoch 7, Batch 90, Loss: 0.03246421739459038\n",
      "Epoch 7, Batch 100, Loss: 0.03882822394371033\n",
      "Epoch 8, Batch 0, Loss: 0.03606242313981056\n",
      "Epoch 8, Batch 10, Loss: 0.030068423599004745\n",
      "Epoch 8, Batch 20, Loss: 0.027260106056928635\n",
      "Epoch 8, Batch 30, Loss: 0.039551690220832825\n",
      "Epoch 8, Batch 40, Loss: 0.03383645787835121\n",
      "Epoch 8, Batch 50, Loss: 0.04463183134794235\n",
      "Epoch 8, Batch 60, Loss: 0.04334700107574463\n",
      "Epoch 8, Batch 70, Loss: 0.0258611012250185\n",
      "Epoch 8, Batch 80, Loss: 0.05576470494270325\n",
      "Epoch 8, Batch 90, Loss: 0.03685382008552551\n",
      "Epoch 8, Batch 100, Loss: 0.031010806560516357\n",
      "Epoch 9, Batch 0, Loss: 0.03379826247692108\n",
      "Epoch 9, Batch 10, Loss: 0.04725779965519905\n",
      "Epoch 9, Batch 20, Loss: 0.02558812126517296\n",
      "Epoch 9, Batch 30, Loss: 0.028613988310098648\n",
      "Epoch 9, Batch 40, Loss: 0.039822615683078766\n",
      "Epoch 9, Batch 50, Loss: 0.0466570220887661\n",
      "Epoch 9, Batch 60, Loss: 0.03525684028863907\n",
      "Epoch 9, Batch 70, Loss: 0.022617686539888382\n",
      "Epoch 9, Batch 80, Loss: 0.023731011897325516\n",
      "Epoch 9, Batch 90, Loss: 0.033071279525756836\n",
      "Epoch 9, Batch 100, Loss: 0.027598783373832703\n",
      "Epoch 10, Batch 0, Loss: 0.038640547543764114\n",
      "Epoch 10, Batch 10, Loss: 0.032008249312639236\n",
      "Epoch 10, Batch 20, Loss: 0.031052984297275543\n",
      "Epoch 10, Batch 30, Loss: 0.036851342767477036\n",
      "Epoch 10, Batch 40, Loss: 0.026874013245105743\n",
      "Epoch 10, Batch 50, Loss: 0.034059926867485046\n",
      "Epoch 10, Batch 60, Loss: 0.031405046582221985\n",
      "Epoch 10, Batch 70, Loss: 0.025514762848615646\n",
      "Epoch 10, Batch 80, Loss: 0.029651764780282974\n",
      "Epoch 10, Batch 90, Loss: 0.03191707655787468\n",
      "Epoch 10, Batch 100, Loss: 0.03582274913787842\n",
      "Epoch 11, Batch 0, Loss: 0.032282814383506775\n",
      "Epoch 11, Batch 10, Loss: 0.03607737272977829\n",
      "Epoch 11, Batch 20, Loss: 0.026009945198893547\n",
      "Epoch 11, Batch 30, Loss: 0.031470295041799545\n",
      "Epoch 11, Batch 40, Loss: 0.02727707102894783\n",
      "Epoch 11, Batch 50, Loss: 0.030999019742012024\n",
      "Epoch 11, Batch 60, Loss: 0.028614329174160957\n",
      "Epoch 11, Batch 70, Loss: 0.02189655601978302\n",
      "Epoch 11, Batch 80, Loss: 0.03726779296994209\n",
      "Epoch 11, Batch 90, Loss: 0.02833804115653038\n",
      "Epoch 11, Batch 100, Loss: 0.056094251573085785\n",
      "Epoch 12, Batch 0, Loss: 0.028889968991279602\n",
      "Epoch 12, Batch 10, Loss: 0.029750511050224304\n",
      "Epoch 12, Batch 20, Loss: 0.031832437962293625\n",
      "Epoch 12, Batch 30, Loss: 0.033858902752399445\n",
      "Epoch 12, Batch 40, Loss: 0.025934556499123573\n",
      "Epoch 12, Batch 50, Loss: 0.0306997112929821\n",
      "Epoch 12, Batch 60, Loss: 0.02598506212234497\n",
      "Epoch 12, Batch 70, Loss: 0.027604924514889717\n",
      "Epoch 12, Batch 80, Loss: 0.040995195508003235\n",
      "Epoch 12, Batch 90, Loss: 0.02664860710501671\n",
      "Epoch 12, Batch 100, Loss: 0.023133207112550735\n",
      "Epoch 13, Batch 0, Loss: 0.027491847053170204\n",
      "Epoch 13, Batch 10, Loss: 0.0239095501601696\n",
      "Epoch 13, Batch 20, Loss: 0.03567216545343399\n",
      "Epoch 13, Batch 30, Loss: 0.02568083256483078\n",
      "Epoch 13, Batch 40, Loss: 0.03976675868034363\n",
      "Epoch 13, Batch 50, Loss: 0.030618606135249138\n",
      "Epoch 13, Batch 60, Loss: 0.03701933100819588\n",
      "Epoch 13, Batch 70, Loss: 0.024999964982271194\n",
      "Epoch 13, Batch 80, Loss: 0.03303280845284462\n",
      "Epoch 13, Batch 90, Loss: 0.02918347343802452\n",
      "Epoch 13, Batch 100, Loss: 0.02967512235045433\n",
      "Epoch 14, Batch 0, Loss: 0.02880023419857025\n",
      "Epoch 14, Batch 10, Loss: 0.026154812425374985\n",
      "Epoch 14, Batch 20, Loss: 0.031343258917331696\n",
      "Epoch 14, Batch 30, Loss: 0.02669772505760193\n",
      "Epoch 14, Batch 40, Loss: 0.019737381488084793\n",
      "Epoch 14, Batch 50, Loss: 0.026097506284713745\n",
      "Epoch 14, Batch 60, Loss: 0.020704643800854683\n",
      "Epoch 14, Batch 70, Loss: 0.03179292008280754\n",
      "Epoch 14, Batch 80, Loss: 0.027968840673565865\n",
      "Epoch 14, Batch 90, Loss: 0.028522368520498276\n",
      "Epoch 14, Batch 100, Loss: 0.02105274423956871\n",
      "Epoch 15, Batch 0, Loss: 0.019732315093278885\n",
      "Epoch 15, Batch 10, Loss: 0.027867969125509262\n",
      "Epoch 15, Batch 20, Loss: 0.02438417449593544\n",
      "Epoch 15, Batch 30, Loss: 0.029899362474679947\n",
      "Epoch 15, Batch 40, Loss: 0.02767392247915268\n",
      "Epoch 15, Batch 50, Loss: 0.02946097031235695\n",
      "Epoch 15, Batch 60, Loss: 0.021848103031516075\n",
      "Epoch 15, Batch 70, Loss: 0.018435576930642128\n",
      "Epoch 15, Batch 80, Loss: 0.02843661606311798\n",
      "Epoch 15, Batch 90, Loss: 0.02330324426293373\n",
      "Epoch 15, Batch 100, Loss: 0.02541184052824974\n",
      "Epoch 16, Batch 0, Loss: 0.025983070954680443\n",
      "Epoch 16, Batch 10, Loss: 0.02435368113219738\n",
      "Epoch 16, Batch 20, Loss: 0.01916491985321045\n",
      "Epoch 16, Batch 30, Loss: 0.029478348791599274\n",
      "Epoch 16, Batch 40, Loss: 0.02422010339796543\n",
      "Epoch 16, Batch 50, Loss: 0.034334804862737656\n",
      "Epoch 16, Batch 60, Loss: 0.02836460806429386\n",
      "Epoch 16, Batch 70, Loss: 0.026117999106645584\n",
      "Epoch 16, Batch 80, Loss: 0.03519650548696518\n",
      "Epoch 16, Batch 90, Loss: 0.02140693925321102\n",
      "Epoch 16, Batch 100, Loss: 0.02667160890996456\n",
      "Epoch 17, Batch 0, Loss: 0.022996686398983\n",
      "Epoch 17, Batch 10, Loss: 0.02606312185525894\n",
      "Epoch 17, Batch 20, Loss: 0.027059786021709442\n",
      "Epoch 17, Batch 30, Loss: 0.017633438110351562\n",
      "Epoch 17, Batch 40, Loss: 0.026007022708654404\n",
      "Epoch 17, Batch 50, Loss: 0.026826445013284683\n",
      "Epoch 17, Batch 60, Loss: 0.02702074497938156\n",
      "Epoch 17, Batch 70, Loss: 0.03157281503081322\n",
      "Epoch 17, Batch 80, Loss: 0.020449865609407425\n",
      "Epoch 17, Batch 90, Loss: 0.02631905861198902\n",
      "Epoch 17, Batch 100, Loss: 0.03242594376206398\n",
      "Epoch 18, Batch 0, Loss: 0.024871937930583954\n",
      "Epoch 18, Batch 10, Loss: 0.02129506878554821\n",
      "Epoch 18, Batch 20, Loss: 0.023123839870095253\n",
      "Epoch 18, Batch 30, Loss: 0.024407900869846344\n",
      "Epoch 18, Batch 40, Loss: 0.019546959549188614\n",
      "Epoch 18, Batch 50, Loss: 0.028961122035980225\n",
      "Epoch 18, Batch 60, Loss: 0.03370184451341629\n",
      "Epoch 18, Batch 70, Loss: 0.03170577436685562\n",
      "Epoch 18, Batch 80, Loss: 0.030160101130604744\n",
      "Epoch 18, Batch 90, Loss: 0.04446256160736084\n",
      "Epoch 18, Batch 100, Loss: 0.02674022689461708\n",
      "Epoch 19, Batch 0, Loss: 0.022135797888040543\n",
      "Epoch 19, Batch 10, Loss: 0.02365940250456333\n",
      "Epoch 19, Batch 20, Loss: 0.024882664903998375\n",
      "Epoch 19, Batch 30, Loss: 0.02851274237036705\n",
      "Epoch 19, Batch 40, Loss: 0.03203502297401428\n",
      "Epoch 19, Batch 50, Loss: 0.024953175336122513\n",
      "Epoch 19, Batch 60, Loss: 0.0192768182605505\n",
      "Epoch 19, Batch 70, Loss: 0.021482067182660103\n",
      "Epoch 19, Batch 80, Loss: 0.02833130955696106\n",
      "Epoch 19, Batch 90, Loss: 0.025075538083910942\n",
      "Epoch 19, Batch 100, Loss: 0.03025864250957966\n",
      "Epoch 20, Batch 0, Loss: 0.033914193511009216\n",
      "Epoch 20, Batch 10, Loss: 0.03719320893287659\n",
      "Epoch 20, Batch 20, Loss: 0.02600480243563652\n",
      "Epoch 20, Batch 30, Loss: 0.027346722781658173\n",
      "Epoch 20, Batch 40, Loss: 0.027971897274255753\n",
      "Epoch 20, Batch 50, Loss: 0.02248818799853325\n",
      "Epoch 20, Batch 60, Loss: 0.03917418792843819\n",
      "Epoch 20, Batch 70, Loss: 0.018103664740920067\n",
      "Epoch 20, Batch 80, Loss: 0.023686964064836502\n",
      "Epoch 20, Batch 90, Loss: 0.025529049336910248\n",
      "Epoch 20, Batch 100, Loss: 0.030724897980690002\n",
      "Epoch 21, Batch 0, Loss: 0.03295554965734482\n",
      "Epoch 21, Batch 10, Loss: 0.031318679451942444\n",
      "Epoch 21, Batch 20, Loss: 0.03059377893805504\n",
      "Epoch 21, Batch 30, Loss: 0.0262653399258852\n",
      "Epoch 21, Batch 40, Loss: 0.030894659459590912\n",
      "Epoch 21, Batch 50, Loss: 0.034584324806928635\n",
      "Epoch 21, Batch 60, Loss: 0.03066823072731495\n",
      "Epoch 21, Batch 70, Loss: 0.02763965353369713\n",
      "Epoch 21, Batch 80, Loss: 0.030646272003650665\n",
      "Epoch 21, Batch 90, Loss: 0.028230564668774605\n",
      "Epoch 21, Batch 100, Loss: 0.023147262632846832\n",
      "Epoch 22, Batch 0, Loss: 0.01933375932276249\n",
      "Epoch 22, Batch 10, Loss: 0.02745332568883896\n",
      "Epoch 22, Batch 20, Loss: 0.021301820874214172\n",
      "Epoch 22, Batch 30, Loss: 0.01657409593462944\n",
      "Epoch 22, Batch 40, Loss: 0.03267960622906685\n",
      "Epoch 22, Batch 50, Loss: 0.027384620159864426\n",
      "Epoch 22, Batch 60, Loss: 0.02628335729241371\n",
      "Epoch 22, Batch 70, Loss: 0.02775348164141178\n",
      "Epoch 22, Batch 80, Loss: 0.024451877921819687\n",
      "Epoch 22, Batch 90, Loss: 0.02469906583428383\n",
      "Epoch 22, Batch 100, Loss: 0.024526303634047508\n",
      "Epoch 23, Batch 0, Loss: 0.01823488064110279\n",
      "Epoch 23, Batch 10, Loss: 0.027670076116919518\n",
      "Epoch 23, Batch 20, Loss: 0.02300415001809597\n",
      "Epoch 23, Batch 30, Loss: 0.01978387124836445\n",
      "Epoch 23, Batch 40, Loss: 0.03342629224061966\n",
      "Epoch 23, Batch 50, Loss: 0.027148980647325516\n",
      "Epoch 23, Batch 60, Loss: 0.0271822027862072\n",
      "Epoch 23, Batch 70, Loss: 0.034238867461681366\n",
      "Epoch 23, Batch 80, Loss: 0.03076576441526413\n",
      "Epoch 23, Batch 90, Loss: 0.030903790146112442\n",
      "Epoch 23, Batch 100, Loss: 0.031446803361177444\n",
      "Epoch 24, Batch 0, Loss: 0.023912418633699417\n",
      "Epoch 24, Batch 10, Loss: 0.017798874527215958\n",
      "Epoch 24, Batch 20, Loss: 0.029505101963877678\n",
      "Epoch 24, Batch 30, Loss: 0.028086137026548386\n",
      "Epoch 24, Batch 40, Loss: 0.01885918714106083\n",
      "Epoch 24, Batch 50, Loss: 0.02217998169362545\n",
      "Epoch 24, Batch 60, Loss: 0.030810341238975525\n",
      "Epoch 24, Batch 70, Loss: 0.035108782351017\n",
      "Epoch 24, Batch 80, Loss: 0.02496480382978916\n",
      "Epoch 24, Batch 90, Loss: 0.02377137541770935\n",
      "Epoch 24, Batch 100, Loss: 0.02336680144071579\n",
      "Epoch 25, Batch 0, Loss: 0.030551228672266006\n",
      "Epoch 25, Batch 10, Loss: 0.02189234271645546\n",
      "Epoch 25, Batch 20, Loss: 0.021453116089105606\n",
      "Epoch 25, Batch 30, Loss: 0.02334432676434517\n",
      "Epoch 25, Batch 40, Loss: 0.02614106424152851\n",
      "Epoch 25, Batch 50, Loss: 0.02494126744568348\n",
      "Epoch 25, Batch 60, Loss: 0.022202488034963608\n",
      "Epoch 25, Batch 70, Loss: 0.03560913726687431\n",
      "Epoch 25, Batch 80, Loss: 0.025208694860339165\n",
      "Epoch 25, Batch 90, Loss: 0.028439635410904884\n",
      "Epoch 25, Batch 100, Loss: 0.02398880198597908\n",
      "Epoch 26, Batch 0, Loss: 0.02547086961567402\n",
      "Epoch 26, Batch 10, Loss: 0.022106459364295006\n",
      "Epoch 26, Batch 20, Loss: 0.026687070727348328\n",
      "Epoch 26, Batch 30, Loss: 0.030692128464579582\n",
      "Epoch 26, Batch 40, Loss: 0.020929815247654915\n",
      "Epoch 26, Batch 50, Loss: 0.029010677710175514\n",
      "Epoch 26, Batch 60, Loss: 0.022524764761328697\n",
      "Epoch 26, Batch 70, Loss: 0.035712432116270065\n",
      "Epoch 26, Batch 80, Loss: 0.02337220124900341\n",
      "Epoch 26, Batch 90, Loss: 0.025421200320124626\n",
      "Epoch 26, Batch 100, Loss: 0.02069590613245964\n",
      "Epoch 27, Batch 0, Loss: 0.028834665194153786\n",
      "Epoch 27, Batch 10, Loss: 0.018034417182207108\n",
      "Epoch 27, Batch 20, Loss: 0.02034907042980194\n",
      "Epoch 27, Batch 30, Loss: 0.020110543817281723\n",
      "Epoch 27, Batch 40, Loss: 0.028563305735588074\n",
      "Epoch 27, Batch 50, Loss: 0.022068660706281662\n",
      "Epoch 27, Batch 60, Loss: 0.01952953077852726\n",
      "Epoch 27, Batch 70, Loss: 0.022743836045265198\n",
      "Epoch 27, Batch 80, Loss: 0.02546905353665352\n",
      "Epoch 27, Batch 90, Loss: 0.028115976601839066\n",
      "Epoch 27, Batch 100, Loss: 0.02520470879971981\n",
      "Epoch 28, Batch 0, Loss: 0.02017257548868656\n",
      "Epoch 28, Batch 10, Loss: 0.027342885732650757\n",
      "Epoch 28, Batch 20, Loss: 0.02448694035410881\n",
      "Epoch 28, Batch 30, Loss: 0.026211101561784744\n",
      "Epoch 28, Batch 40, Loss: 0.03146642819046974\n",
      "Epoch 28, Batch 50, Loss: 0.026185527443885803\n",
      "Epoch 28, Batch 60, Loss: 0.023978710174560547\n",
      "Epoch 28, Batch 70, Loss: 0.026974504813551903\n",
      "Epoch 28, Batch 80, Loss: 0.02135728858411312\n",
      "Epoch 28, Batch 90, Loss: 0.031289536505937576\n",
      "Epoch 28, Batch 100, Loss: 0.027185577899217606\n",
      "Epoch 29, Batch 0, Loss: 0.02647026628255844\n",
      "Epoch 29, Batch 10, Loss: 0.024164550006389618\n",
      "Epoch 29, Batch 20, Loss: 0.026165783405303955\n",
      "Epoch 29, Batch 30, Loss: 0.02889372408390045\n",
      "Epoch 29, Batch 40, Loss: 0.027710851281881332\n",
      "Epoch 29, Batch 50, Loss: 0.023056916892528534\n",
      "Epoch 29, Batch 60, Loss: 0.029328910633921623\n",
      "Epoch 29, Batch 70, Loss: 0.02930419147014618\n",
      "Epoch 29, Batch 80, Loss: 0.02769182249903679\n",
      "Epoch 29, Batch 90, Loss: 0.023836037144064903\n",
      "Epoch 29, Batch 100, Loss: 0.025614427402615547\n",
      "Epoch 30, Batch 0, Loss: 0.02441512979567051\n",
      "Epoch 30, Batch 10, Loss: 0.021397609263658524\n",
      "Epoch 30, Batch 20, Loss: 0.030796382576227188\n",
      "Epoch 30, Batch 30, Loss: 0.025376711040735245\n",
      "Epoch 30, Batch 40, Loss: 0.02168886736035347\n",
      "Epoch 30, Batch 50, Loss: 0.024631928652524948\n",
      "Epoch 30, Batch 60, Loss: 0.021455179899930954\n",
      "Epoch 30, Batch 70, Loss: 0.020929094403982162\n",
      "Epoch 30, Batch 80, Loss: 0.02406877465546131\n",
      "Epoch 30, Batch 90, Loss: 0.031091777607798576\n",
      "Epoch 30, Batch 100, Loss: 0.02107931300997734\n",
      "Epoch 31, Batch 0, Loss: 0.026102324947714806\n",
      "Epoch 31, Batch 10, Loss: 0.02333071455359459\n",
      "Epoch 31, Batch 20, Loss: 0.029726754873991013\n",
      "Epoch 31, Batch 30, Loss: 0.028168799355626106\n",
      "Epoch 31, Batch 40, Loss: 0.020500268787145615\n",
      "Epoch 31, Batch 50, Loss: 0.027367208153009415\n",
      "Epoch 31, Batch 60, Loss: 0.023970242589712143\n",
      "Epoch 31, Batch 70, Loss: 0.029743719846010208\n",
      "Epoch 31, Batch 80, Loss: 0.02591656520962715\n",
      "Epoch 31, Batch 90, Loss: 0.032646842300891876\n",
      "Epoch 31, Batch 100, Loss: 0.023717021569609642\n",
      "Epoch 32, Batch 0, Loss: 0.023229144513607025\n",
      "Epoch 32, Batch 10, Loss: 0.033830635249614716\n",
      "Epoch 32, Batch 20, Loss: 0.02774546667933464\n",
      "Epoch 32, Batch 30, Loss: 0.02879764325916767\n",
      "Epoch 32, Batch 40, Loss: 0.027631305158138275\n",
      "Epoch 32, Batch 50, Loss: 0.0183895044028759\n",
      "Epoch 32, Batch 60, Loss: 0.025849346071481705\n",
      "Epoch 32, Batch 70, Loss: 0.02336648665368557\n",
      "Epoch 32, Batch 80, Loss: 0.02612483501434326\n",
      "Epoch 32, Batch 90, Loss: 0.02204643003642559\n",
      "Epoch 32, Batch 100, Loss: 0.022429361939430237\n",
      "Epoch 33, Batch 0, Loss: 0.026977580040693283\n",
      "Epoch 33, Batch 10, Loss: 0.025969844311475754\n",
      "Epoch 33, Batch 20, Loss: 0.0162773709744215\n",
      "Epoch 33, Batch 30, Loss: 0.027816273272037506\n",
      "Epoch 33, Batch 40, Loss: 0.02088187448680401\n",
      "Epoch 33, Batch 50, Loss: 0.02491355501115322\n",
      "Epoch 33, Batch 60, Loss: 0.03160463273525238\n",
      "Epoch 33, Batch 70, Loss: 0.016876565292477608\n",
      "Epoch 33, Batch 80, Loss: 0.024265659973025322\n",
      "Epoch 33, Batch 90, Loss: 0.02745828405022621\n",
      "Epoch 33, Batch 100, Loss: 0.025442907586693764\n",
      "Epoch 34, Batch 0, Loss: 0.0212760791182518\n",
      "Epoch 34, Batch 10, Loss: 0.02420322224497795\n",
      "Epoch 34, Batch 20, Loss: 0.027073616161942482\n",
      "Epoch 34, Batch 30, Loss: 0.029342250898480415\n",
      "Epoch 34, Batch 40, Loss: 0.031465694308280945\n",
      "Epoch 34, Batch 50, Loss: 0.029117204248905182\n",
      "Epoch 34, Batch 60, Loss: 0.025490382686257362\n",
      "Epoch 34, Batch 70, Loss: 0.02885652892291546\n",
      "Epoch 34, Batch 80, Loss: 0.020140182226896286\n",
      "Epoch 34, Batch 90, Loss: 0.02644071914255619\n",
      "Epoch 34, Batch 100, Loss: 0.01900395192205906\n",
      "Epoch 35, Batch 0, Loss: 0.023026714101433754\n",
      "Epoch 35, Batch 10, Loss: 0.0183979831635952\n",
      "Epoch 35, Batch 20, Loss: 0.022271133959293365\n",
      "Epoch 35, Batch 30, Loss: 0.022047344595193863\n",
      "Epoch 35, Batch 40, Loss: 0.024662505835294724\n",
      "Epoch 35, Batch 50, Loss: 0.025991275906562805\n",
      "Epoch 35, Batch 60, Loss: 0.025799550116062164\n",
      "Epoch 35, Batch 70, Loss: 0.02873322181403637\n",
      "Epoch 35, Batch 80, Loss: 0.032129667699337006\n",
      "Epoch 35, Batch 90, Loss: 0.027221141383051872\n",
      "Epoch 35, Batch 100, Loss: 0.025059469044208527\n",
      "Epoch 36, Batch 0, Loss: 0.028754770755767822\n",
      "Epoch 36, Batch 10, Loss: 0.02656475082039833\n",
      "Epoch 36, Batch 20, Loss: 0.028135670349001884\n",
      "Epoch 36, Batch 30, Loss: 0.026836274191737175\n",
      "Epoch 36, Batch 40, Loss: 0.028568293899297714\n",
      "Epoch 36, Batch 50, Loss: 0.020945891737937927\n",
      "Epoch 36, Batch 60, Loss: 0.020867295563220978\n",
      "Epoch 36, Batch 70, Loss: 0.027117745950818062\n",
      "Epoch 36, Batch 80, Loss: 0.025774691253900528\n",
      "Epoch 36, Batch 90, Loss: 0.030393484979867935\n",
      "Epoch 36, Batch 100, Loss: 0.021496281027793884\n",
      "Epoch 37, Batch 0, Loss: 0.0232568196952343\n",
      "Epoch 37, Batch 10, Loss: 0.024207651615142822\n",
      "Epoch 37, Batch 20, Loss: 0.024380210787057877\n",
      "Epoch 37, Batch 30, Loss: 0.019922403618693352\n",
      "Epoch 37, Batch 40, Loss: 0.028608499094843864\n",
      "Epoch 37, Batch 50, Loss: 0.017185986042022705\n",
      "Epoch 37, Batch 60, Loss: 0.0304702278226614\n",
      "Epoch 37, Batch 70, Loss: 0.023884985595941544\n",
      "Epoch 37, Batch 80, Loss: 0.02405574545264244\n",
      "Epoch 37, Batch 90, Loss: 0.027237117290496826\n",
      "Epoch 37, Batch 100, Loss: 0.024018805474042892\n",
      "Epoch 38, Batch 0, Loss: 0.019381970167160034\n",
      "Epoch 38, Batch 10, Loss: 0.021783264353871346\n",
      "Epoch 38, Batch 20, Loss: 0.02182900160551071\n",
      "Epoch 38, Batch 30, Loss: 0.027635706588625908\n",
      "Epoch 38, Batch 40, Loss: 0.020052369683980942\n",
      "Epoch 38, Batch 50, Loss: 0.024189447984099388\n",
      "Epoch 38, Batch 60, Loss: 0.02459479309618473\n",
      "Epoch 38, Batch 70, Loss: 0.02692544274032116\n",
      "Epoch 38, Batch 80, Loss: 0.019737625494599342\n",
      "Epoch 38, Batch 90, Loss: 0.025249844416975975\n",
      "Epoch 38, Batch 100, Loss: 0.020283211022615433\n",
      "Epoch 39, Batch 0, Loss: 0.028521953150629997\n",
      "Epoch 39, Batch 10, Loss: 0.023010941222310066\n",
      "Epoch 39, Batch 20, Loss: 0.02493423782289028\n",
      "Epoch 39, Batch 30, Loss: 0.02052181586623192\n",
      "Epoch 39, Batch 40, Loss: 0.021885287016630173\n",
      "Epoch 39, Batch 50, Loss: 0.022439422085881233\n",
      "Epoch 39, Batch 60, Loss: 0.02452920749783516\n",
      "Epoch 39, Batch 70, Loss: 0.019690554589033127\n",
      "Epoch 39, Batch 80, Loss: 0.03501225262880325\n",
      "Epoch 39, Batch 90, Loss: 0.021148791536688805\n",
      "Epoch 39, Batch 100, Loss: 0.027056830003857613\n",
      "Epoch 40, Batch 0, Loss: 0.025907466188073158\n",
      "Epoch 40, Batch 10, Loss: 0.03283243253827095\n",
      "Epoch 40, Batch 20, Loss: 0.02310383878648281\n",
      "Epoch 40, Batch 30, Loss: 0.024369237944483757\n",
      "Epoch 40, Batch 40, Loss: 0.026631589978933334\n",
      "Epoch 40, Batch 50, Loss: 0.02049773745238781\n",
      "Epoch 40, Batch 60, Loss: 0.026738572865724564\n",
      "Epoch 40, Batch 70, Loss: 0.024290133267641068\n",
      "Epoch 40, Batch 80, Loss: 0.022135302424430847\n",
      "Epoch 40, Batch 90, Loss: 0.023334607481956482\n",
      "Epoch 40, Batch 100, Loss: 0.027576036751270294\n",
      "Epoch 41, Batch 0, Loss: 0.025153178721666336\n",
      "Epoch 41, Batch 10, Loss: 0.01872832328081131\n",
      "Epoch 41, Batch 20, Loss: 0.02601855807006359\n",
      "Epoch 41, Batch 30, Loss: 0.023217596113681793\n",
      "Epoch 41, Batch 40, Loss: 0.022878993302583694\n",
      "Epoch 41, Batch 50, Loss: 0.02116629108786583\n",
      "Epoch 41, Batch 60, Loss: 0.023890968412160873\n",
      "Epoch 41, Batch 70, Loss: 0.017256086692214012\n",
      "Epoch 41, Batch 80, Loss: 0.019085997715592384\n",
      "Epoch 41, Batch 90, Loss: 0.024449728429317474\n",
      "Epoch 41, Batch 100, Loss: 0.018346138298511505\n",
      "Epoch 42, Batch 0, Loss: 0.02334974706172943\n",
      "Epoch 42, Batch 10, Loss: 0.020954346284270287\n",
      "Epoch 42, Batch 20, Loss: 0.028767142444849014\n",
      "Epoch 42, Batch 30, Loss: 0.02636364847421646\n",
      "Epoch 42, Batch 40, Loss: 0.023934410884976387\n",
      "Epoch 42, Batch 50, Loss: 0.03259219229221344\n",
      "Epoch 42, Batch 60, Loss: 0.022381626069545746\n",
      "Epoch 42, Batch 70, Loss: 0.031231578439474106\n",
      "Epoch 42, Batch 80, Loss: 0.0278244037181139\n",
      "Epoch 42, Batch 90, Loss: 0.02425703965127468\n",
      "Epoch 42, Batch 100, Loss: 0.02071392349898815\n",
      "Epoch 43, Batch 0, Loss: 0.023802967742085457\n",
      "Epoch 43, Batch 10, Loss: 0.028434783220291138\n",
      "Epoch 43, Batch 20, Loss: 0.02255893684923649\n",
      "Epoch 43, Batch 30, Loss: 0.033871110528707504\n",
      "Epoch 43, Batch 40, Loss: 0.017023436725139618\n",
      "Epoch 43, Batch 50, Loss: 0.02421722188591957\n",
      "Epoch 43, Batch 60, Loss: 0.026985131204128265\n",
      "Epoch 43, Batch 70, Loss: 0.02222577854990959\n",
      "Epoch 43, Batch 80, Loss: 0.022431015968322754\n",
      "Epoch 43, Batch 90, Loss: 0.01979834958910942\n",
      "Epoch 43, Batch 100, Loss: 0.027000604197382927\n",
      "Epoch 44, Batch 0, Loss: 0.025822384282946587\n",
      "Epoch 44, Batch 10, Loss: 0.022115454077720642\n",
      "Epoch 44, Batch 20, Loss: 0.02359442040324211\n",
      "Epoch 44, Batch 30, Loss: 0.016145091503858566\n",
      "Epoch 44, Batch 40, Loss: 0.021570250391960144\n",
      "Epoch 44, Batch 50, Loss: 0.031729232519865036\n",
      "Epoch 44, Batch 60, Loss: 0.020979948341846466\n",
      "Epoch 44, Batch 70, Loss: 0.03106362745165825\n",
      "Epoch 44, Batch 80, Loss: 0.031751759350299835\n",
      "Epoch 44, Batch 90, Loss: 0.026902170851826668\n",
      "Epoch 44, Batch 100, Loss: 0.024026239290833473\n",
      "Epoch 45, Batch 0, Loss: 0.033651214092969894\n",
      "Epoch 45, Batch 10, Loss: 0.026948334649205208\n",
      "Epoch 45, Batch 20, Loss: 0.02853996679186821\n",
      "Epoch 45, Batch 30, Loss: 0.03449510782957077\n",
      "Epoch 45, Batch 40, Loss: 0.02421370893716812\n",
      "Epoch 45, Batch 50, Loss: 0.021029921248555183\n",
      "Epoch 45, Batch 60, Loss: 0.022403623908758163\n",
      "Epoch 45, Batch 70, Loss: 0.01842489093542099\n",
      "Epoch 45, Batch 80, Loss: 0.01945369876921177\n",
      "Epoch 45, Batch 90, Loss: 0.02277209796011448\n",
      "Epoch 45, Batch 100, Loss: 0.02760450914502144\n",
      "Epoch 46, Batch 0, Loss: 0.027392884716391563\n",
      "Epoch 46, Batch 10, Loss: 0.017415866255760193\n",
      "Epoch 46, Batch 20, Loss: 0.023108433932065964\n",
      "Epoch 46, Batch 30, Loss: 0.02524171583354473\n",
      "Epoch 46, Batch 40, Loss: 0.027898166328668594\n",
      "Epoch 46, Batch 50, Loss: 0.02702762559056282\n",
      "Epoch 46, Batch 60, Loss: 0.017144933342933655\n",
      "Epoch 46, Batch 70, Loss: 0.024801529943943024\n",
      "Epoch 46, Batch 80, Loss: 0.017618892714381218\n",
      "Epoch 46, Batch 90, Loss: 0.021691668778657913\n",
      "Epoch 46, Batch 100, Loss: 0.029880158603191376\n",
      "Epoch 47, Batch 0, Loss: 0.022423777729272842\n",
      "Epoch 47, Batch 10, Loss: 0.022001663222908974\n",
      "Epoch 47, Batch 20, Loss: 0.02520272508263588\n",
      "Epoch 47, Batch 30, Loss: 0.01917363330721855\n",
      "Epoch 47, Batch 40, Loss: 0.0228574201464653\n",
      "Epoch 47, Batch 50, Loss: 0.019198916852474213\n",
      "Epoch 47, Batch 60, Loss: 0.02136535570025444\n",
      "Epoch 47, Batch 70, Loss: 0.022177567705512047\n",
      "Epoch 47, Batch 80, Loss: 0.026207031682133675\n",
      "Epoch 47, Batch 90, Loss: 0.022697091102600098\n",
      "Epoch 47, Batch 100, Loss: 0.026461387053132057\n",
      "Epoch 48, Batch 0, Loss: 0.029348742216825485\n",
      "Epoch 48, Batch 10, Loss: 0.02592182159423828\n",
      "Epoch 48, Batch 20, Loss: 0.029065459966659546\n",
      "Epoch 48, Batch 30, Loss: 0.02060299925506115\n",
      "Epoch 48, Batch 40, Loss: 0.025668496266007423\n",
      "Epoch 48, Batch 50, Loss: 0.022678717970848083\n",
      "Epoch 48, Batch 60, Loss: 0.02507040649652481\n",
      "Epoch 48, Batch 70, Loss: 0.01971832476556301\n",
      "Epoch 48, Batch 80, Loss: 0.02340538799762726\n",
      "Epoch 48, Batch 90, Loss: 0.022215425968170166\n",
      "Epoch 48, Batch 100, Loss: 0.02306506037712097\n",
      "Epoch 49, Batch 0, Loss: 0.031053664162755013\n",
      "Epoch 49, Batch 10, Loss: 0.02059168554842472\n",
      "Epoch 49, Batch 20, Loss: 0.030726321041584015\n",
      "Epoch 49, Batch 30, Loss: 0.024954039603471756\n",
      "Epoch 49, Batch 40, Loss: 0.025409962981939316\n",
      "Epoch 49, Batch 50, Loss: 0.033479832112789154\n",
      "Epoch 49, Batch 60, Loss: 0.02623654715716839\n",
      "Epoch 49, Batch 70, Loss: 0.02564399316906929\n",
      "Epoch 49, Batch 80, Loss: 0.02078704535961151\n",
      "Epoch 49, Batch 90, Loss: 0.031171198934316635\n",
      "Epoch 49, Batch 100, Loss: 0.02358068898320198\n",
      "Epoch 50, Batch 0, Loss: 0.021236056461930275\n",
      "Epoch 50, Batch 10, Loss: 0.021033506840467453\n",
      "Epoch 50, Batch 20, Loss: 0.023251043632626534\n",
      "Epoch 50, Batch 30, Loss: 0.027059093117713928\n",
      "Epoch 50, Batch 40, Loss: 0.031478263437747955\n",
      "Epoch 50, Batch 50, Loss: 0.015076588839292526\n",
      "Epoch 50, Batch 60, Loss: 0.020027149468660355\n",
      "Epoch 50, Batch 70, Loss: 0.02161996439099312\n",
      "Epoch 50, Batch 80, Loss: 0.022877998650074005\n",
      "Epoch 50, Batch 90, Loss: 0.026202984154224396\n",
      "Epoch 50, Batch 100, Loss: 0.019584184512495995\n",
      "Epoch 51, Batch 0, Loss: 0.022059837356209755\n",
      "Epoch 51, Batch 10, Loss: 0.015872158110141754\n",
      "Epoch 51, Batch 20, Loss: 0.019839778542518616\n",
      "Epoch 51, Batch 30, Loss: 0.02890644408762455\n",
      "Epoch 51, Batch 40, Loss: 0.025507913902401924\n",
      "Epoch 51, Batch 50, Loss: 0.02101704478263855\n",
      "Epoch 51, Batch 60, Loss: 0.01987544633448124\n",
      "Epoch 51, Batch 70, Loss: 0.021292669698596\n",
      "Epoch 51, Batch 80, Loss: 0.035008978098630905\n",
      "Epoch 51, Batch 90, Loss: 0.02671976387500763\n",
      "Epoch 51, Batch 100, Loss: 0.028669454157352448\n",
      "Epoch 52, Batch 0, Loss: 0.020120471715927124\n",
      "Epoch 52, Batch 10, Loss: 0.026883326470851898\n",
      "Epoch 52, Batch 20, Loss: 0.02053539827466011\n",
      "Epoch 52, Batch 30, Loss: 0.03130055218935013\n",
      "Epoch 52, Batch 40, Loss: 0.022430291399359703\n",
      "Epoch 52, Batch 50, Loss: 0.026343615725636482\n",
      "Epoch 52, Batch 60, Loss: 0.018354399129748344\n",
      "Epoch 52, Batch 70, Loss: 0.02390117011964321\n",
      "Epoch 52, Batch 80, Loss: 0.030117210000753403\n",
      "Epoch 52, Batch 90, Loss: 0.020487293601036072\n",
      "Epoch 52, Batch 100, Loss: 0.022597262635827065\n",
      "Epoch 53, Batch 0, Loss: 0.030268725007772446\n",
      "Epoch 53, Batch 10, Loss: 0.023605000227689743\n",
      "Epoch 53, Batch 20, Loss: 0.018748974427580833\n",
      "Epoch 53, Batch 30, Loss: 0.023259539157152176\n",
      "Epoch 53, Batch 40, Loss: 0.020681720227003098\n",
      "Epoch 53, Batch 50, Loss: 0.0296466127038002\n",
      "Epoch 53, Batch 60, Loss: 0.02014905959367752\n",
      "Epoch 53, Batch 70, Loss: 0.030215926468372345\n",
      "Epoch 53, Batch 80, Loss: 0.019752616062760353\n",
      "Epoch 53, Batch 90, Loss: 0.02437494322657585\n",
      "Epoch 53, Batch 100, Loss: 0.026643402874469757\n",
      "Epoch 54, Batch 0, Loss: 0.02320023812353611\n",
      "Epoch 54, Batch 10, Loss: 0.03781675174832344\n",
      "Epoch 54, Batch 20, Loss: 0.033625874668359756\n",
      "Epoch 54, Batch 30, Loss: 0.03135792911052704\n",
      "Epoch 54, Batch 40, Loss: 0.02196904644370079\n",
      "Epoch 54, Batch 50, Loss: 0.030109720304608345\n",
      "Epoch 54, Batch 60, Loss: 0.02911403216421604\n",
      "Epoch 54, Batch 70, Loss: 0.022563109174370766\n",
      "Epoch 54, Batch 80, Loss: 0.021065620705485344\n",
      "Epoch 54, Batch 90, Loss: 0.02427477017045021\n",
      "Epoch 54, Batch 100, Loss: 0.018677491694688797\n",
      "Epoch 55, Batch 0, Loss: 0.02196671813726425\n",
      "Epoch 55, Batch 10, Loss: 0.02748623676598072\n",
      "Epoch 55, Batch 20, Loss: 0.026977187022566795\n",
      "Epoch 55, Batch 30, Loss: 0.02114538848400116\n",
      "Epoch 55, Batch 40, Loss: 0.020590536296367645\n",
      "Epoch 55, Batch 50, Loss: 0.019847657531499863\n",
      "Epoch 55, Batch 60, Loss: 0.024465881288051605\n",
      "Epoch 55, Batch 70, Loss: 0.023719601333141327\n",
      "Epoch 55, Batch 80, Loss: 0.019550254568457603\n",
      "Epoch 55, Batch 90, Loss: 0.022059213370084763\n",
      "Epoch 55, Batch 100, Loss: 0.02510840818285942\n",
      "Epoch 56, Batch 0, Loss: 0.02121679112315178\n",
      "Epoch 56, Batch 10, Loss: 0.024242158979177475\n",
      "Epoch 56, Batch 20, Loss: 0.024211905896663666\n",
      "Epoch 56, Batch 30, Loss: 0.024277985095977783\n",
      "Epoch 56, Batch 40, Loss: 0.023998817428946495\n",
      "Epoch 56, Batch 50, Loss: 0.020513290539383888\n",
      "Epoch 56, Batch 60, Loss: 0.014944609254598618\n",
      "Epoch 56, Batch 70, Loss: 0.024131594225764275\n",
      "Epoch 56, Batch 80, Loss: 0.026216648519039154\n",
      "Epoch 56, Batch 90, Loss: 0.027377421036362648\n",
      "Epoch 56, Batch 100, Loss: 0.024270614609122276\n",
      "Epoch 57, Batch 0, Loss: 0.02320774272084236\n",
      "Epoch 57, Batch 10, Loss: 0.028890622779726982\n",
      "Epoch 57, Batch 20, Loss: 0.024005062878131866\n",
      "Epoch 57, Batch 30, Loss: 0.019018404185771942\n",
      "Epoch 57, Batch 40, Loss: 0.016973650082945824\n",
      "Epoch 57, Batch 50, Loss: 0.025750916451215744\n",
      "Epoch 57, Batch 60, Loss: 0.017537599429488182\n",
      "Epoch 57, Batch 70, Loss: 0.018081601709127426\n",
      "Epoch 57, Batch 80, Loss: 0.02353859692811966\n",
      "Epoch 57, Batch 90, Loss: 0.033623598515987396\n",
      "Epoch 57, Batch 100, Loss: 0.024098945781588554\n",
      "Epoch 58, Batch 0, Loss: 0.019649697467684746\n",
      "Epoch 58, Batch 10, Loss: 0.022979576140642166\n",
      "Epoch 58, Batch 20, Loss: 0.021026303991675377\n",
      "Epoch 58, Batch 30, Loss: 0.021820537745952606\n",
      "Epoch 58, Batch 40, Loss: 0.0190306156873703\n",
      "Epoch 58, Batch 50, Loss: 0.026749689131975174\n",
      "Epoch 58, Batch 60, Loss: 0.027641238644719124\n",
      "Epoch 58, Batch 70, Loss: 0.01886695995926857\n",
      "Epoch 58, Batch 80, Loss: 0.0252671055495739\n",
      "Epoch 58, Batch 90, Loss: 0.02153792418539524\n",
      "Epoch 58, Batch 100, Loss: 0.020601607859134674\n",
      "Epoch 59, Batch 0, Loss: 0.028854085132479668\n",
      "Epoch 59, Batch 10, Loss: 0.028055571019649506\n",
      "Epoch 59, Batch 20, Loss: 0.020504353567957878\n",
      "Epoch 59, Batch 30, Loss: 0.023215893656015396\n",
      "Epoch 59, Batch 40, Loss: 0.02217317745089531\n",
      "Epoch 59, Batch 50, Loss: 0.02389346994459629\n",
      "Epoch 59, Batch 60, Loss: 0.022968506440520287\n",
      "Epoch 59, Batch 70, Loss: 0.02665085345506668\n",
      "Epoch 59, Batch 80, Loss: 0.031610697507858276\n",
      "Epoch 59, Batch 90, Loss: 0.023178543895483017\n",
      "Epoch 59, Batch 100, Loss: 0.021158896386623383\n",
      "Epoch 60, Batch 0, Loss: 0.022416234016418457\n",
      "Epoch 60, Batch 10, Loss: 0.03151456266641617\n",
      "Epoch 60, Batch 20, Loss: 0.02322147786617279\n",
      "Epoch 60, Batch 30, Loss: 0.025103624910116196\n",
      "Epoch 60, Batch 40, Loss: 0.025985904037952423\n",
      "Epoch 60, Batch 50, Loss: 0.027234183624386787\n",
      "Epoch 60, Batch 60, Loss: 0.023669712245464325\n",
      "Epoch 60, Batch 70, Loss: 0.024651672691106796\n",
      "Epoch 60, Batch 80, Loss: 0.023647308349609375\n",
      "Epoch 60, Batch 90, Loss: 0.025716129690408707\n",
      "Epoch 60, Batch 100, Loss: 0.024547826498746872\n",
      "Epoch 61, Batch 0, Loss: 0.01754630357027054\n",
      "Epoch 61, Batch 10, Loss: 0.028672248125076294\n",
      "Epoch 61, Batch 20, Loss: 0.02411133237183094\n",
      "Epoch 61, Batch 30, Loss: 0.02269604057073593\n",
      "Epoch 61, Batch 40, Loss: 0.03764999285340309\n",
      "Epoch 61, Batch 50, Loss: 0.02611774206161499\n",
      "Epoch 61, Batch 60, Loss: 0.01935333013534546\n",
      "Epoch 61, Batch 70, Loss: 0.021319381892681122\n",
      "Epoch 61, Batch 80, Loss: 0.02986161969602108\n",
      "Epoch 61, Batch 90, Loss: 0.019158590584993362\n",
      "Epoch 61, Batch 100, Loss: 0.02724129892885685\n",
      "Epoch 62, Batch 0, Loss: 0.01901313103735447\n",
      "Epoch 62, Batch 10, Loss: 0.021879326552152634\n",
      "Epoch 62, Batch 20, Loss: 0.02477184310555458\n",
      "Epoch 62, Batch 30, Loss: 0.019467270001769066\n",
      "Epoch 62, Batch 40, Loss: 0.02821989171206951\n",
      "Epoch 62, Batch 50, Loss: 0.03171755373477936\n",
      "Epoch 62, Batch 60, Loss: 0.022876467555761337\n",
      "Epoch 62, Batch 70, Loss: 0.024086162447929382\n",
      "Epoch 62, Batch 80, Loss: 0.022902416065335274\n",
      "Epoch 62, Batch 90, Loss: 0.024868736043572426\n",
      "Epoch 62, Batch 100, Loss: 0.021225761622190475\n",
      "Epoch 63, Batch 0, Loss: 0.03342405706644058\n",
      "Epoch 63, Batch 10, Loss: 0.019974444061517715\n",
      "Epoch 63, Batch 20, Loss: 0.023036904633045197\n",
      "Epoch 63, Batch 30, Loss: 0.02846686728298664\n",
      "Epoch 63, Batch 40, Loss: 0.022073034197092056\n",
      "Epoch 63, Batch 50, Loss: 0.02572057768702507\n",
      "Epoch 63, Batch 60, Loss: 0.017721692100167274\n",
      "Epoch 63, Batch 70, Loss: 0.01918528601527214\n",
      "Epoch 63, Batch 80, Loss: 0.019393056631088257\n",
      "Epoch 63, Batch 90, Loss: 0.022294724360108376\n",
      "Epoch 63, Batch 100, Loss: 0.020057883113622665\n",
      "Epoch 64, Batch 0, Loss: 0.018319960683584213\n",
      "Epoch 64, Batch 10, Loss: 0.022442281246185303\n",
      "Epoch 64, Batch 20, Loss: 0.02355661615729332\n",
      "Epoch 64, Batch 30, Loss: 0.026769375428557396\n",
      "Epoch 64, Batch 40, Loss: 0.02552018128335476\n",
      "Epoch 64, Batch 50, Loss: 0.04094654321670532\n",
      "Epoch 64, Batch 60, Loss: 0.026046736165881157\n",
      "Epoch 64, Batch 70, Loss: 0.01988602988421917\n",
      "Epoch 64, Batch 80, Loss: 0.019635522738099098\n",
      "Epoch 64, Batch 90, Loss: 0.02346520498394966\n",
      "Epoch 64, Batch 100, Loss: 0.027567587792873383\n",
      "Epoch 65, Batch 0, Loss: 0.029277168214321136\n",
      "Epoch 65, Batch 10, Loss: 0.019289975985884666\n",
      "Epoch 65, Batch 20, Loss: 0.029276961460709572\n",
      "Epoch 65, Batch 30, Loss: 0.02573845535516739\n",
      "Epoch 65, Batch 40, Loss: 0.019033856689929962\n",
      "Epoch 65, Batch 50, Loss: 0.024565400555729866\n",
      "Epoch 65, Batch 60, Loss: 0.028287433087825775\n",
      "Epoch 65, Batch 70, Loss: 0.02331271767616272\n",
      "Epoch 65, Batch 80, Loss: 0.019521042704582214\n",
      "Epoch 65, Batch 90, Loss: 0.02718290686607361\n",
      "Epoch 65, Batch 100, Loss: 0.030616655945777893\n",
      "Epoch 66, Batch 0, Loss: 0.016419915482401848\n",
      "Epoch 66, Batch 10, Loss: 0.017426423728466034\n",
      "Epoch 66, Batch 20, Loss: 0.02479216828942299\n",
      "Epoch 66, Batch 30, Loss: 0.020945217460393906\n",
      "Epoch 66, Batch 40, Loss: 0.021520163863897324\n",
      "Epoch 66, Batch 50, Loss: 0.02809753827750683\n",
      "Epoch 66, Batch 60, Loss: 0.03504878282546997\n",
      "Epoch 66, Batch 70, Loss: 0.025180064141750336\n",
      "Epoch 66, Batch 80, Loss: 0.022003566846251488\n",
      "Epoch 66, Batch 90, Loss: 0.022002317011356354\n",
      "Epoch 66, Batch 100, Loss: 0.023569155484437943\n",
      "Epoch 67, Batch 0, Loss: 0.027941759675741196\n",
      "Epoch 67, Batch 10, Loss: 0.032611262053251266\n",
      "Epoch 67, Batch 20, Loss: 0.016504600644111633\n",
      "Epoch 67, Batch 30, Loss: 0.0192891713231802\n",
      "Epoch 67, Batch 40, Loss: 0.021948056295514107\n",
      "Epoch 67, Batch 50, Loss: 0.02120230160653591\n",
      "Epoch 67, Batch 60, Loss: 0.023709183558821678\n",
      "Epoch 67, Batch 70, Loss: 0.025568880140781403\n",
      "Epoch 67, Batch 80, Loss: 0.017668845131993294\n",
      "Epoch 67, Batch 90, Loss: 0.024365130811929703\n",
      "Epoch 67, Batch 100, Loss: 0.02678106538951397\n",
      "Epoch 68, Batch 0, Loss: 0.02559574507176876\n",
      "Epoch 68, Batch 10, Loss: 0.02915400080382824\n",
      "Epoch 68, Batch 20, Loss: 0.02015218697488308\n",
      "Epoch 68, Batch 30, Loss: 0.02357301488518715\n",
      "Epoch 68, Batch 40, Loss: 0.02146664448082447\n",
      "Epoch 68, Batch 50, Loss: 0.02649959921836853\n",
      "Epoch 68, Batch 60, Loss: 0.03104046732187271\n",
      "Epoch 68, Batch 70, Loss: 0.028101757168769836\n",
      "Epoch 68, Batch 80, Loss: 0.02370305359363556\n",
      "Epoch 68, Batch 90, Loss: 0.024835940450429916\n",
      "Epoch 68, Batch 100, Loss: 0.021967114880681038\n",
      "Epoch 69, Batch 0, Loss: 0.03583049401640892\n",
      "Epoch 69, Batch 10, Loss: 0.02238350547850132\n",
      "Epoch 69, Batch 20, Loss: 0.02093658782541752\n",
      "Epoch 69, Batch 30, Loss: 0.020490821450948715\n",
      "Epoch 69, Batch 40, Loss: 0.028687428683042526\n",
      "Epoch 69, Batch 50, Loss: 0.022638237103819847\n",
      "Epoch 69, Batch 60, Loss: 0.01684170588850975\n",
      "Epoch 69, Batch 70, Loss: 0.023785006254911423\n",
      "Epoch 69, Batch 80, Loss: 0.02073105424642563\n",
      "Epoch 69, Batch 90, Loss: 0.021850107237696648\n",
      "Epoch 69, Batch 100, Loss: 0.02613561600446701\n",
      "Epoch 70, Batch 0, Loss: 0.028391029685735703\n",
      "Epoch 70, Batch 10, Loss: 0.023300349712371826\n",
      "Epoch 70, Batch 20, Loss: 0.026809755712747574\n",
      "Epoch 70, Batch 30, Loss: 0.026187395676970482\n",
      "Epoch 70, Batch 40, Loss: 0.022045154124498367\n",
      "Epoch 70, Batch 50, Loss: 0.023808009922504425\n",
      "Epoch 70, Batch 60, Loss: 0.028793899342417717\n",
      "Epoch 70, Batch 70, Loss: 0.016399450600147247\n",
      "Epoch 70, Batch 80, Loss: 0.018293021246790886\n",
      "Epoch 70, Batch 90, Loss: 0.016279323026537895\n",
      "Epoch 70, Batch 100, Loss: 0.030029568821191788\n",
      "Epoch 71, Batch 0, Loss: 0.01959608867764473\n",
      "Epoch 71, Batch 10, Loss: 0.019498281180858612\n",
      "Epoch 71, Batch 20, Loss: 0.024217287078499794\n",
      "Epoch 71, Batch 30, Loss: 0.028893131762742996\n",
      "Epoch 71, Batch 40, Loss: 0.016702847555279732\n",
      "Epoch 71, Batch 50, Loss: 0.02667248621582985\n",
      "Epoch 71, Batch 60, Loss: 0.02394658699631691\n",
      "Epoch 71, Batch 70, Loss: 0.028976205736398697\n",
      "Epoch 71, Batch 80, Loss: 0.016093773767352104\n",
      "Epoch 71, Batch 90, Loss: 0.02753487229347229\n",
      "Epoch 71, Batch 100, Loss: 0.028820039704442024\n",
      "Epoch 72, Batch 0, Loss: 0.026430994272232056\n",
      "Epoch 72, Batch 10, Loss: 0.01775854267179966\n",
      "Epoch 72, Batch 20, Loss: 0.0198139026761055\n",
      "Epoch 72, Batch 30, Loss: 0.01900181546807289\n",
      "Epoch 72, Batch 40, Loss: 0.024276502430438995\n",
      "Epoch 72, Batch 50, Loss: 0.021703297272324562\n",
      "Epoch 72, Batch 60, Loss: 0.03766639530658722\n",
      "Epoch 72, Batch 70, Loss: 0.022747810930013657\n",
      "Epoch 72, Batch 80, Loss: 0.019566096365451813\n",
      "Epoch 72, Batch 90, Loss: 0.02889264188706875\n",
      "Epoch 72, Batch 100, Loss: 0.020742760971188545\n",
      "Epoch 73, Batch 0, Loss: 0.022030161693692207\n",
      "Epoch 73, Batch 10, Loss: 0.021230924874544144\n",
      "Epoch 73, Batch 20, Loss: 0.025726495310664177\n",
      "Epoch 73, Batch 30, Loss: 0.031900033354759216\n",
      "Epoch 73, Batch 40, Loss: 0.019668936729431152\n",
      "Epoch 73, Batch 50, Loss: 0.023152191191911697\n",
      "Epoch 73, Batch 60, Loss: 0.02092876471579075\n",
      "Epoch 73, Batch 70, Loss: 0.02592029981315136\n",
      "Epoch 73, Batch 80, Loss: 0.020647039636969566\n",
      "Epoch 73, Batch 90, Loss: 0.02284889668226242\n",
      "Epoch 73, Batch 100, Loss: 0.02334117516875267\n",
      "Epoch 74, Batch 0, Loss: 0.021388543769717216\n",
      "Epoch 74, Batch 10, Loss: 0.027939625084400177\n",
      "Epoch 74, Batch 20, Loss: 0.019136546179652214\n",
      "Epoch 74, Batch 30, Loss: 0.019462035968899727\n",
      "Epoch 74, Batch 40, Loss: 0.03335133194923401\n",
      "Epoch 74, Batch 50, Loss: 0.01970086619257927\n",
      "Epoch 74, Batch 60, Loss: 0.02309367060661316\n",
      "Epoch 74, Batch 70, Loss: 0.02685515210032463\n",
      "Epoch 74, Batch 80, Loss: 0.024253137409687042\n",
      "Epoch 74, Batch 90, Loss: 0.034002527594566345\n",
      "Epoch 74, Batch 100, Loss: 0.020107321441173553\n",
      "Epoch 75, Batch 0, Loss: 0.021014872938394547\n",
      "Epoch 75, Batch 10, Loss: 0.03122866526246071\n",
      "Epoch 75, Batch 20, Loss: 0.026690110564231873\n",
      "Epoch 75, Batch 30, Loss: 0.023662565276026726\n",
      "Epoch 75, Batch 40, Loss: 0.024565767496824265\n",
      "Epoch 75, Batch 50, Loss: 0.01925761066377163\n",
      "Epoch 75, Batch 60, Loss: 0.03299844264984131\n",
      "Epoch 75, Batch 70, Loss: 0.02140435017645359\n",
      "Epoch 75, Batch 80, Loss: 0.026847966015338898\n",
      "Epoch 75, Batch 90, Loss: 0.017047923058271408\n",
      "Epoch 75, Batch 100, Loss: 0.030848216265439987\n",
      "Epoch 76, Batch 0, Loss: 0.020497465506196022\n",
      "Epoch 76, Batch 10, Loss: 0.02790963090956211\n",
      "Epoch 76, Batch 20, Loss: 0.02802206017076969\n",
      "Epoch 76, Batch 30, Loss: 0.019867299124598503\n",
      "Epoch 76, Batch 40, Loss: 0.02546466514468193\n",
      "Epoch 76, Batch 50, Loss: 0.0253312848508358\n",
      "Epoch 76, Batch 60, Loss: 0.01609865389764309\n",
      "Epoch 76, Batch 70, Loss: 0.02833985723555088\n",
      "Epoch 76, Batch 80, Loss: 0.01869363710284233\n",
      "Epoch 76, Batch 90, Loss: 0.019894156605005264\n",
      "Epoch 76, Batch 100, Loss: 0.02381557784974575\n",
      "Epoch 77, Batch 0, Loss: 0.021681537851691246\n",
      "Epoch 77, Batch 10, Loss: 0.019920825958251953\n",
      "Epoch 77, Batch 20, Loss: 0.025844665244221687\n",
      "Epoch 77, Batch 30, Loss: 0.022553743794560432\n",
      "Epoch 77, Batch 40, Loss: 0.026030585169792175\n",
      "Epoch 77, Batch 50, Loss: 0.02097262069582939\n",
      "Epoch 77, Batch 60, Loss: 0.023473719134926796\n",
      "Epoch 77, Batch 70, Loss: 0.024801727384328842\n",
      "Epoch 77, Batch 80, Loss: 0.027181118726730347\n",
      "Epoch 77, Batch 90, Loss: 0.018882926553487778\n",
      "Epoch 77, Batch 100, Loss: 0.022943660616874695\n",
      "Epoch 78, Batch 0, Loss: 0.02741413563489914\n",
      "Epoch 78, Batch 10, Loss: 0.02597665600478649\n",
      "Epoch 78, Batch 20, Loss: 0.02712186612188816\n",
      "Epoch 78, Batch 30, Loss: 0.0192423053085804\n",
      "Epoch 78, Batch 40, Loss: 0.03072349540889263\n",
      "Epoch 78, Batch 50, Loss: 0.02274121530354023\n",
      "Epoch 78, Batch 60, Loss: 0.0241074301302433\n",
      "Epoch 78, Batch 70, Loss: 0.025016041472554207\n",
      "Epoch 78, Batch 80, Loss: 0.025641508400440216\n",
      "Epoch 78, Batch 90, Loss: 0.02338322252035141\n",
      "Epoch 78, Batch 100, Loss: 0.021564282476902008\n",
      "Epoch 79, Batch 0, Loss: 0.03037998080253601\n",
      "Epoch 79, Batch 10, Loss: 0.024188591167330742\n",
      "Epoch 79, Batch 20, Loss: 0.024623477831482887\n",
      "Epoch 79, Batch 30, Loss: 0.0256599523127079\n",
      "Epoch 79, Batch 40, Loss: 0.027084827423095703\n",
      "Epoch 79, Batch 50, Loss: 0.019143719226121902\n",
      "Epoch 79, Batch 60, Loss: 0.029699603095650673\n",
      "Epoch 79, Batch 70, Loss: 0.027383040636777878\n",
      "Epoch 79, Batch 80, Loss: 0.025103436782956123\n",
      "Epoch 79, Batch 90, Loss: 0.025509601458907127\n",
      "Epoch 79, Batch 100, Loss: 0.019486913457512856\n",
      "Epoch 80, Batch 0, Loss: 0.02520783618092537\n",
      "Epoch 80, Batch 10, Loss: 0.02612879127264023\n",
      "Epoch 80, Batch 20, Loss: 0.02443399652838707\n",
      "Epoch 80, Batch 30, Loss: 0.017901429906487465\n",
      "Epoch 80, Batch 40, Loss: 0.03012312762439251\n",
      "Epoch 80, Batch 50, Loss: 0.016866998746991158\n",
      "Epoch 80, Batch 60, Loss: 0.027457047253847122\n",
      "Epoch 80, Batch 70, Loss: 0.02218419499695301\n",
      "Epoch 80, Batch 80, Loss: 0.02911856397986412\n",
      "Epoch 80, Batch 90, Loss: 0.028154592961072922\n",
      "Epoch 80, Batch 100, Loss: 0.020558802410960197\n",
      "Epoch 81, Batch 0, Loss: 0.027345119044184685\n",
      "Epoch 81, Batch 10, Loss: 0.024848386645317078\n",
      "Epoch 81, Batch 20, Loss: 0.017732711508870125\n",
      "Epoch 81, Batch 30, Loss: 0.03099486045539379\n",
      "Epoch 81, Batch 40, Loss: 0.015836002305150032\n",
      "Epoch 81, Batch 50, Loss: 0.023677032440900803\n",
      "Epoch 81, Batch 60, Loss: 0.019199397414922714\n",
      "Epoch 81, Batch 70, Loss: 0.03401971235871315\n",
      "Epoch 81, Batch 80, Loss: 0.027649328112602234\n",
      "Epoch 81, Batch 90, Loss: 0.02237938903272152\n",
      "Epoch 81, Batch 100, Loss: 0.01941807195544243\n",
      "Epoch 82, Batch 0, Loss: 0.01713922806084156\n",
      "Epoch 82, Batch 10, Loss: 0.027367064729332924\n",
      "Epoch 82, Batch 20, Loss: 0.028553035110235214\n",
      "Epoch 82, Batch 30, Loss: 0.023333795368671417\n",
      "Epoch 82, Batch 40, Loss: 0.022409377619624138\n",
      "Epoch 82, Batch 50, Loss: 0.021494723856449127\n",
      "Epoch 82, Batch 60, Loss: 0.0187336727976799\n",
      "Epoch 82, Batch 70, Loss: 0.026424270123243332\n",
      "Epoch 82, Batch 80, Loss: 0.02214839495718479\n",
      "Epoch 82, Batch 90, Loss: 0.017876647412776947\n",
      "Epoch 82, Batch 100, Loss: 0.020909316837787628\n",
      "Epoch 83, Batch 0, Loss: 0.03432042896747589\n",
      "Epoch 83, Batch 10, Loss: 0.021301496773958206\n",
      "Epoch 83, Batch 20, Loss: 0.031104441732168198\n",
      "Epoch 83, Batch 30, Loss: 0.024406233802437782\n",
      "Epoch 83, Batch 40, Loss: 0.02440553531050682\n",
      "Epoch 83, Batch 50, Loss: 0.02487741969525814\n",
      "Epoch 83, Batch 60, Loss: 0.024357743561267853\n",
      "Epoch 83, Batch 70, Loss: 0.023182038217782974\n",
      "Epoch 83, Batch 80, Loss: 0.027006154879927635\n",
      "Epoch 83, Batch 90, Loss: 0.020000632852315903\n",
      "Epoch 83, Batch 100, Loss: 0.028658827766776085\n",
      "Epoch 84, Batch 0, Loss: 0.02941550873219967\n",
      "Epoch 84, Batch 10, Loss: 0.033294931054115295\n",
      "Epoch 84, Batch 20, Loss: 0.026481453329324722\n",
      "Epoch 84, Batch 30, Loss: 0.02890995517373085\n",
      "Epoch 84, Batch 40, Loss: 0.0282946415245533\n",
      "Epoch 84, Batch 50, Loss: 0.015833061188459396\n",
      "Epoch 84, Batch 60, Loss: 0.02680724300444126\n",
      "Epoch 84, Batch 70, Loss: 0.029610756784677505\n",
      "Epoch 84, Batch 80, Loss: 0.025408407673239708\n",
      "Epoch 84, Batch 90, Loss: 0.02302139438688755\n",
      "Epoch 84, Batch 100, Loss: 0.028177956119179726\n",
      "Epoch 85, Batch 0, Loss: 0.02452462539076805\n",
      "Epoch 85, Batch 10, Loss: 0.025723736733198166\n",
      "Epoch 85, Batch 20, Loss: 0.0207301527261734\n",
      "Epoch 85, Batch 30, Loss: 0.02702212706208229\n",
      "Epoch 85, Batch 40, Loss: 0.024237439036369324\n",
      "Epoch 85, Batch 50, Loss: 0.020256832242012024\n",
      "Epoch 85, Batch 60, Loss: 0.025869060307741165\n",
      "Epoch 85, Batch 70, Loss: 0.025537922978401184\n",
      "Epoch 85, Batch 80, Loss: 0.024390187114477158\n",
      "Epoch 85, Batch 90, Loss: 0.026981737464666367\n",
      "Epoch 85, Batch 100, Loss: 0.023488909006118774\n",
      "Epoch 86, Batch 0, Loss: 0.026136038824915886\n",
      "Epoch 86, Batch 10, Loss: 0.027977777644991875\n",
      "Epoch 86, Batch 20, Loss: 0.02194514311850071\n",
      "Epoch 86, Batch 30, Loss: 0.02614264003932476\n",
      "Epoch 86, Batch 40, Loss: 0.023287655785679817\n",
      "Epoch 86, Batch 50, Loss: 0.02463054470717907\n",
      "Epoch 86, Batch 60, Loss: 0.024936703965067863\n",
      "Epoch 86, Batch 70, Loss: 0.020759159699082375\n",
      "Epoch 86, Batch 80, Loss: 0.02989690564572811\n",
      "Epoch 86, Batch 90, Loss: 0.02386474609375\n",
      "Epoch 86, Batch 100, Loss: 0.02314765378832817\n",
      "Epoch 87, Batch 0, Loss: 0.02608223631978035\n",
      "Epoch 87, Batch 10, Loss: 0.03135541453957558\n",
      "Epoch 87, Batch 20, Loss: 0.023808710277080536\n",
      "Epoch 87, Batch 30, Loss: 0.018600400537252426\n",
      "Epoch 87, Batch 40, Loss: 0.025860188528895378\n",
      "Epoch 87, Batch 50, Loss: 0.022418860346078873\n",
      "Epoch 87, Batch 60, Loss: 0.023423170670866966\n",
      "Epoch 87, Batch 70, Loss: 0.02271440625190735\n",
      "Epoch 87, Batch 80, Loss: 0.020569290965795517\n",
      "Epoch 87, Batch 90, Loss: 0.028870083391666412\n",
      "Epoch 87, Batch 100, Loss: 0.015136651694774628\n",
      "Epoch 88, Batch 0, Loss: 0.03320670127868652\n",
      "Epoch 88, Batch 10, Loss: 0.029455488547682762\n",
      "Epoch 88, Batch 20, Loss: 0.024020085111260414\n",
      "Epoch 88, Batch 30, Loss: 0.023019716143608093\n",
      "Epoch 88, Batch 40, Loss: 0.019402146339416504\n",
      "Epoch 88, Batch 50, Loss: 0.020611396059393883\n",
      "Epoch 88, Batch 60, Loss: 0.021093636751174927\n",
      "Epoch 88, Batch 70, Loss: 0.027840331196784973\n",
      "Epoch 88, Batch 80, Loss: 0.03291407227516174\n",
      "Epoch 88, Batch 90, Loss: 0.021366527304053307\n",
      "Epoch 88, Batch 100, Loss: 0.02153702825307846\n",
      "Epoch 89, Batch 0, Loss: 0.026122311130166054\n",
      "Epoch 89, Batch 10, Loss: 0.0338476188480854\n",
      "Epoch 89, Batch 20, Loss: 0.02117670699954033\n",
      "Epoch 89, Batch 30, Loss: 0.0279674232006073\n",
      "Epoch 89, Batch 40, Loss: 0.023216182366013527\n",
      "Epoch 89, Batch 50, Loss: 0.024589968845248222\n",
      "Epoch 89, Batch 60, Loss: 0.0219972413033247\n",
      "Epoch 89, Batch 70, Loss: 0.024987399578094482\n",
      "Epoch 89, Batch 80, Loss: 0.028276225551962852\n",
      "Epoch 89, Batch 90, Loss: 0.01922784000635147\n",
      "Epoch 89, Batch 100, Loss: 0.018026193603873253\n",
      "Epoch 90, Batch 0, Loss: 0.021893072873353958\n",
      "Epoch 90, Batch 10, Loss: 0.03425661474466324\n",
      "Epoch 90, Batch 20, Loss: 0.018347682431340218\n",
      "Epoch 90, Batch 30, Loss: 0.024392081424593925\n",
      "Epoch 90, Batch 40, Loss: 0.018814973533153534\n",
      "Epoch 90, Batch 50, Loss: 0.024863211438059807\n",
      "Epoch 90, Batch 60, Loss: 0.022440636530518532\n",
      "Epoch 90, Batch 70, Loss: 0.030088134109973907\n",
      "Epoch 90, Batch 80, Loss: 0.025463342666625977\n",
      "Epoch 90, Batch 90, Loss: 0.02498764358460903\n",
      "Epoch 90, Batch 100, Loss: 0.015428929589688778\n",
      "Epoch 91, Batch 0, Loss: 0.030715567991137505\n",
      "Epoch 91, Batch 10, Loss: 0.02579125389456749\n",
      "Epoch 91, Batch 20, Loss: 0.023334907367825508\n",
      "Epoch 91, Batch 30, Loss: 0.0215945765376091\n",
      "Epoch 91, Batch 40, Loss: 0.04031158611178398\n",
      "Epoch 91, Batch 50, Loss: 0.024864142760634422\n",
      "Epoch 91, Batch 60, Loss: 0.02457498013973236\n",
      "Epoch 91, Batch 70, Loss: 0.024792401120066643\n",
      "Epoch 91, Batch 80, Loss: 0.021637002006173134\n",
      "Epoch 91, Batch 90, Loss: 0.027924275025725365\n",
      "Epoch 91, Batch 100, Loss: 0.021279528737068176\n",
      "Epoch 92, Batch 0, Loss: 0.018644675612449646\n",
      "Epoch 92, Batch 10, Loss: 0.017580028623342514\n",
      "Epoch 92, Batch 20, Loss: 0.01815180853009224\n",
      "Epoch 92, Batch 30, Loss: 0.019013818353414536\n",
      "Epoch 92, Batch 40, Loss: 0.017299819737672806\n",
      "Epoch 92, Batch 50, Loss: 0.021247193217277527\n",
      "Epoch 92, Batch 60, Loss: 0.020890096202492714\n",
      "Epoch 92, Batch 70, Loss: 0.027507934719324112\n",
      "Epoch 92, Batch 80, Loss: 0.02247701585292816\n",
      "Epoch 92, Batch 90, Loss: 0.029101068153977394\n",
      "Epoch 92, Batch 100, Loss: 0.01771613024175167\n",
      "Epoch 93, Batch 0, Loss: 0.028041882440447807\n",
      "Epoch 93, Batch 10, Loss: 0.026348324492573738\n",
      "Epoch 93, Batch 20, Loss: 0.028135502710938454\n",
      "Epoch 93, Batch 30, Loss: 0.02159820683300495\n",
      "Epoch 93, Batch 40, Loss: 0.020605875179171562\n",
      "Epoch 93, Batch 50, Loss: 0.021954862400889397\n",
      "Epoch 93, Batch 60, Loss: 0.0306992270052433\n",
      "Epoch 93, Batch 70, Loss: 0.019999703392386436\n",
      "Epoch 93, Batch 80, Loss: 0.02864885702729225\n",
      "Epoch 93, Batch 90, Loss: 0.022351838648319244\n",
      "Epoch 93, Batch 100, Loss: 0.032901689410209656\n",
      "Epoch 94, Batch 0, Loss: 0.02561991661787033\n",
      "Epoch 94, Batch 10, Loss: 0.032016802579164505\n",
      "Epoch 94, Batch 20, Loss: 0.019085921347141266\n",
      "Epoch 94, Batch 30, Loss: 0.02298586443066597\n",
      "Epoch 94, Batch 40, Loss: 0.021939020603895187\n",
      "Epoch 94, Batch 50, Loss: 0.014899910427629948\n",
      "Epoch 94, Batch 60, Loss: 0.019798574969172478\n",
      "Epoch 94, Batch 70, Loss: 0.021640947088599205\n",
      "Epoch 94, Batch 80, Loss: 0.017766883596777916\n",
      "Epoch 94, Batch 90, Loss: 0.022656841203570366\n",
      "Epoch 94, Batch 100, Loss: 0.0243435800075531\n",
      "Epoch 95, Batch 0, Loss: 0.021659748628735542\n",
      "Epoch 95, Batch 10, Loss: 0.022988535463809967\n",
      "Epoch 95, Batch 20, Loss: 0.020244386047124863\n",
      "Epoch 95, Batch 30, Loss: 0.025975441560149193\n",
      "Epoch 95, Batch 40, Loss: 0.02858399599790573\n",
      "Epoch 95, Batch 50, Loss: 0.02712365798652172\n",
      "Epoch 95, Batch 60, Loss: 0.025351859629154205\n",
      "Epoch 95, Batch 70, Loss: 0.022228257730603218\n",
      "Epoch 95, Batch 80, Loss: 0.025717925280332565\n",
      "Epoch 95, Batch 90, Loss: 0.020105993375182152\n",
      "Epoch 95, Batch 100, Loss: 0.032812558114528656\n",
      "Epoch 96, Batch 0, Loss: 0.01929151453077793\n",
      "Epoch 96, Batch 10, Loss: 0.02577722817659378\n",
      "Epoch 96, Batch 20, Loss: 0.018558036535978317\n",
      "Epoch 96, Batch 30, Loss: 0.02833884209394455\n",
      "Epoch 96, Batch 40, Loss: 0.0284720566123724\n",
      "Epoch 96, Batch 50, Loss: 0.02966511808335781\n",
      "Epoch 96, Batch 60, Loss: 0.026968959718942642\n",
      "Epoch 96, Batch 70, Loss: 0.023341014981269836\n",
      "Epoch 96, Batch 80, Loss: 0.02428930252790451\n",
      "Epoch 96, Batch 90, Loss: 0.023981362581253052\n",
      "Epoch 96, Batch 100, Loss: 0.023373236879706383\n",
      "Epoch 97, Batch 0, Loss: 0.027941446751356125\n",
      "Epoch 97, Batch 10, Loss: 0.026879703626036644\n",
      "Epoch 97, Batch 20, Loss: 0.02382485568523407\n",
      "Epoch 97, Batch 30, Loss: 0.030664365738630295\n",
      "Epoch 97, Batch 40, Loss: 0.02355394884943962\n",
      "Epoch 97, Batch 50, Loss: 0.01820448227226734\n",
      "Epoch 97, Batch 60, Loss: 0.025531865656375885\n",
      "Epoch 97, Batch 70, Loss: 0.016985734924674034\n",
      "Epoch 97, Batch 80, Loss: 0.020302092656493187\n",
      "Epoch 97, Batch 90, Loss: 0.023708507418632507\n",
      "Epoch 97, Batch 100, Loss: 0.021481402218341827\n",
      "Epoch 98, Batch 0, Loss: 0.020736927166581154\n",
      "Epoch 98, Batch 10, Loss: 0.0175850261002779\n",
      "Epoch 98, Batch 20, Loss: 0.02375483326613903\n",
      "Epoch 98, Batch 30, Loss: 0.025396322831511497\n",
      "Epoch 98, Batch 40, Loss: 0.026655517518520355\n",
      "Epoch 98, Batch 50, Loss: 0.023547183722257614\n",
      "Epoch 98, Batch 60, Loss: 0.02551358938217163\n",
      "Epoch 98, Batch 70, Loss: 0.01950923725962639\n",
      "Epoch 98, Batch 80, Loss: 0.026324059814214706\n",
      "Epoch 98, Batch 90, Loss: 0.02159661054611206\n",
      "Epoch 98, Batch 100, Loss: 0.02360219694674015\n",
      "Epoch 99, Batch 0, Loss: 0.03074546717107296\n",
      "Epoch 99, Batch 10, Loss: 0.022124681621789932\n",
      "Epoch 99, Batch 20, Loss: 0.022191843017935753\n",
      "Epoch 99, Batch 30, Loss: 0.017744002863764763\n",
      "Epoch 99, Batch 40, Loss: 0.02321210503578186\n",
      "Epoch 99, Batch 50, Loss: 0.027354899793863297\n",
      "Epoch 99, Batch 60, Loss: 0.015174446627497673\n",
      "Epoch 99, Batch 70, Loss: 0.024161644279956818\n",
      "Epoch 99, Batch 80, Loss: 0.021843211725354195\n",
      "Epoch 99, Batch 90, Loss: 0.020319480448961258\n",
      "Epoch 99, Batch 100, Loss: 0.026399146765470505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_BILSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (bilstm): BILSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True, bidirectional=True)\n",
       "    (fc1): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.039768; Test RMSE 44.994372\n",
      "\n",
      "Train  MAE: 0.023019; Test  MAE 31.383991\n",
      "\n",
      "Train  R^2: 0.998418; Test  R^2 0.959002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
