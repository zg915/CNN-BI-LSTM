{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_preprocessing import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6618, 10, 8]) torch.Size([6618, 1]) torch.Size([490, 10, 8]) (490, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, y_scaler, train_loader, test_loader = get_data(\n",
    "            timestep = 10,\n",
    "            batch_size = 64,\n",
    "            y_name = 'Closing price',\n",
    "            train_size = 6627,\n",
    "            path = \"Data/SSE000001.csv\")\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    print (\"GPU device not found.\")\n",
    "\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer parameters from the table\n",
    "        self.conv1d = nn.Conv1d(in_channels= 8, out_channels= 32, kernel_size= 1, padding=\"same\")\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Pooling\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size= 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layer\n",
    "        x = self.conv1d(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64, lstm_dropout = 0.2, fc1_output_size = 16):\n",
    "        super(BILSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True,\n",
    "                             bidirectional = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size * 2, self.fc1_output_size)\n",
    "        self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        h_fc2 = self.fc2(h_fc1)\n",
    "        output = h_fc2[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64,  fc1_output_size = 16):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        # self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p = self.lstm_dropout)\n",
    "        # self.fc1 = nn.Linear(self.lstm_hidden_size, self.fc1_output_size)\n",
    "        # self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "        # h_dropout = self.dropout(h_lstm)\n",
    "\n",
    "        # h_fc1 = self.fc1(h_lstm)\n",
    "        # h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        # h_fc2 = self.fc2(h_fc1)\n",
    "        # output = h_fc2[:, -1, :]\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        output = h_fc1[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.lstm = LSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x)\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Reshape x to fit BiLSTM input requirements\n",
    "        x = x.contiguous().view(batch_size, seq_len, -1)\n",
    "        x = self.lstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_BiLSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.8225922584533691\n",
      "Epoch 0, Batch 10, Loss: 0.9547179341316223\n",
      "Epoch 0, Batch 20, Loss: 0.6931480169296265\n",
      "Epoch 0, Batch 30, Loss: 0.9737200736999512\n",
      "Epoch 0, Batch 40, Loss: 0.6662105321884155\n",
      "Epoch 0, Batch 50, Loss: 0.7251437306404114\n",
      "Epoch 0, Batch 60, Loss: 0.7388264536857605\n",
      "Epoch 0, Batch 70, Loss: 0.6835612058639526\n",
      "Epoch 0, Batch 80, Loss: 0.7202428579330444\n",
      "Epoch 0, Batch 90, Loss: 0.5747403502464294\n",
      "Epoch 0, Batch 100, Loss: 0.6590320467948914\n",
      "Epoch 1, Batch 0, Loss: 0.5939779281616211\n",
      "Epoch 1, Batch 10, Loss: 0.48948001861572266\n",
      "Epoch 1, Batch 20, Loss: 0.42448103427886963\n",
      "Epoch 1, Batch 30, Loss: 0.34403321146965027\n",
      "Epoch 1, Batch 40, Loss: 0.2578132450580597\n",
      "Epoch 1, Batch 50, Loss: 0.11058269441127777\n",
      "Epoch 1, Batch 60, Loss: 0.15083293616771698\n",
      "Epoch 1, Batch 70, Loss: 0.13186773657798767\n",
      "Epoch 1, Batch 80, Loss: 0.14412513375282288\n",
      "Epoch 1, Batch 90, Loss: 0.13775332272052765\n",
      "Epoch 1, Batch 100, Loss: 0.11249219626188278\n",
      "Epoch 2, Batch 0, Loss: 0.10952335596084595\n",
      "Epoch 2, Batch 10, Loss: 0.1478879749774933\n",
      "Epoch 2, Batch 20, Loss: 0.06722642481327057\n",
      "Epoch 2, Batch 30, Loss: 0.04328955337405205\n",
      "Epoch 2, Batch 40, Loss: 0.07492069900035858\n",
      "Epoch 2, Batch 50, Loss: 0.11963574588298798\n",
      "Epoch 2, Batch 60, Loss: 0.07177851349115372\n",
      "Epoch 2, Batch 70, Loss: 0.10185229033231735\n",
      "Epoch 2, Batch 80, Loss: 0.09935716539621353\n",
      "Epoch 2, Batch 90, Loss: 0.06680010259151459\n",
      "Epoch 2, Batch 100, Loss: 0.054237231612205505\n",
      "Epoch 3, Batch 0, Loss: 0.04601265862584114\n",
      "Epoch 3, Batch 10, Loss: 0.05474831908941269\n",
      "Epoch 3, Batch 20, Loss: 0.08735755831003189\n",
      "Epoch 3, Batch 30, Loss: 0.08550120145082474\n",
      "Epoch 3, Batch 40, Loss: 0.07086469233036041\n",
      "Epoch 3, Batch 50, Loss: 0.08355559408664703\n",
      "Epoch 3, Batch 60, Loss: 0.045226991176605225\n",
      "Epoch 3, Batch 70, Loss: 0.049859873950481415\n",
      "Epoch 3, Batch 80, Loss: 0.06578368693590164\n",
      "Epoch 3, Batch 90, Loss: 0.04288126900792122\n",
      "Epoch 3, Batch 100, Loss: 0.043995678424835205\n",
      "Epoch 4, Batch 0, Loss: 0.04261156544089317\n",
      "Epoch 4, Batch 10, Loss: 0.06457693874835968\n",
      "Epoch 4, Batch 20, Loss: 0.05782122164964676\n",
      "Epoch 4, Batch 30, Loss: 0.059844788163900375\n",
      "Epoch 4, Batch 40, Loss: 0.044141240417957306\n",
      "Epoch 4, Batch 50, Loss: 0.05947175621986389\n",
      "Epoch 4, Batch 60, Loss: 0.04312360659241676\n",
      "Epoch 4, Batch 70, Loss: 0.03831909969449043\n",
      "Epoch 4, Batch 80, Loss: 0.02710345946252346\n",
      "Epoch 4, Batch 90, Loss: 0.05356688052415848\n",
      "Epoch 4, Batch 100, Loss: 0.05014733970165253\n",
      "Epoch 5, Batch 0, Loss: 0.058435339480638504\n",
      "Epoch 5, Batch 10, Loss: 0.05354994535446167\n",
      "Epoch 5, Batch 20, Loss: 0.04154966399073601\n",
      "Epoch 5, Batch 30, Loss: 0.03495441749691963\n",
      "Epoch 5, Batch 40, Loss: 0.028660491108894348\n",
      "Epoch 5, Batch 50, Loss: 0.032501328736543655\n",
      "Epoch 5, Batch 60, Loss: 0.041633013635873795\n",
      "Epoch 5, Batch 70, Loss: 0.03785306215286255\n",
      "Epoch 5, Batch 80, Loss: 0.04216825217008591\n",
      "Epoch 5, Batch 90, Loss: 0.04620756581425667\n",
      "Epoch 5, Batch 100, Loss: 0.055676285177469254\n",
      "Epoch 6, Batch 0, Loss: 0.027906667441129684\n",
      "Epoch 6, Batch 10, Loss: 0.03901488333940506\n",
      "Epoch 6, Batch 20, Loss: 0.033884044736623764\n",
      "Epoch 6, Batch 30, Loss: 0.03530452400445938\n",
      "Epoch 6, Batch 40, Loss: 0.024260368198156357\n",
      "Epoch 6, Batch 50, Loss: 0.05415652319788933\n",
      "Epoch 6, Batch 60, Loss: 0.04311773180961609\n",
      "Epoch 6, Batch 70, Loss: 0.05493977665901184\n",
      "Epoch 6, Batch 80, Loss: 0.06645946204662323\n",
      "Epoch 6, Batch 90, Loss: 0.052167944610118866\n",
      "Epoch 6, Batch 100, Loss: 0.04615918546915054\n",
      "Epoch 7, Batch 0, Loss: 0.04469708353281021\n",
      "Epoch 7, Batch 10, Loss: 0.04368976503610611\n",
      "Epoch 7, Batch 20, Loss: 0.04244744032621384\n",
      "Epoch 7, Batch 30, Loss: 0.03741426393389702\n",
      "Epoch 7, Batch 40, Loss: 0.051626577973365784\n",
      "Epoch 7, Batch 50, Loss: 0.04279769957065582\n",
      "Epoch 7, Batch 60, Loss: 0.039737775921821594\n",
      "Epoch 7, Batch 70, Loss: 0.023589493706822395\n",
      "Epoch 7, Batch 80, Loss: 0.038679443299770355\n",
      "Epoch 7, Batch 90, Loss: 0.031511418521404266\n",
      "Epoch 7, Batch 100, Loss: 0.02745324932038784\n",
      "Epoch 8, Batch 0, Loss: 0.047364432364702225\n",
      "Epoch 8, Batch 10, Loss: 0.02744428440928459\n",
      "Epoch 8, Batch 20, Loss: 0.04627397656440735\n",
      "Epoch 8, Batch 30, Loss: 0.04999220371246338\n",
      "Epoch 8, Batch 40, Loss: 0.031085478141903877\n",
      "Epoch 8, Batch 50, Loss: 0.03706185519695282\n",
      "Epoch 8, Batch 60, Loss: 0.041672900319099426\n",
      "Epoch 8, Batch 70, Loss: 0.04175310209393501\n",
      "Epoch 8, Batch 80, Loss: 0.027585849165916443\n",
      "Epoch 8, Batch 90, Loss: 0.03187958896160126\n",
      "Epoch 8, Batch 100, Loss: 0.03459247574210167\n",
      "Epoch 9, Batch 0, Loss: 0.028481557965278625\n",
      "Epoch 9, Batch 10, Loss: 0.049602631479501724\n",
      "Epoch 9, Batch 20, Loss: 0.03721471130847931\n",
      "Epoch 9, Batch 30, Loss: 0.040068045258522034\n",
      "Epoch 9, Batch 40, Loss: 0.03976801782846451\n",
      "Epoch 9, Batch 50, Loss: 0.04770561680197716\n",
      "Epoch 9, Batch 60, Loss: 0.03550926223397255\n",
      "Epoch 9, Batch 70, Loss: 0.028445297852158546\n",
      "Epoch 9, Batch 80, Loss: 0.02905413694679737\n",
      "Epoch 9, Batch 90, Loss: 0.030668580904603004\n",
      "Epoch 9, Batch 100, Loss: 0.02729441411793232\n",
      "Epoch 10, Batch 0, Loss: 0.034785352647304535\n",
      "Epoch 10, Batch 10, Loss: 0.02883417159318924\n",
      "Epoch 10, Batch 20, Loss: 0.02393479086458683\n",
      "Epoch 10, Batch 30, Loss: 0.02256806381046772\n",
      "Epoch 10, Batch 40, Loss: 0.04054499417543411\n",
      "Epoch 10, Batch 50, Loss: 0.03344683721661568\n",
      "Epoch 10, Batch 60, Loss: 0.022938739508390427\n",
      "Epoch 10, Batch 70, Loss: 0.029205579310655594\n",
      "Epoch 10, Batch 80, Loss: 0.02342841401696205\n",
      "Epoch 10, Batch 90, Loss: 0.0282828938215971\n",
      "Epoch 10, Batch 100, Loss: 0.03355951979756355\n",
      "Epoch 11, Batch 0, Loss: 0.03416677564382553\n",
      "Epoch 11, Batch 10, Loss: 0.03638593852519989\n",
      "Epoch 11, Batch 20, Loss: 0.03858858346939087\n",
      "Epoch 11, Batch 30, Loss: 0.02967306599020958\n",
      "Epoch 11, Batch 40, Loss: 0.027346663177013397\n",
      "Epoch 11, Batch 50, Loss: 0.027183666825294495\n",
      "Epoch 11, Batch 60, Loss: 0.03375358134508133\n",
      "Epoch 11, Batch 70, Loss: 0.0341796875\n",
      "Epoch 11, Batch 80, Loss: 0.03611596301198006\n",
      "Epoch 11, Batch 90, Loss: 0.03738779574632645\n",
      "Epoch 11, Batch 100, Loss: 0.022330479696393013\n",
      "Epoch 12, Batch 0, Loss: 0.02852964960038662\n",
      "Epoch 12, Batch 10, Loss: 0.03789617493748665\n",
      "Epoch 12, Batch 20, Loss: 0.025437070056796074\n",
      "Epoch 12, Batch 30, Loss: 0.021132033318281174\n",
      "Epoch 12, Batch 40, Loss: 0.028892964124679565\n",
      "Epoch 12, Batch 50, Loss: 0.028763826936483383\n",
      "Epoch 12, Batch 60, Loss: 0.03412472829222679\n",
      "Epoch 12, Batch 70, Loss: 0.030344489961862564\n",
      "Epoch 12, Batch 80, Loss: 0.03470229730010033\n",
      "Epoch 12, Batch 90, Loss: 0.025573205202817917\n",
      "Epoch 12, Batch 100, Loss: 0.03285865858197212\n",
      "Epoch 13, Batch 0, Loss: 0.01960742473602295\n",
      "Epoch 13, Batch 10, Loss: 0.02507753297686577\n",
      "Epoch 13, Batch 20, Loss: 0.03630983829498291\n",
      "Epoch 13, Batch 30, Loss: 0.027463294565677643\n",
      "Epoch 13, Batch 40, Loss: 0.02960129827260971\n",
      "Epoch 13, Batch 50, Loss: 0.0336269773542881\n",
      "Epoch 13, Batch 60, Loss: 0.02433597669005394\n",
      "Epoch 13, Batch 70, Loss: 0.03448627516627312\n",
      "Epoch 13, Batch 80, Loss: 0.03308849781751633\n",
      "Epoch 13, Batch 90, Loss: 0.03241341561079025\n",
      "Epoch 13, Batch 100, Loss: 0.03342540189623833\n",
      "Epoch 14, Batch 0, Loss: 0.027822265401482582\n",
      "Epoch 14, Batch 10, Loss: 0.031002404168248177\n",
      "Epoch 14, Batch 20, Loss: 0.02266499772667885\n",
      "Epoch 14, Batch 30, Loss: 0.02722962573170662\n",
      "Epoch 14, Batch 40, Loss: 0.02918437123298645\n",
      "Epoch 14, Batch 50, Loss: 0.025880958884954453\n",
      "Epoch 14, Batch 60, Loss: 0.02621375024318695\n",
      "Epoch 14, Batch 70, Loss: 0.022581404075026512\n",
      "Epoch 14, Batch 80, Loss: 0.02683456428349018\n",
      "Epoch 14, Batch 90, Loss: 0.036916326731443405\n",
      "Epoch 14, Batch 100, Loss: 0.03496133163571358\n",
      "Epoch 15, Batch 0, Loss: 0.025037411600351334\n",
      "Epoch 15, Batch 10, Loss: 0.03569255396723747\n",
      "Epoch 15, Batch 20, Loss: 0.020258769392967224\n",
      "Epoch 15, Batch 30, Loss: 0.022537557408213615\n",
      "Epoch 15, Batch 40, Loss: 0.03146699443459511\n",
      "Epoch 15, Batch 50, Loss: 0.025801517069339752\n",
      "Epoch 15, Batch 60, Loss: 0.027482163161039352\n",
      "Epoch 15, Batch 70, Loss: 0.030644966289401054\n",
      "Epoch 15, Batch 80, Loss: 0.02623862586915493\n",
      "Epoch 15, Batch 90, Loss: 0.025367984548211098\n",
      "Epoch 15, Batch 100, Loss: 0.028058335185050964\n",
      "Epoch 16, Batch 0, Loss: 0.029046986252069473\n",
      "Epoch 16, Batch 10, Loss: 0.026802243664860725\n",
      "Epoch 16, Batch 20, Loss: 0.035430874675512314\n",
      "Epoch 16, Batch 30, Loss: 0.01936349831521511\n",
      "Epoch 16, Batch 40, Loss: 0.025863012298941612\n",
      "Epoch 16, Batch 50, Loss: 0.020711444318294525\n",
      "Epoch 16, Batch 60, Loss: 0.02794661559164524\n",
      "Epoch 16, Batch 70, Loss: 0.034697141498327255\n",
      "Epoch 16, Batch 80, Loss: 0.035654857754707336\n",
      "Epoch 16, Batch 90, Loss: 0.036402054131031036\n",
      "Epoch 16, Batch 100, Loss: 0.02870996668934822\n",
      "Epoch 17, Batch 0, Loss: 0.03098396584391594\n",
      "Epoch 17, Batch 10, Loss: 0.029614504426717758\n",
      "Epoch 17, Batch 20, Loss: 0.023818964138627052\n",
      "Epoch 17, Batch 30, Loss: 0.025332439690828323\n",
      "Epoch 17, Batch 40, Loss: 0.034223467111587524\n",
      "Epoch 17, Batch 50, Loss: 0.027432728558778763\n",
      "Epoch 17, Batch 60, Loss: 0.02434246614575386\n",
      "Epoch 17, Batch 70, Loss: 0.024818766862154007\n",
      "Epoch 17, Batch 80, Loss: 0.020071247592568398\n",
      "Epoch 17, Batch 90, Loss: 0.024726200848817825\n",
      "Epoch 17, Batch 100, Loss: 0.023918453603982925\n",
      "Epoch 18, Batch 0, Loss: 0.0407780222594738\n",
      "Epoch 18, Batch 10, Loss: 0.02792893908917904\n",
      "Epoch 18, Batch 20, Loss: 0.022717706859111786\n",
      "Epoch 18, Batch 30, Loss: 0.022533638402819633\n",
      "Epoch 18, Batch 40, Loss: 0.028504248708486557\n",
      "Epoch 18, Batch 50, Loss: 0.026856478303670883\n",
      "Epoch 18, Batch 60, Loss: 0.024403635412454605\n",
      "Epoch 18, Batch 70, Loss: 0.03312293812632561\n",
      "Epoch 18, Batch 80, Loss: 0.02984970435500145\n",
      "Epoch 18, Batch 90, Loss: 0.026794303208589554\n",
      "Epoch 18, Batch 100, Loss: 0.026867039501667023\n",
      "Epoch 19, Batch 0, Loss: 0.017508098855614662\n",
      "Epoch 19, Batch 10, Loss: 0.04464554414153099\n",
      "Epoch 19, Batch 20, Loss: 0.01857181452214718\n",
      "Epoch 19, Batch 30, Loss: 0.025129463523626328\n",
      "Epoch 19, Batch 40, Loss: 0.037962332367897034\n",
      "Epoch 19, Batch 50, Loss: 0.02168137952685356\n",
      "Epoch 19, Batch 60, Loss: 0.02063736878335476\n",
      "Epoch 19, Batch 70, Loss: 0.024397237226366997\n",
      "Epoch 19, Batch 80, Loss: 0.04245353490114212\n",
      "Epoch 19, Batch 90, Loss: 0.02863522619009018\n",
      "Epoch 19, Batch 100, Loss: 0.023415634408593178\n",
      "Epoch 20, Batch 0, Loss: 0.01915474608540535\n",
      "Epoch 20, Batch 10, Loss: 0.027045026421546936\n",
      "Epoch 20, Batch 20, Loss: 0.032156579196453094\n",
      "Epoch 20, Batch 30, Loss: 0.021045809611678123\n",
      "Epoch 20, Batch 40, Loss: 0.028548965230584145\n",
      "Epoch 20, Batch 50, Loss: 0.02665276639163494\n",
      "Epoch 20, Batch 60, Loss: 0.03470176085829735\n",
      "Epoch 20, Batch 70, Loss: 0.025571143254637718\n",
      "Epoch 20, Batch 80, Loss: 0.023941898718476295\n",
      "Epoch 20, Batch 90, Loss: 0.03432347625494003\n",
      "Epoch 20, Batch 100, Loss: 0.029591219499707222\n",
      "Epoch 21, Batch 0, Loss: 0.023907678201794624\n",
      "Epoch 21, Batch 10, Loss: 0.033342234790325165\n",
      "Epoch 21, Batch 20, Loss: 0.03350939601659775\n",
      "Epoch 21, Batch 30, Loss: 0.02024219185113907\n",
      "Epoch 21, Batch 40, Loss: 0.02243073098361492\n",
      "Epoch 21, Batch 50, Loss: 0.027336077764630318\n",
      "Epoch 21, Batch 60, Loss: 0.017258256673812866\n",
      "Epoch 21, Batch 70, Loss: 0.020225781947374344\n",
      "Epoch 21, Batch 80, Loss: 0.027648933231830597\n",
      "Epoch 21, Batch 90, Loss: 0.022557850927114487\n",
      "Epoch 21, Batch 100, Loss: 0.027778036892414093\n",
      "Epoch 22, Batch 0, Loss: 0.020877033472061157\n",
      "Epoch 22, Batch 10, Loss: 0.02324768155813217\n",
      "Epoch 22, Batch 20, Loss: 0.0250466950237751\n",
      "Epoch 22, Batch 30, Loss: 0.031859610229730606\n",
      "Epoch 22, Batch 40, Loss: 0.035147830843925476\n",
      "Epoch 22, Batch 50, Loss: 0.0238503310829401\n",
      "Epoch 22, Batch 60, Loss: 0.020760463550686836\n",
      "Epoch 22, Batch 70, Loss: 0.023581236600875854\n",
      "Epoch 22, Batch 80, Loss: 0.02871864102780819\n",
      "Epoch 22, Batch 90, Loss: 0.028720900416374207\n",
      "Epoch 22, Batch 100, Loss: 0.026247158646583557\n",
      "Epoch 23, Batch 0, Loss: 0.033694490790367126\n",
      "Epoch 23, Batch 10, Loss: 0.020440122112631798\n",
      "Epoch 23, Batch 20, Loss: 0.020643409341573715\n",
      "Epoch 23, Batch 30, Loss: 0.01876640133559704\n",
      "Epoch 23, Batch 40, Loss: 0.026812635362148285\n",
      "Epoch 23, Batch 50, Loss: 0.022687554359436035\n",
      "Epoch 23, Batch 60, Loss: 0.02005881443619728\n",
      "Epoch 23, Batch 70, Loss: 0.029063843190670013\n",
      "Epoch 23, Batch 80, Loss: 0.020582962781190872\n",
      "Epoch 23, Batch 90, Loss: 0.025089865550398827\n",
      "Epoch 23, Batch 100, Loss: 0.02817799709737301\n",
      "Epoch 24, Batch 0, Loss: 0.025719257071614265\n",
      "Epoch 24, Batch 10, Loss: 0.025920262560248375\n",
      "Epoch 24, Batch 20, Loss: 0.021860633045434952\n",
      "Epoch 24, Batch 30, Loss: 0.033227793872356415\n",
      "Epoch 24, Batch 40, Loss: 0.025951411575078964\n",
      "Epoch 24, Batch 50, Loss: 0.03859902173280716\n",
      "Epoch 24, Batch 60, Loss: 0.02462795376777649\n",
      "Epoch 24, Batch 70, Loss: 0.023037198930978775\n",
      "Epoch 24, Batch 80, Loss: 0.025404132902622223\n",
      "Epoch 24, Batch 90, Loss: 0.021892592310905457\n",
      "Epoch 24, Batch 100, Loss: 0.031021010130643845\n",
      "Epoch 25, Batch 0, Loss: 0.016418546438217163\n",
      "Epoch 25, Batch 10, Loss: 0.023580031469464302\n",
      "Epoch 25, Batch 20, Loss: 0.025815218687057495\n",
      "Epoch 25, Batch 30, Loss: 0.028392983600497246\n",
      "Epoch 25, Batch 40, Loss: 0.023144574835896492\n",
      "Epoch 25, Batch 50, Loss: 0.028498079627752304\n",
      "Epoch 25, Batch 60, Loss: 0.028285108506679535\n",
      "Epoch 25, Batch 70, Loss: 0.023188326507806778\n",
      "Epoch 25, Batch 80, Loss: 0.023613005876541138\n",
      "Epoch 25, Batch 90, Loss: 0.03128481283783913\n",
      "Epoch 25, Batch 100, Loss: 0.017957784235477448\n",
      "Epoch 26, Batch 0, Loss: 0.02912936732172966\n",
      "Epoch 26, Batch 10, Loss: 0.030734708532691002\n",
      "Epoch 26, Batch 20, Loss: 0.022033464163541794\n",
      "Epoch 26, Batch 30, Loss: 0.02716245874762535\n",
      "Epoch 26, Batch 40, Loss: 0.02358047105371952\n",
      "Epoch 26, Batch 50, Loss: 0.024744248017668724\n",
      "Epoch 26, Batch 60, Loss: 0.01983955316245556\n",
      "Epoch 26, Batch 70, Loss: 0.03159409761428833\n",
      "Epoch 26, Batch 80, Loss: 0.023982159793376923\n",
      "Epoch 26, Batch 90, Loss: 0.029672687873244286\n",
      "Epoch 26, Batch 100, Loss: 0.030928364023566246\n",
      "Epoch 27, Batch 0, Loss: 0.02923014387488365\n",
      "Epoch 27, Batch 10, Loss: 0.020811649039387703\n",
      "Epoch 27, Batch 20, Loss: 0.018732957541942596\n",
      "Epoch 27, Batch 30, Loss: 0.028880786150693893\n",
      "Epoch 27, Batch 40, Loss: 0.0364353321492672\n",
      "Epoch 27, Batch 50, Loss: 0.02760254591703415\n",
      "Epoch 27, Batch 60, Loss: 0.01958969607949257\n",
      "Epoch 27, Batch 70, Loss: 0.02488180622458458\n",
      "Epoch 27, Batch 80, Loss: 0.02139834128320217\n",
      "Epoch 27, Batch 90, Loss: 0.025571754202246666\n",
      "Epoch 27, Batch 100, Loss: 0.027493854984641075\n",
      "Epoch 28, Batch 0, Loss: 0.026924632489681244\n",
      "Epoch 28, Batch 10, Loss: 0.029946399852633476\n",
      "Epoch 28, Batch 20, Loss: 0.02238164283335209\n",
      "Epoch 28, Batch 30, Loss: 0.022143175825476646\n",
      "Epoch 28, Batch 40, Loss: 0.026232577860355377\n",
      "Epoch 28, Batch 50, Loss: 0.03491498529911041\n",
      "Epoch 28, Batch 60, Loss: 0.020763445645570755\n",
      "Epoch 28, Batch 70, Loss: 0.028221193701028824\n",
      "Epoch 28, Batch 80, Loss: 0.028551680967211723\n",
      "Epoch 28, Batch 90, Loss: 0.026495641097426414\n",
      "Epoch 28, Batch 100, Loss: 0.02226569689810276\n",
      "Epoch 29, Batch 0, Loss: 0.018626296892762184\n",
      "Epoch 29, Batch 10, Loss: 0.030215328559279442\n",
      "Epoch 29, Batch 20, Loss: 0.02405577152967453\n",
      "Epoch 29, Batch 30, Loss: 0.02519982121884823\n",
      "Epoch 29, Batch 40, Loss: 0.027605630457401276\n",
      "Epoch 29, Batch 50, Loss: 0.017771681770682335\n",
      "Epoch 29, Batch 60, Loss: 0.023783212527632713\n",
      "Epoch 29, Batch 70, Loss: 0.030242227017879486\n",
      "Epoch 29, Batch 80, Loss: 0.02926524542272091\n",
      "Epoch 29, Batch 90, Loss: 0.018676716834306717\n",
      "Epoch 29, Batch 100, Loss: 0.02323763817548752\n",
      "Epoch 30, Batch 0, Loss: 0.021737180650234222\n",
      "Epoch 30, Batch 10, Loss: 0.024507056921720505\n",
      "Epoch 30, Batch 20, Loss: 0.019423440098762512\n",
      "Epoch 30, Batch 30, Loss: 0.023884844034910202\n",
      "Epoch 30, Batch 40, Loss: 0.020429551601409912\n",
      "Epoch 30, Batch 50, Loss: 0.0282643660902977\n",
      "Epoch 30, Batch 60, Loss: 0.02490891143679619\n",
      "Epoch 30, Batch 70, Loss: 0.020413381978869438\n",
      "Epoch 30, Batch 80, Loss: 0.02343239262700081\n",
      "Epoch 30, Batch 90, Loss: 0.02622220106422901\n",
      "Epoch 30, Batch 100, Loss: 0.029290705919265747\n",
      "Epoch 31, Batch 0, Loss: 0.02501131407916546\n",
      "Epoch 31, Batch 10, Loss: 0.018434572964906693\n",
      "Epoch 31, Batch 20, Loss: 0.020751524716615677\n",
      "Epoch 31, Batch 30, Loss: 0.025449858978390694\n",
      "Epoch 31, Batch 40, Loss: 0.02940160036087036\n",
      "Epoch 31, Batch 50, Loss: 0.024055644869804382\n",
      "Epoch 31, Batch 60, Loss: 0.025456666946411133\n",
      "Epoch 31, Batch 70, Loss: 0.026413248851895332\n",
      "Epoch 31, Batch 80, Loss: 0.02306336723268032\n",
      "Epoch 31, Batch 90, Loss: 0.024929553270339966\n",
      "Epoch 31, Batch 100, Loss: 0.019502470269799232\n",
      "Epoch 32, Batch 0, Loss: 0.020728904753923416\n",
      "Epoch 32, Batch 10, Loss: 0.033775851130485535\n",
      "Epoch 32, Batch 20, Loss: 0.02214367687702179\n",
      "Epoch 32, Batch 30, Loss: 0.028516098856925964\n",
      "Epoch 32, Batch 40, Loss: 0.0256002489477396\n",
      "Epoch 32, Batch 50, Loss: 0.027547672390937805\n",
      "Epoch 32, Batch 60, Loss: 0.021417880430817604\n",
      "Epoch 32, Batch 70, Loss: 0.023960169404745102\n",
      "Epoch 32, Batch 80, Loss: 0.026030030101537704\n",
      "Epoch 32, Batch 90, Loss: 0.021593313664197922\n",
      "Epoch 32, Batch 100, Loss: 0.028667233884334564\n",
      "Epoch 33, Batch 0, Loss: 0.026634177193045616\n",
      "Epoch 33, Batch 10, Loss: 0.025506075471639633\n",
      "Epoch 33, Batch 20, Loss: 0.026207210496068\n",
      "Epoch 33, Batch 30, Loss: 0.019280044361948967\n",
      "Epoch 33, Batch 40, Loss: 0.020767133682966232\n",
      "Epoch 33, Batch 50, Loss: 0.027335522696375847\n",
      "Epoch 33, Batch 60, Loss: 0.018856797367334366\n",
      "Epoch 33, Batch 70, Loss: 0.024184463545680046\n",
      "Epoch 33, Batch 80, Loss: 0.026071159169077873\n",
      "Epoch 33, Batch 90, Loss: 0.026892337948083878\n",
      "Epoch 33, Batch 100, Loss: 0.024389367550611496\n",
      "Epoch 34, Batch 0, Loss: 0.01907462440431118\n",
      "Epoch 34, Batch 10, Loss: 0.021077731624245644\n",
      "Epoch 34, Batch 20, Loss: 0.03373975679278374\n",
      "Epoch 34, Batch 30, Loss: 0.02700454369187355\n",
      "Epoch 34, Batch 40, Loss: 0.018487192690372467\n",
      "Epoch 34, Batch 50, Loss: 0.02511611580848694\n",
      "Epoch 34, Batch 60, Loss: 0.021771475672721863\n",
      "Epoch 34, Batch 70, Loss: 0.024800511077046394\n",
      "Epoch 34, Batch 80, Loss: 0.019839763641357422\n",
      "Epoch 34, Batch 90, Loss: 0.027019893750548363\n",
      "Epoch 34, Batch 100, Loss: 0.03131873160600662\n",
      "Epoch 35, Batch 0, Loss: 0.02322942577302456\n",
      "Epoch 35, Batch 10, Loss: 0.0198184996843338\n",
      "Epoch 35, Batch 20, Loss: 0.0238823052495718\n",
      "Epoch 35, Batch 30, Loss: 0.023054739460349083\n",
      "Epoch 35, Batch 40, Loss: 0.023183438926935196\n",
      "Epoch 35, Batch 50, Loss: 0.026531144976615906\n",
      "Epoch 35, Batch 60, Loss: 0.022749772295355797\n",
      "Epoch 35, Batch 70, Loss: 0.030391257256269455\n",
      "Epoch 35, Batch 80, Loss: 0.02951672486960888\n",
      "Epoch 35, Batch 90, Loss: 0.030183056369423866\n",
      "Epoch 35, Batch 100, Loss: 0.02847035974264145\n",
      "Epoch 36, Batch 0, Loss: 0.0199133288115263\n",
      "Epoch 36, Batch 10, Loss: 0.019192002713680267\n",
      "Epoch 36, Batch 20, Loss: 0.02301909402012825\n",
      "Epoch 36, Batch 30, Loss: 0.021969838067889214\n",
      "Epoch 36, Batch 40, Loss: 0.028529100120067596\n",
      "Epoch 36, Batch 50, Loss: 0.01829778030514717\n",
      "Epoch 36, Batch 60, Loss: 0.025097480043768883\n",
      "Epoch 36, Batch 70, Loss: 0.020351791754364967\n",
      "Epoch 36, Batch 80, Loss: 0.02164621278643608\n",
      "Epoch 36, Batch 90, Loss: 0.021078389137983322\n",
      "Epoch 36, Batch 100, Loss: 0.027034636586904526\n",
      "Epoch 37, Batch 0, Loss: 0.023576630279421806\n",
      "Epoch 37, Batch 10, Loss: 0.03342205658555031\n",
      "Epoch 37, Batch 20, Loss: 0.0313476137816906\n",
      "Epoch 37, Batch 30, Loss: 0.024799536913633347\n",
      "Epoch 37, Batch 40, Loss: 0.01691836677491665\n",
      "Epoch 37, Batch 50, Loss: 0.027043912559747696\n",
      "Epoch 37, Batch 60, Loss: 0.018729370087385178\n",
      "Epoch 37, Batch 70, Loss: 0.02336587943136692\n",
      "Epoch 37, Batch 80, Loss: 0.024334821850061417\n",
      "Epoch 37, Batch 90, Loss: 0.01629299484193325\n",
      "Epoch 37, Batch 100, Loss: 0.023969462141394615\n",
      "Epoch 38, Batch 0, Loss: 0.020623264834284782\n",
      "Epoch 38, Batch 10, Loss: 0.03253813832998276\n",
      "Epoch 38, Batch 20, Loss: 0.02082265540957451\n",
      "Epoch 38, Batch 30, Loss: 0.016639526933431625\n",
      "Epoch 38, Batch 40, Loss: 0.025411512702703476\n",
      "Epoch 38, Batch 50, Loss: 0.022981766611337662\n",
      "Epoch 38, Batch 60, Loss: 0.027768846601247787\n",
      "Epoch 38, Batch 70, Loss: 0.028965258970856667\n",
      "Epoch 38, Batch 80, Loss: 0.019721653312444687\n",
      "Epoch 38, Batch 90, Loss: 0.023974282667040825\n",
      "Epoch 38, Batch 100, Loss: 0.02572629600763321\n",
      "Epoch 39, Batch 0, Loss: 0.022441022098064423\n",
      "Epoch 39, Batch 10, Loss: 0.021649161353707314\n",
      "Epoch 39, Batch 20, Loss: 0.019067568704485893\n",
      "Epoch 39, Batch 30, Loss: 0.02187475934624672\n",
      "Epoch 39, Batch 40, Loss: 0.023805469274520874\n",
      "Epoch 39, Batch 50, Loss: 0.02560235932469368\n",
      "Epoch 39, Batch 60, Loss: 0.022658824920654297\n",
      "Epoch 39, Batch 70, Loss: 0.02239779382944107\n",
      "Epoch 39, Batch 80, Loss: 0.021625889465212822\n",
      "Epoch 39, Batch 90, Loss: 0.02283962443470955\n",
      "Epoch 39, Batch 100, Loss: 0.021341519430279732\n",
      "Epoch 40, Batch 0, Loss: 0.02030310407280922\n",
      "Epoch 40, Batch 10, Loss: 0.029501521959900856\n",
      "Epoch 40, Batch 20, Loss: 0.026590976864099503\n",
      "Epoch 40, Batch 30, Loss: 0.04533062130212784\n",
      "Epoch 40, Batch 40, Loss: 0.02205893211066723\n",
      "Epoch 40, Batch 50, Loss: 0.029459558427333832\n",
      "Epoch 40, Batch 60, Loss: 0.030860424041748047\n",
      "Epoch 40, Batch 70, Loss: 0.023542385548353195\n",
      "Epoch 40, Batch 80, Loss: 0.01815447025001049\n",
      "Epoch 40, Batch 90, Loss: 0.02068723365664482\n",
      "Epoch 40, Batch 100, Loss: 0.019002487882971764\n",
      "Epoch 41, Batch 0, Loss: 0.02558152750134468\n",
      "Epoch 41, Batch 10, Loss: 0.025157339870929718\n",
      "Epoch 41, Batch 20, Loss: 0.017691567540168762\n",
      "Epoch 41, Batch 30, Loss: 0.03230047971010208\n",
      "Epoch 41, Batch 40, Loss: 0.030378632247447968\n",
      "Epoch 41, Batch 50, Loss: 0.021773602813482285\n",
      "Epoch 41, Batch 60, Loss: 0.028737038373947144\n",
      "Epoch 41, Batch 70, Loss: 0.021765245124697685\n",
      "Epoch 41, Batch 80, Loss: 0.02234039083123207\n",
      "Epoch 41, Batch 90, Loss: 0.02251308038830757\n",
      "Epoch 41, Batch 100, Loss: 0.023097947239875793\n",
      "Epoch 42, Batch 0, Loss: 0.023148292675614357\n",
      "Epoch 42, Batch 10, Loss: 0.03363446518778801\n",
      "Epoch 42, Batch 20, Loss: 0.02652139961719513\n",
      "Epoch 42, Batch 30, Loss: 0.020882315933704376\n",
      "Epoch 42, Batch 40, Loss: 0.021057842299342155\n",
      "Epoch 42, Batch 50, Loss: 0.026906393468379974\n",
      "Epoch 42, Batch 60, Loss: 0.01698813959956169\n",
      "Epoch 42, Batch 70, Loss: 0.02385086566209793\n",
      "Epoch 42, Batch 80, Loss: 0.023642124608159065\n",
      "Epoch 42, Batch 90, Loss: 0.02283908985555172\n",
      "Epoch 42, Batch 100, Loss: 0.027959313243627548\n",
      "Epoch 43, Batch 0, Loss: 0.02148670144379139\n",
      "Epoch 43, Batch 10, Loss: 0.028303096070885658\n",
      "Epoch 43, Batch 20, Loss: 0.028599150478839874\n",
      "Epoch 43, Batch 30, Loss: 0.02149614691734314\n",
      "Epoch 43, Batch 40, Loss: 0.026080206036567688\n",
      "Epoch 43, Batch 50, Loss: 0.03020070679485798\n",
      "Epoch 43, Batch 60, Loss: 0.02169092930853367\n",
      "Epoch 43, Batch 70, Loss: 0.018592990934848785\n",
      "Epoch 43, Batch 80, Loss: 0.024041449651122093\n",
      "Epoch 43, Batch 90, Loss: 0.024390416219830513\n",
      "Epoch 43, Batch 100, Loss: 0.031589411199092865\n",
      "Epoch 44, Batch 0, Loss: 0.020230518653988838\n",
      "Epoch 44, Batch 10, Loss: 0.023683570325374603\n",
      "Epoch 44, Batch 20, Loss: 0.024599120020866394\n",
      "Epoch 44, Batch 30, Loss: 0.035407740622758865\n",
      "Epoch 44, Batch 40, Loss: 0.03350251168012619\n",
      "Epoch 44, Batch 50, Loss: 0.02074574865400791\n",
      "Epoch 44, Batch 60, Loss: 0.027545519173145294\n",
      "Epoch 44, Batch 70, Loss: 0.020175160840153694\n",
      "Epoch 44, Batch 80, Loss: 0.019679799675941467\n",
      "Epoch 44, Batch 90, Loss: 0.02645738795399666\n",
      "Epoch 44, Batch 100, Loss: 0.028548823669552803\n",
      "Epoch 45, Batch 0, Loss: 0.029084082692861557\n",
      "Epoch 45, Batch 10, Loss: 0.026144251227378845\n",
      "Epoch 45, Batch 20, Loss: 0.02844206430017948\n",
      "Epoch 45, Batch 30, Loss: 0.027116624638438225\n",
      "Epoch 45, Batch 40, Loss: 0.021100478246808052\n",
      "Epoch 45, Batch 50, Loss: 0.022417603060603142\n",
      "Epoch 45, Batch 60, Loss: 0.027858253568410873\n",
      "Epoch 45, Batch 70, Loss: 0.022334372624754906\n",
      "Epoch 45, Batch 80, Loss: 0.023562252521514893\n",
      "Epoch 45, Batch 90, Loss: 0.025275813415646553\n",
      "Epoch 45, Batch 100, Loss: 0.022844405844807625\n",
      "Epoch 46, Batch 0, Loss: 0.026550326496362686\n",
      "Epoch 46, Batch 10, Loss: 0.02539835497736931\n",
      "Epoch 46, Batch 20, Loss: 0.01796378754079342\n",
      "Epoch 46, Batch 30, Loss: 0.019731061533093452\n",
      "Epoch 46, Batch 40, Loss: 0.025110770016908646\n",
      "Epoch 46, Batch 50, Loss: 0.019470708444714546\n",
      "Epoch 46, Batch 60, Loss: 0.029110565781593323\n",
      "Epoch 46, Batch 70, Loss: 0.030324086546897888\n",
      "Epoch 46, Batch 80, Loss: 0.028558753430843353\n",
      "Epoch 46, Batch 90, Loss: 0.028059998527169228\n",
      "Epoch 46, Batch 100, Loss: 0.017895318567752838\n",
      "Epoch 47, Batch 0, Loss: 0.023663509637117386\n",
      "Epoch 47, Batch 10, Loss: 0.030314579606056213\n",
      "Epoch 47, Batch 20, Loss: 0.03432542830705643\n",
      "Epoch 47, Batch 30, Loss: 0.026873964816331863\n",
      "Epoch 47, Batch 40, Loss: 0.022977540269494057\n",
      "Epoch 47, Batch 50, Loss: 0.026661466807127\n",
      "Epoch 47, Batch 60, Loss: 0.040018197149038315\n",
      "Epoch 47, Batch 70, Loss: 0.02040431648492813\n",
      "Epoch 47, Batch 80, Loss: 0.0239834263920784\n",
      "Epoch 47, Batch 90, Loss: 0.02668253518640995\n",
      "Epoch 47, Batch 100, Loss: 0.026889363303780556\n",
      "Epoch 48, Batch 0, Loss: 0.03362651914358139\n",
      "Epoch 48, Batch 10, Loss: 0.024526074528694153\n",
      "Epoch 48, Batch 20, Loss: 0.024116147309541702\n",
      "Epoch 48, Batch 30, Loss: 0.0233920905739069\n",
      "Epoch 48, Batch 40, Loss: 0.018905680626630783\n",
      "Epoch 48, Batch 50, Loss: 0.0307293813675642\n",
      "Epoch 48, Batch 60, Loss: 0.02338200993835926\n",
      "Epoch 48, Batch 70, Loss: 0.020555702969431877\n",
      "Epoch 48, Batch 80, Loss: 0.03279925882816315\n",
      "Epoch 48, Batch 90, Loss: 0.026056939736008644\n",
      "Epoch 48, Batch 100, Loss: 0.029187124222517014\n",
      "Epoch 49, Batch 0, Loss: 0.02675267495214939\n",
      "Epoch 49, Batch 10, Loss: 0.02586239203810692\n",
      "Epoch 49, Batch 20, Loss: 0.018927350640296936\n",
      "Epoch 49, Batch 30, Loss: 0.017149822786450386\n",
      "Epoch 49, Batch 40, Loss: 0.01839999295771122\n",
      "Epoch 49, Batch 50, Loss: 0.03442685678601265\n",
      "Epoch 49, Batch 60, Loss: 0.021627184003591537\n",
      "Epoch 49, Batch 70, Loss: 0.029885847121477127\n",
      "Epoch 49, Batch 80, Loss: 0.0243991632014513\n",
      "Epoch 49, Batch 90, Loss: 0.02338404953479767\n",
      "Epoch 49, Batch 100, Loss: 0.024421287700533867\n",
      "Epoch 50, Batch 0, Loss: 0.021187271922826767\n",
      "Epoch 50, Batch 10, Loss: 0.02675766497850418\n",
      "Epoch 50, Batch 20, Loss: 0.01860484853386879\n",
      "Epoch 50, Batch 30, Loss: 0.018645690754055977\n",
      "Epoch 50, Batch 40, Loss: 0.022233998402953148\n",
      "Epoch 50, Batch 50, Loss: 0.023105407133698463\n",
      "Epoch 50, Batch 60, Loss: 0.024842403829097748\n",
      "Epoch 50, Batch 70, Loss: 0.027087295427918434\n",
      "Epoch 50, Batch 80, Loss: 0.026726387441158295\n",
      "Epoch 50, Batch 90, Loss: 0.02137080393731594\n",
      "Epoch 50, Batch 100, Loss: 0.02678270824253559\n",
      "Epoch 51, Batch 0, Loss: 0.02763698250055313\n",
      "Epoch 51, Batch 10, Loss: 0.022386537864804268\n",
      "Epoch 51, Batch 20, Loss: 0.02522893436253071\n",
      "Epoch 51, Batch 30, Loss: 0.02342851087450981\n",
      "Epoch 51, Batch 40, Loss: 0.017277870327234268\n",
      "Epoch 51, Batch 50, Loss: 0.019710518419742584\n",
      "Epoch 51, Batch 60, Loss: 0.02690078690648079\n",
      "Epoch 51, Batch 70, Loss: 0.01826442778110504\n",
      "Epoch 51, Batch 80, Loss: 0.026570850983262062\n",
      "Epoch 51, Batch 90, Loss: 0.02637208253145218\n",
      "Epoch 51, Batch 100, Loss: 0.020150691270828247\n",
      "Epoch 52, Batch 0, Loss: 0.021196730434894562\n",
      "Epoch 52, Batch 10, Loss: 0.024750348180532455\n",
      "Epoch 52, Batch 20, Loss: 0.02810017392039299\n",
      "Epoch 52, Batch 30, Loss: 0.027000539004802704\n",
      "Epoch 52, Batch 40, Loss: 0.018246646970510483\n",
      "Epoch 52, Batch 50, Loss: 0.026291923597455025\n",
      "Epoch 52, Batch 60, Loss: 0.018440386280417442\n",
      "Epoch 52, Batch 70, Loss: 0.026112157851457596\n"
     ]
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.039798; Test RMSE 40.119644\n",
      "\n",
      "Train  MAE: 0.022829; Test  MAE 28.328704\n",
      "\n",
      "Train  R^2: 0.998414; Test  R^2 0.964476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
