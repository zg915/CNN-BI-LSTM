{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_preprocessing import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6618, 10, 8]) torch.Size([6618, 1]) torch.Size([490, 10, 8]) (490, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, y_scaler, train_loader, test_loader = get_data(\n",
    "            timestep = 10,\n",
    "            batch_size = 64,\n",
    "            y_name = 'Closing price',\n",
    "            train_size = 6627,\n",
    "            path = \"Data/SSE000001.csv\")\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    print (\"GPU device not found.\")\n",
    "\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer parameters from the table\n",
    "        self.conv1d = nn.Conv1d(in_channels= 8, out_channels= 32, kernel_size= 1, padding=\"same\")\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Pooling\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size= 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layer\n",
    "        x = self.conv1d(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64, lstm_dropout = 0.2, fc1_output_size = 16):\n",
    "        super(BILSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True,\n",
    "                             bidirectional = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size * 2, self.fc1_output_size)\n",
    "        self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        h_fc2 = self.fc2(h_fc1)\n",
    "        output = h_fc2[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64,  fc1_output_size = 16):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        # self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p = self.lstm_dropout)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size, self.fc1_output_size)\n",
    "        self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "        # h_dropout = self.dropout(h_lstm)\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        h_fc2 = self.fc2(h_fc1)\n",
    "        output = h_fc2[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.lstm = LSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x)\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Reshape x to fit BiLSTM input requirements\n",
    "        x = x.contiguous().view(batch_size, seq_len, -1)\n",
    "        x = self.lstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 20\n",
    "\n",
    "model = CNN_BiLSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.7840930223464966\n",
      "Epoch 0, Batch 10, Loss: 0.8014177083969116\n",
      "Epoch 0, Batch 20, Loss: 0.7768257856369019\n",
      "Epoch 0, Batch 30, Loss: 0.8865891695022583\n",
      "Epoch 0, Batch 40, Loss: 0.8622677326202393\n",
      "Epoch 0, Batch 50, Loss: 0.8164482116699219\n",
      "Epoch 0, Batch 60, Loss: 0.8551117181777954\n",
      "Epoch 0, Batch 70, Loss: 0.6956839561462402\n",
      "Epoch 0, Batch 80, Loss: 0.7023064494132996\n",
      "Epoch 0, Batch 90, Loss: 0.7838397026062012\n",
      "Epoch 0, Batch 100, Loss: 0.5964908003807068\n",
      "Epoch 1, Batch 0, Loss: 0.5842689275741577\n",
      "Epoch 1, Batch 10, Loss: 0.7036736607551575\n",
      "Epoch 1, Batch 20, Loss: 0.6841187477111816\n",
      "Epoch 1, Batch 30, Loss: 0.5703263282775879\n",
      "Epoch 1, Batch 40, Loss: 0.4365098476409912\n",
      "Epoch 1, Batch 50, Loss: 0.3237282633781433\n",
      "Epoch 1, Batch 60, Loss: 0.29942458868026733\n",
      "Epoch 1, Batch 70, Loss: 0.2904644012451172\n",
      "Epoch 1, Batch 80, Loss: 0.3288952112197876\n",
      "Epoch 1, Batch 90, Loss: 0.320485383272171\n",
      "Epoch 1, Batch 100, Loss: 0.23158499598503113\n",
      "Epoch 2, Batch 0, Loss: 0.2220010757446289\n",
      "Epoch 2, Batch 10, Loss: 0.2161215990781784\n",
      "Epoch 2, Batch 20, Loss: 0.21933747828006744\n",
      "Epoch 2, Batch 30, Loss: 0.16747461259365082\n",
      "Epoch 2, Batch 40, Loss: 0.25995784997940063\n",
      "Epoch 2, Batch 50, Loss: 0.19802027940750122\n",
      "Epoch 2, Batch 60, Loss: 0.11886626482009888\n",
      "Epoch 2, Batch 70, Loss: 0.1829959750175476\n",
      "Epoch 2, Batch 80, Loss: 0.15284469723701477\n",
      "Epoch 2, Batch 90, Loss: 0.1285065859556198\n",
      "Epoch 2, Batch 100, Loss: 0.18560223281383514\n",
      "Epoch 3, Batch 0, Loss: 0.16278953850269318\n",
      "Epoch 3, Batch 10, Loss: 0.12115617096424103\n",
      "Epoch 3, Batch 20, Loss: 0.10893672704696655\n",
      "Epoch 3, Batch 30, Loss: 0.07700735330581665\n",
      "Epoch 3, Batch 40, Loss: 0.09845930337905884\n",
      "Epoch 3, Batch 50, Loss: 0.11334249377250671\n",
      "Epoch 3, Batch 60, Loss: 0.17543116211891174\n",
      "Epoch 3, Batch 70, Loss: 0.09709711372852325\n",
      "Epoch 3, Batch 80, Loss: 0.06791544705629349\n",
      "Epoch 3, Batch 90, Loss: 0.07147859036922455\n",
      "Epoch 3, Batch 100, Loss: 0.07320839166641235\n",
      "Epoch 4, Batch 0, Loss: 0.06669437140226364\n",
      "Epoch 4, Batch 10, Loss: 0.13710786402225494\n",
      "Epoch 4, Batch 20, Loss: 0.07189164310693741\n",
      "Epoch 4, Batch 30, Loss: 0.06574854999780655\n",
      "Epoch 4, Batch 40, Loss: 0.07960300147533417\n",
      "Epoch 4, Batch 50, Loss: 0.03997256979346275\n",
      "Epoch 4, Batch 60, Loss: 0.047860197722911835\n",
      "Epoch 4, Batch 70, Loss: 0.052406758069992065\n",
      "Epoch 4, Batch 80, Loss: 0.041940901428461075\n",
      "Epoch 4, Batch 90, Loss: 0.04671909287571907\n",
      "Epoch 4, Batch 100, Loss: 0.05992879346013069\n",
      "Epoch 5, Batch 0, Loss: 0.05299455672502518\n",
      "Epoch 5, Batch 10, Loss: 0.060987040400505066\n",
      "Epoch 5, Batch 20, Loss: 0.10005970299243927\n",
      "Epoch 5, Batch 30, Loss: 0.041080087423324585\n",
      "Epoch 5, Batch 40, Loss: 0.06598417460918427\n",
      "Epoch 5, Batch 50, Loss: 0.03434270992875099\n",
      "Epoch 5, Batch 60, Loss: 0.042130667716264725\n",
      "Epoch 5, Batch 70, Loss: 0.061515241861343384\n",
      "Epoch 5, Batch 80, Loss: 0.056604042649269104\n",
      "Epoch 5, Batch 90, Loss: 0.06027800589799881\n",
      "Epoch 5, Batch 100, Loss: 0.041322048753499985\n",
      "Epoch 6, Batch 0, Loss: 0.043555546551942825\n",
      "Epoch 6, Batch 10, Loss: 0.05519741773605347\n",
      "Epoch 6, Batch 20, Loss: 0.07030975073575974\n",
      "Epoch 6, Batch 30, Loss: 0.06627227365970612\n",
      "Epoch 6, Batch 40, Loss: 0.02882263995707035\n",
      "Epoch 6, Batch 50, Loss: 0.026154138147830963\n",
      "Epoch 6, Batch 60, Loss: 0.057067837566137314\n",
      "Epoch 6, Batch 70, Loss: 0.03151114284992218\n",
      "Epoch 6, Batch 80, Loss: 0.0416698195040226\n",
      "Epoch 6, Batch 90, Loss: 0.03100884146988392\n",
      "Epoch 6, Batch 100, Loss: 0.02729145810008049\n",
      "Epoch 7, Batch 0, Loss: 0.05940946936607361\n",
      "Epoch 7, Batch 10, Loss: 0.06383020430803299\n",
      "Epoch 7, Batch 20, Loss: 0.03218827769160271\n",
      "Epoch 7, Batch 30, Loss: 0.05089619755744934\n",
      "Epoch 7, Batch 40, Loss: 0.04274088144302368\n",
      "Epoch 7, Batch 50, Loss: 0.0820457935333252\n",
      "Epoch 7, Batch 60, Loss: 0.03990470990538597\n",
      "Epoch 7, Batch 70, Loss: 0.02728075534105301\n",
      "Epoch 7, Batch 80, Loss: 0.022444050759077072\n",
      "Epoch 7, Batch 90, Loss: 0.03954874724149704\n",
      "Epoch 7, Batch 100, Loss: 0.047364331781864166\n",
      "Epoch 8, Batch 0, Loss: 0.03605439141392708\n",
      "Epoch 8, Batch 10, Loss: 0.03904745355248451\n",
      "Epoch 8, Batch 20, Loss: 0.03174605593085289\n",
      "Epoch 8, Batch 30, Loss: 0.05649478733539581\n",
      "Epoch 8, Batch 40, Loss: 0.05465812608599663\n",
      "Epoch 8, Batch 50, Loss: 0.03900102898478508\n",
      "Epoch 8, Batch 60, Loss: 0.05054200068116188\n",
      "Epoch 8, Batch 70, Loss: 0.026922892779111862\n",
      "Epoch 8, Batch 80, Loss: 0.04950277879834175\n",
      "Epoch 8, Batch 90, Loss: 0.03004569560289383\n",
      "Epoch 8, Batch 100, Loss: 0.032248929142951965\n",
      "Epoch 9, Batch 0, Loss: 0.021653924137353897\n",
      "Epoch 9, Batch 10, Loss: 0.03213345259428024\n",
      "Epoch 9, Batch 20, Loss: 0.04551514983177185\n",
      "Epoch 9, Batch 30, Loss: 0.04257259890437126\n",
      "Epoch 9, Batch 40, Loss: 0.03804527968168259\n",
      "Epoch 9, Batch 50, Loss: 0.02940673753619194\n",
      "Epoch 9, Batch 60, Loss: 0.043068401515483856\n",
      "Epoch 9, Batch 70, Loss: 0.033169329166412354\n",
      "Epoch 9, Batch 80, Loss: 0.028134502470493317\n",
      "Epoch 9, Batch 90, Loss: 0.03422636166214943\n",
      "Epoch 9, Batch 100, Loss: 0.035532064735889435\n",
      "Epoch 10, Batch 0, Loss: 0.0360882431268692\n",
      "Epoch 10, Batch 10, Loss: 0.0424492321908474\n",
      "Epoch 10, Batch 20, Loss: 0.027860721573233604\n",
      "Epoch 10, Batch 30, Loss: 0.03505745157599449\n",
      "Epoch 10, Batch 40, Loss: 0.03762717545032501\n",
      "Epoch 10, Batch 50, Loss: 0.02924184501171112\n",
      "Epoch 10, Batch 60, Loss: 0.02783915027976036\n",
      "Epoch 10, Batch 70, Loss: 0.0351194366812706\n",
      "Epoch 10, Batch 80, Loss: 0.02878071367740631\n",
      "Epoch 10, Batch 90, Loss: 0.03624095767736435\n",
      "Epoch 10, Batch 100, Loss: 0.035695549100637436\n",
      "Epoch 11, Batch 0, Loss: 0.028422251343727112\n",
      "Epoch 11, Batch 10, Loss: 0.03879635035991669\n",
      "Epoch 11, Batch 20, Loss: 0.02897658944129944\n",
      "Epoch 11, Batch 30, Loss: 0.029305992648005486\n",
      "Epoch 11, Batch 40, Loss: 0.04193832725286484\n",
      "Epoch 11, Batch 50, Loss: 0.030276071280241013\n",
      "Epoch 11, Batch 60, Loss: 0.025302113965153694\n",
      "Epoch 11, Batch 70, Loss: 0.03274739533662796\n",
      "Epoch 11, Batch 80, Loss: 0.03389105200767517\n",
      "Epoch 11, Batch 90, Loss: 0.028731126338243484\n",
      "Epoch 11, Batch 100, Loss: 0.02108551189303398\n",
      "Epoch 12, Batch 0, Loss: 0.022842271253466606\n",
      "Epoch 12, Batch 10, Loss: 0.028293021023273468\n",
      "Epoch 12, Batch 20, Loss: 0.03852641209959984\n",
      "Epoch 12, Batch 30, Loss: 0.025922520086169243\n",
      "Epoch 12, Batch 40, Loss: 0.02719385176897049\n",
      "Epoch 12, Batch 50, Loss: 0.02397284284234047\n",
      "Epoch 12, Batch 60, Loss: 0.026566969230771065\n",
      "Epoch 12, Batch 70, Loss: 0.0239203330129385\n",
      "Epoch 12, Batch 80, Loss: 0.03226405009627342\n",
      "Epoch 12, Batch 90, Loss: 0.026308365166187286\n",
      "Epoch 12, Batch 100, Loss: 0.028839368373155594\n",
      "Epoch 13, Batch 0, Loss: 0.03686048090457916\n",
      "Epoch 13, Batch 10, Loss: 0.02761620096862316\n",
      "Epoch 13, Batch 20, Loss: 0.02531198039650917\n",
      "Epoch 13, Batch 30, Loss: 0.026706086471676826\n",
      "Epoch 13, Batch 40, Loss: 0.03308195620775223\n",
      "Epoch 13, Batch 50, Loss: 0.03181998059153557\n",
      "Epoch 13, Batch 60, Loss: 0.026146169751882553\n",
      "Epoch 13, Batch 70, Loss: 0.037883203476667404\n",
      "Epoch 13, Batch 80, Loss: 0.024257779121398926\n",
      "Epoch 13, Batch 90, Loss: 0.03547968715429306\n",
      "Epoch 13, Batch 100, Loss: 0.02424907125532627\n",
      "Epoch 14, Batch 0, Loss: 0.0209822878241539\n",
      "Epoch 14, Batch 10, Loss: 0.018535783514380455\n",
      "Epoch 14, Batch 20, Loss: 0.02658761292695999\n",
      "Epoch 14, Batch 30, Loss: 0.028797879815101624\n",
      "Epoch 14, Batch 40, Loss: 0.017892351374030113\n",
      "Epoch 14, Batch 50, Loss: 0.03926027566194534\n",
      "Epoch 14, Batch 60, Loss: 0.030571432784199715\n",
      "Epoch 14, Batch 70, Loss: 0.020234597846865654\n",
      "Epoch 14, Batch 80, Loss: 0.023360051214694977\n",
      "Epoch 14, Batch 90, Loss: 0.029597405344247818\n",
      "Epoch 14, Batch 100, Loss: 0.03167310729622841\n",
      "Epoch 15, Batch 0, Loss: 0.029810791835188866\n",
      "Epoch 15, Batch 10, Loss: 0.021572349593043327\n",
      "Epoch 15, Batch 20, Loss: 0.026439445093274117\n",
      "Epoch 15, Batch 30, Loss: 0.022843798622488976\n",
      "Epoch 15, Batch 40, Loss: 0.02257387898862362\n",
      "Epoch 15, Batch 50, Loss: 0.03177376464009285\n",
      "Epoch 15, Batch 60, Loss: 0.03271616995334625\n",
      "Epoch 15, Batch 70, Loss: 0.03309869393706322\n",
      "Epoch 15, Batch 80, Loss: 0.025850558653473854\n",
      "Epoch 15, Batch 90, Loss: 0.024748368188738823\n",
      "Epoch 15, Batch 100, Loss: 0.024464011192321777\n",
      "Epoch 16, Batch 0, Loss: 0.019423535093665123\n",
      "Epoch 16, Batch 10, Loss: 0.035814568400382996\n",
      "Epoch 16, Batch 20, Loss: 0.021580638363957405\n",
      "Epoch 16, Batch 30, Loss: 0.021749988198280334\n",
      "Epoch 16, Batch 40, Loss: 0.02398129552602768\n",
      "Epoch 16, Batch 50, Loss: 0.023998644202947617\n",
      "Epoch 16, Batch 60, Loss: 0.03605956584215164\n",
      "Epoch 16, Batch 70, Loss: 0.035315029323101044\n",
      "Epoch 16, Batch 80, Loss: 0.027835994958877563\n",
      "Epoch 16, Batch 90, Loss: 0.030517172068357468\n",
      "Epoch 16, Batch 100, Loss: 0.03482774272561073\n",
      "Epoch 17, Batch 0, Loss: 0.02083296701312065\n",
      "Epoch 17, Batch 10, Loss: 0.030627062544226646\n",
      "Epoch 17, Batch 20, Loss: 0.03161895275115967\n",
      "Epoch 17, Batch 30, Loss: 0.02789175882935524\n",
      "Epoch 17, Batch 40, Loss: 0.028157617896795273\n",
      "Epoch 17, Batch 50, Loss: 0.025259383022785187\n",
      "Epoch 17, Batch 60, Loss: 0.02705877460539341\n",
      "Epoch 17, Batch 70, Loss: 0.02762998454272747\n",
      "Epoch 17, Batch 80, Loss: 0.02295864000916481\n",
      "Epoch 17, Batch 90, Loss: 0.03224433958530426\n",
      "Epoch 17, Batch 100, Loss: 0.025994978845119476\n",
      "Epoch 18, Batch 0, Loss: 0.02464369870722294\n",
      "Epoch 18, Batch 10, Loss: 0.029130829498171806\n",
      "Epoch 18, Batch 20, Loss: 0.026397569105029106\n",
      "Epoch 18, Batch 30, Loss: 0.021886423230171204\n",
      "Epoch 18, Batch 40, Loss: 0.02684597112238407\n",
      "Epoch 18, Batch 50, Loss: 0.030159682035446167\n",
      "Epoch 18, Batch 60, Loss: 0.02673587016761303\n",
      "Epoch 18, Batch 70, Loss: 0.03565407171845436\n",
      "Epoch 18, Batch 80, Loss: 0.022791557013988495\n",
      "Epoch 18, Batch 90, Loss: 0.023847727105021477\n",
      "Epoch 18, Batch 100, Loss: 0.02926487848162651\n",
      "Epoch 19, Batch 0, Loss: 0.01888856664299965\n",
      "Epoch 19, Batch 10, Loss: 0.020527103915810585\n",
      "Epoch 19, Batch 20, Loss: 0.03348088264465332\n",
      "Epoch 19, Batch 30, Loss: 0.029581982642412186\n",
      "Epoch 19, Batch 40, Loss: 0.025050587952136993\n",
      "Epoch 19, Batch 50, Loss: 0.026898309588432312\n",
      "Epoch 19, Batch 60, Loss: 0.030824968591332436\n",
      "Epoch 19, Batch 70, Loss: 0.03290911763906479\n",
      "Epoch 19, Batch 80, Loss: 0.040589891374111176\n",
      "Epoch 19, Batch 90, Loss: 0.0286277923732996\n",
      "Epoch 19, Batch 100, Loss: 0.0224449560046196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_BiLSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True)\n",
       "    (fc1): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.048857; Test RMSE 52.625295\n",
      "\n",
      "Train  MAE: 0.026714; Test  MAE 38.450717\n",
      "\n",
      "Train  R^2: 0.997588; Test  R^2 0.932287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
