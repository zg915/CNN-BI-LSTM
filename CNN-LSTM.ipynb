{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_preprocessing import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6618, 10, 8]) torch.Size([6618, 1]) torch.Size([490, 10, 8]) (490, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, y_scaler, train_loader, test_loader = get_data(\n",
    "            timestep = 10,\n",
    "            batch_size = 64,\n",
    "            y_name = 'Closing price',\n",
    "            train_size = 6627,\n",
    "            path = \"Data/SSE000001.csv\")\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    print (\"GPU device not found.\")\n",
    "\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer parameters from the table\n",
    "        self.conv1d = nn.Conv1d(in_channels= 8, out_channels= 32, kernel_size= 1, padding=\"same\")\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Pooling\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size= 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layer\n",
    "        x = self.conv1d(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64, lstm_dropout = 0.2, fc1_output_size = 16):\n",
    "        super(BILSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True,\n",
    "                             bidirectional = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size * 2, self.fc1_output_size)\n",
    "        self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        h_fc2 = self.fc2(h_fc1)\n",
    "        output = h_fc2[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64,  fc1_output_size = 16):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        # self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p = self.lstm_dropout)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size, self.fc1_output_size)\n",
    "        self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "        # h_dropout = self.dropout(h_lstm)\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        h_fc2 = self.fc2(h_fc1)\n",
    "        output = h_fc2[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.lstm = LSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x)\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Reshape x to fit BiLSTM input requirements\n",
    "        x = x.contiguous().view(batch_size, seq_len, -1)\n",
    "        x = self.lstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_BiLSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.7404946088790894\n",
      "Epoch 0, Batch 10, Loss: 0.6889724731445312\n",
      "Epoch 0, Batch 20, Loss: 0.784409761428833\n",
      "Epoch 0, Batch 30, Loss: 0.8748260736465454\n",
      "Epoch 0, Batch 40, Loss: 0.6092668175697327\n",
      "Epoch 0, Batch 50, Loss: 0.7252610325813293\n",
      "Epoch 0, Batch 60, Loss: 0.7906787991523743\n",
      "Epoch 0, Batch 70, Loss: 0.7357273101806641\n",
      "Epoch 0, Batch 80, Loss: 0.7913323640823364\n",
      "Epoch 0, Batch 90, Loss: 0.5281060934066772\n",
      "Epoch 0, Batch 100, Loss: 0.6204863786697388\n",
      "Epoch 1, Batch 0, Loss: 0.7089163064956665\n",
      "Epoch 1, Batch 10, Loss: 0.5858460664749146\n",
      "Epoch 1, Batch 20, Loss: 0.555395781993866\n",
      "Epoch 1, Batch 30, Loss: 0.46581602096557617\n",
      "Epoch 1, Batch 40, Loss: 0.3337157070636749\n",
      "Epoch 1, Batch 50, Loss: 0.3966148793697357\n",
      "Epoch 1, Batch 60, Loss: 0.3564123213291168\n",
      "Epoch 1, Batch 70, Loss: 0.31215155124664307\n",
      "Epoch 1, Batch 80, Loss: 0.19650158286094666\n",
      "Epoch 1, Batch 90, Loss: 0.21869659423828125\n",
      "Epoch 1, Batch 100, Loss: 0.18870683014392853\n",
      "Epoch 2, Batch 0, Loss: 0.17678187787532806\n",
      "Epoch 2, Batch 10, Loss: 0.1257355511188507\n",
      "Epoch 2, Batch 20, Loss: 0.1732378900051117\n",
      "Epoch 2, Batch 30, Loss: 0.1230427622795105\n",
      "Epoch 2, Batch 40, Loss: 0.10572751611471176\n",
      "Epoch 2, Batch 50, Loss: 0.1319698840379715\n",
      "Epoch 2, Batch 60, Loss: 0.10642470419406891\n",
      "Epoch 2, Batch 70, Loss: 0.1393592804670334\n",
      "Epoch 2, Batch 80, Loss: 0.11150364577770233\n",
      "Epoch 2, Batch 90, Loss: 0.06165193021297455\n",
      "Epoch 2, Batch 100, Loss: 0.08449538052082062\n",
      "Epoch 3, Batch 0, Loss: 0.12750057876110077\n",
      "Epoch 3, Batch 10, Loss: 0.08022887259721756\n",
      "Epoch 3, Batch 20, Loss: 0.07975354045629501\n",
      "Epoch 3, Batch 30, Loss: 0.07106570154428482\n",
      "Epoch 3, Batch 40, Loss: 0.08716167509555817\n",
      "Epoch 3, Batch 50, Loss: 0.05465187132358551\n",
      "Epoch 3, Batch 60, Loss: 0.049365632236003876\n",
      "Epoch 3, Batch 70, Loss: 0.13299722969532013\n",
      "Epoch 3, Batch 80, Loss: 0.11170563846826553\n",
      "Epoch 3, Batch 90, Loss: 0.08714013546705246\n",
      "Epoch 3, Batch 100, Loss: 0.06418778747320175\n",
      "Epoch 4, Batch 0, Loss: 0.045187078416347504\n",
      "Epoch 4, Batch 10, Loss: 0.045490484684705734\n",
      "Epoch 4, Batch 20, Loss: 0.07716812193393707\n",
      "Epoch 4, Batch 30, Loss: 0.04186725616455078\n",
      "Epoch 4, Batch 40, Loss: 0.04226158186793327\n",
      "Epoch 4, Batch 50, Loss: 0.0670459046959877\n",
      "Epoch 4, Batch 60, Loss: 0.07349137961864471\n",
      "Epoch 4, Batch 70, Loss: 0.06532106548547745\n",
      "Epoch 4, Batch 80, Loss: 0.06788862496614456\n",
      "Epoch 4, Batch 90, Loss: 0.05950023606419563\n",
      "Epoch 4, Batch 100, Loss: 0.10080131888389587\n",
      "Epoch 5, Batch 0, Loss: 0.03883136436343193\n",
      "Epoch 5, Batch 10, Loss: 0.05002029985189438\n",
      "Epoch 5, Batch 20, Loss: 0.04215424135327339\n",
      "Epoch 5, Batch 30, Loss: 0.05750798434019089\n",
      "Epoch 5, Batch 40, Loss: 0.06899551302194595\n",
      "Epoch 5, Batch 50, Loss: 0.04037554934620857\n",
      "Epoch 5, Batch 60, Loss: 0.06720376014709473\n",
      "Epoch 5, Batch 70, Loss: 0.06202616170048714\n",
      "Epoch 5, Batch 80, Loss: 0.0496084950864315\n",
      "Epoch 5, Batch 90, Loss: 0.039617396891117096\n",
      "Epoch 5, Batch 100, Loss: 0.03802186995744705\n",
      "Epoch 6, Batch 0, Loss: 0.03780761733651161\n",
      "Epoch 6, Batch 10, Loss: 0.061512820422649384\n",
      "Epoch 6, Batch 20, Loss: 0.05812504142522812\n",
      "Epoch 6, Batch 30, Loss: 0.046355944126844406\n",
      "Epoch 6, Batch 40, Loss: 0.03504461422562599\n",
      "Epoch 6, Batch 50, Loss: 0.034179214388132095\n",
      "Epoch 6, Batch 60, Loss: 0.029482945799827576\n",
      "Epoch 6, Batch 70, Loss: 0.027721548452973366\n",
      "Epoch 6, Batch 80, Loss: 0.05427481234073639\n",
      "Epoch 6, Batch 90, Loss: 0.060979485511779785\n",
      "Epoch 6, Batch 100, Loss: 0.031318299472332\n",
      "Epoch 7, Batch 0, Loss: 0.041270092129707336\n",
      "Epoch 7, Batch 10, Loss: 0.03695220500230789\n",
      "Epoch 7, Batch 20, Loss: 0.033712465316057205\n",
      "Epoch 7, Batch 30, Loss: 0.04029019549489021\n",
      "Epoch 7, Batch 40, Loss: 0.04047910496592522\n",
      "Epoch 7, Batch 50, Loss: 0.03371014446020126\n",
      "Epoch 7, Batch 60, Loss: 0.030098896473646164\n",
      "Epoch 7, Batch 70, Loss: 0.035398803651332855\n",
      "Epoch 7, Batch 80, Loss: 0.02827884629368782\n",
      "Epoch 7, Batch 90, Loss: 0.04575730115175247\n",
      "Epoch 7, Batch 100, Loss: 0.02890399843454361\n",
      "Epoch 8, Batch 0, Loss: 0.03513838350772858\n",
      "Epoch 8, Batch 10, Loss: 0.03586730360984802\n",
      "Epoch 8, Batch 20, Loss: 0.023802587762475014\n",
      "Epoch 8, Batch 30, Loss: 0.017325060442090034\n",
      "Epoch 8, Batch 40, Loss: 0.03005366399884224\n",
      "Epoch 8, Batch 50, Loss: 0.029886960983276367\n",
      "Epoch 8, Batch 60, Loss: 0.03438800200819969\n",
      "Epoch 8, Batch 70, Loss: 0.03539114445447922\n",
      "Epoch 8, Batch 80, Loss: 0.03766225650906563\n",
      "Epoch 8, Batch 90, Loss: 0.038110341876745224\n",
      "Epoch 8, Batch 100, Loss: 0.03785976022481918\n",
      "Epoch 9, Batch 0, Loss: 0.03222010284662247\n",
      "Epoch 9, Batch 10, Loss: 0.03090623766183853\n",
      "Epoch 9, Batch 20, Loss: 0.030045704916119576\n",
      "Epoch 9, Batch 30, Loss: 0.028528785333037376\n",
      "Epoch 9, Batch 40, Loss: 0.025590676814317703\n",
      "Epoch 9, Batch 50, Loss: 0.04475392401218414\n",
      "Epoch 9, Batch 60, Loss: 0.040125370025634766\n",
      "Epoch 9, Batch 70, Loss: 0.039229683578014374\n",
      "Epoch 9, Batch 80, Loss: 0.03597809001803398\n",
      "Epoch 9, Batch 90, Loss: 0.03259807080030441\n",
      "Epoch 9, Batch 100, Loss: 0.027197860181331635\n",
      "Epoch 10, Batch 0, Loss: 0.035124342888593674\n",
      "Epoch 10, Batch 10, Loss: 0.027843140065670013\n",
      "Epoch 10, Batch 20, Loss: 0.02318369783461094\n",
      "Epoch 10, Batch 30, Loss: 0.031658023595809937\n",
      "Epoch 10, Batch 40, Loss: 0.03388603404164314\n",
      "Epoch 10, Batch 50, Loss: 0.03817293792963028\n",
      "Epoch 10, Batch 60, Loss: 0.03905477747321129\n",
      "Epoch 10, Batch 70, Loss: 0.026967205107212067\n",
      "Epoch 10, Batch 80, Loss: 0.031549129635095596\n",
      "Epoch 10, Batch 90, Loss: 0.035582270473241806\n",
      "Epoch 10, Batch 100, Loss: 0.03856232762336731\n",
      "Epoch 11, Batch 0, Loss: 0.02687940187752247\n",
      "Epoch 11, Batch 10, Loss: 0.02583109773695469\n",
      "Epoch 11, Batch 20, Loss: 0.030458388850092888\n",
      "Epoch 11, Batch 30, Loss: 0.027520956471562386\n",
      "Epoch 11, Batch 40, Loss: 0.034032002091407776\n",
      "Epoch 11, Batch 50, Loss: 0.027936438098549843\n",
      "Epoch 11, Batch 60, Loss: 0.0312720462679863\n",
      "Epoch 11, Batch 70, Loss: 0.035343896597623825\n",
      "Epoch 11, Batch 80, Loss: 0.029372122138738632\n",
      "Epoch 11, Batch 90, Loss: 0.024394739419221878\n",
      "Epoch 11, Batch 100, Loss: 0.021941103041172028\n",
      "Epoch 12, Batch 0, Loss: 0.025673825293779373\n",
      "Epoch 12, Batch 10, Loss: 0.0404888316988945\n",
      "Epoch 12, Batch 20, Loss: 0.03163996711373329\n",
      "Epoch 12, Batch 30, Loss: 0.04815292730927467\n",
      "Epoch 12, Batch 40, Loss: 0.026722844690084457\n",
      "Epoch 12, Batch 50, Loss: 0.022616010159254074\n",
      "Epoch 12, Batch 60, Loss: 0.03557178005576134\n",
      "Epoch 12, Batch 70, Loss: 0.027951549738645554\n",
      "Epoch 12, Batch 80, Loss: 0.028819192200899124\n",
      "Epoch 12, Batch 90, Loss: 0.02859024703502655\n",
      "Epoch 12, Batch 100, Loss: 0.027708793058991432\n",
      "Epoch 13, Batch 0, Loss: 0.02424379624426365\n",
      "Epoch 13, Batch 10, Loss: 0.024760620668530464\n",
      "Epoch 13, Batch 20, Loss: 0.03515544533729553\n",
      "Epoch 13, Batch 30, Loss: 0.038987353444099426\n",
      "Epoch 13, Batch 40, Loss: 0.02545267343521118\n",
      "Epoch 13, Batch 50, Loss: 0.048985354602336884\n",
      "Epoch 13, Batch 60, Loss: 0.026083601638674736\n",
      "Epoch 13, Batch 70, Loss: 0.029240313917398453\n",
      "Epoch 13, Batch 80, Loss: 0.030656013637781143\n",
      "Epoch 13, Batch 90, Loss: 0.03145133703947067\n",
      "Epoch 13, Batch 100, Loss: 0.02502192184329033\n",
      "Epoch 14, Batch 0, Loss: 0.024208534508943558\n",
      "Epoch 14, Batch 10, Loss: 0.024584416300058365\n",
      "Epoch 14, Batch 20, Loss: 0.04046783968806267\n",
      "Epoch 14, Batch 30, Loss: 0.03398050367832184\n",
      "Epoch 14, Batch 40, Loss: 0.02489541657269001\n",
      "Epoch 14, Batch 50, Loss: 0.02686723694205284\n",
      "Epoch 14, Batch 60, Loss: 0.03010965883731842\n",
      "Epoch 14, Batch 70, Loss: 0.029947776347398758\n",
      "Epoch 14, Batch 80, Loss: 0.02927851490676403\n",
      "Epoch 14, Batch 90, Loss: 0.02361314371228218\n",
      "Epoch 14, Batch 100, Loss: 0.02666766382753849\n",
      "Epoch 15, Batch 0, Loss: 0.03944291174411774\n",
      "Epoch 15, Batch 10, Loss: 0.033540938049554825\n",
      "Epoch 15, Batch 20, Loss: 0.029869072139263153\n",
      "Epoch 15, Batch 30, Loss: 0.029864951968193054\n",
      "Epoch 15, Batch 40, Loss: 0.025822453200817108\n",
      "Epoch 15, Batch 50, Loss: 0.03741605207324028\n",
      "Epoch 15, Batch 60, Loss: 0.03656763210892677\n",
      "Epoch 15, Batch 70, Loss: 0.027317829430103302\n",
      "Epoch 15, Batch 80, Loss: 0.03709206357598305\n",
      "Epoch 15, Batch 90, Loss: 0.03068910911679268\n",
      "Epoch 15, Batch 100, Loss: 0.02739345096051693\n",
      "Epoch 16, Batch 0, Loss: 0.030669910833239555\n",
      "Epoch 16, Batch 10, Loss: 0.0269954614341259\n",
      "Epoch 16, Batch 20, Loss: 0.03411734104156494\n",
      "Epoch 16, Batch 30, Loss: 0.029041431844234467\n",
      "Epoch 16, Batch 40, Loss: 0.02873987704515457\n",
      "Epoch 16, Batch 50, Loss: 0.02542772889137268\n",
      "Epoch 16, Batch 60, Loss: 0.024988757446408272\n",
      "Epoch 16, Batch 70, Loss: 0.02953350730240345\n",
      "Epoch 16, Batch 80, Loss: 0.02846558392047882\n",
      "Epoch 16, Batch 90, Loss: 0.031845904886722565\n",
      "Epoch 16, Batch 100, Loss: 0.03013918548822403\n",
      "Epoch 17, Batch 0, Loss: 0.02653089538216591\n",
      "Epoch 17, Batch 10, Loss: 0.021955082193017006\n",
      "Epoch 17, Batch 20, Loss: 0.025584649294614792\n",
      "Epoch 17, Batch 30, Loss: 0.027803823351860046\n",
      "Epoch 17, Batch 40, Loss: 0.0331517718732357\n",
      "Epoch 17, Batch 50, Loss: 0.026680752635002136\n",
      "Epoch 17, Batch 60, Loss: 0.03197208791971207\n",
      "Epoch 17, Batch 70, Loss: 0.021478528156876564\n",
      "Epoch 17, Batch 80, Loss: 0.026723694056272507\n",
      "Epoch 17, Batch 90, Loss: 0.026902969926595688\n",
      "Epoch 17, Batch 100, Loss: 0.032250307500362396\n",
      "Epoch 18, Batch 0, Loss: 0.027446504682302475\n",
      "Epoch 18, Batch 10, Loss: 0.026838595047593117\n",
      "Epoch 18, Batch 20, Loss: 0.022675782442092896\n",
      "Epoch 18, Batch 30, Loss: 0.02849184162914753\n",
      "Epoch 18, Batch 40, Loss: 0.027652766555547714\n",
      "Epoch 18, Batch 50, Loss: 0.03218194842338562\n",
      "Epoch 18, Batch 60, Loss: 0.02307751029729843\n",
      "Epoch 18, Batch 70, Loss: 0.029615409672260284\n",
      "Epoch 18, Batch 80, Loss: 0.027935612946748734\n",
      "Epoch 18, Batch 90, Loss: 0.02203025296330452\n",
      "Epoch 18, Batch 100, Loss: 0.02506459690630436\n",
      "Epoch 19, Batch 0, Loss: 0.0328114815056324\n",
      "Epoch 19, Batch 10, Loss: 0.027909008786082268\n",
      "Epoch 19, Batch 20, Loss: 0.04431099072098732\n",
      "Epoch 19, Batch 30, Loss: 0.0259642843157053\n",
      "Epoch 19, Batch 40, Loss: 0.031644683331251144\n",
      "Epoch 19, Batch 50, Loss: 0.029661228880286217\n",
      "Epoch 19, Batch 60, Loss: 0.02968408539891243\n",
      "Epoch 19, Batch 70, Loss: 0.03455309942364693\n",
      "Epoch 19, Batch 80, Loss: 0.020189369097352028\n",
      "Epoch 19, Batch 90, Loss: 0.028305530548095703\n",
      "Epoch 19, Batch 100, Loss: 0.029525037854909897\n",
      "Epoch 20, Batch 0, Loss: 0.023465082049369812\n",
      "Epoch 20, Batch 10, Loss: 0.034609269350767136\n",
      "Epoch 20, Batch 20, Loss: 0.02991158328950405\n",
      "Epoch 20, Batch 30, Loss: 0.017989570274949074\n",
      "Epoch 20, Batch 40, Loss: 0.026133643463253975\n",
      "Epoch 20, Batch 50, Loss: 0.02850143052637577\n",
      "Epoch 20, Batch 60, Loss: 0.03200700134038925\n",
      "Epoch 20, Batch 70, Loss: 0.02316291071474552\n",
      "Epoch 20, Batch 80, Loss: 0.032152365893125534\n",
      "Epoch 20, Batch 90, Loss: 0.03367338329553604\n",
      "Epoch 20, Batch 100, Loss: 0.03607998415827751\n",
      "Epoch 21, Batch 0, Loss: 0.03484266251325607\n",
      "Epoch 21, Batch 10, Loss: 0.026886403560638428\n",
      "Epoch 21, Batch 20, Loss: 0.020969724282622337\n",
      "Epoch 21, Batch 30, Loss: 0.025567924603819847\n",
      "Epoch 21, Batch 40, Loss: 0.030949844047427177\n",
      "Epoch 21, Batch 50, Loss: 0.024173902347683907\n",
      "Epoch 21, Batch 60, Loss: 0.024695919826626778\n",
      "Epoch 21, Batch 70, Loss: 0.024558259174227715\n",
      "Epoch 21, Batch 80, Loss: 0.0287337489426136\n",
      "Epoch 21, Batch 90, Loss: 0.029667817056179047\n",
      "Epoch 21, Batch 100, Loss: 0.02881556749343872\n",
      "Epoch 22, Batch 0, Loss: 0.026320386677980423\n",
      "Epoch 22, Batch 10, Loss: 0.026579931378364563\n",
      "Epoch 22, Batch 20, Loss: 0.03512544929981232\n",
      "Epoch 22, Batch 30, Loss: 0.03647585213184357\n",
      "Epoch 22, Batch 40, Loss: 0.025526145473122597\n",
      "Epoch 22, Batch 50, Loss: 0.030598392710089684\n",
      "Epoch 22, Batch 60, Loss: 0.019936347380280495\n",
      "Epoch 22, Batch 70, Loss: 0.025166481733322144\n",
      "Epoch 22, Batch 80, Loss: 0.03164587542414665\n",
      "Epoch 22, Batch 90, Loss: 0.03271222114562988\n",
      "Epoch 22, Batch 100, Loss: 0.02195761725306511\n",
      "Epoch 23, Batch 0, Loss: 0.025553397834300995\n",
      "Epoch 23, Batch 10, Loss: 0.03678913414478302\n",
      "Epoch 23, Batch 20, Loss: 0.029298629611730576\n",
      "Epoch 23, Batch 30, Loss: 0.02943887747824192\n",
      "Epoch 23, Batch 40, Loss: 0.027660494670271873\n",
      "Epoch 23, Batch 50, Loss: 0.018904749304056168\n",
      "Epoch 23, Batch 60, Loss: 0.03438296914100647\n",
      "Epoch 23, Batch 70, Loss: 0.020953238010406494\n",
      "Epoch 23, Batch 80, Loss: 0.026257874444127083\n",
      "Epoch 23, Batch 90, Loss: 0.031222106888890266\n",
      "Epoch 23, Batch 100, Loss: 0.02415640652179718\n",
      "Epoch 24, Batch 0, Loss: 0.02931814268231392\n",
      "Epoch 24, Batch 10, Loss: 0.02451135218143463\n",
      "Epoch 24, Batch 20, Loss: 0.02296939305961132\n",
      "Epoch 24, Batch 30, Loss: 0.029161222279071808\n",
      "Epoch 24, Batch 40, Loss: 0.029959188774228096\n",
      "Epoch 24, Batch 50, Loss: 0.029745351523160934\n",
      "Epoch 24, Batch 60, Loss: 0.024079469963908195\n",
      "Epoch 24, Batch 70, Loss: 0.020652903243899345\n",
      "Epoch 24, Batch 80, Loss: 0.02604123391211033\n",
      "Epoch 24, Batch 90, Loss: 0.023080259561538696\n",
      "Epoch 24, Batch 100, Loss: 0.03064778260886669\n",
      "Epoch 25, Batch 0, Loss: 0.027073677629232407\n",
      "Epoch 25, Batch 10, Loss: 0.03058263473212719\n",
      "Epoch 25, Batch 20, Loss: 0.01754794828593731\n",
      "Epoch 25, Batch 30, Loss: 0.024410057812929153\n",
      "Epoch 25, Batch 40, Loss: 0.02563663199543953\n",
      "Epoch 25, Batch 50, Loss: 0.02106400765478611\n",
      "Epoch 25, Batch 60, Loss: 0.03423115611076355\n",
      "Epoch 25, Batch 70, Loss: 0.023524407297372818\n",
      "Epoch 25, Batch 80, Loss: 0.03704391419887543\n",
      "Epoch 25, Batch 90, Loss: 0.02857864275574684\n",
      "Epoch 25, Batch 100, Loss: 0.027552640065550804\n",
      "Epoch 26, Batch 0, Loss: 0.02839675545692444\n",
      "Epoch 26, Batch 10, Loss: 0.021611832082271576\n",
      "Epoch 26, Batch 20, Loss: 0.02883465215563774\n",
      "Epoch 26, Batch 30, Loss: 0.027782130986452103\n",
      "Epoch 26, Batch 40, Loss: 0.021661797538399696\n",
      "Epoch 26, Batch 50, Loss: 0.021495910361409187\n",
      "Epoch 26, Batch 60, Loss: 0.02225712686777115\n",
      "Epoch 26, Batch 70, Loss: 0.021711910143494606\n",
      "Epoch 26, Batch 80, Loss: 0.024480368942022324\n",
      "Epoch 26, Batch 90, Loss: 0.01944073848426342\n",
      "Epoch 26, Batch 100, Loss: 0.03220102936029434\n",
      "Epoch 27, Batch 0, Loss: 0.021889703348279\n",
      "Epoch 27, Batch 10, Loss: 0.032374683767557144\n",
      "Epoch 27, Batch 20, Loss: 0.0251829344779253\n",
      "Epoch 27, Batch 30, Loss: 0.027940792962908745\n",
      "Epoch 27, Batch 40, Loss: 0.02267022803425789\n",
      "Epoch 27, Batch 50, Loss: 0.023784659802913666\n",
      "Epoch 27, Batch 60, Loss: 0.02388729900121689\n",
      "Epoch 27, Batch 70, Loss: 0.02977469004690647\n",
      "Epoch 27, Batch 80, Loss: 0.027486203238368034\n",
      "Epoch 27, Batch 90, Loss: 0.022613976150751114\n",
      "Epoch 27, Batch 100, Loss: 0.025095922872424126\n",
      "Epoch 28, Batch 0, Loss: 0.029182152822613716\n",
      "Epoch 28, Batch 10, Loss: 0.028100527822971344\n",
      "Epoch 28, Batch 20, Loss: 0.026980048045516014\n",
      "Epoch 28, Batch 30, Loss: 0.026233335956931114\n",
      "Epoch 28, Batch 40, Loss: 0.020345719531178474\n",
      "Epoch 28, Batch 50, Loss: 0.033860281109809875\n",
      "Epoch 28, Batch 60, Loss: 0.02125982567667961\n",
      "Epoch 28, Batch 70, Loss: 0.02806532382965088\n",
      "Epoch 28, Batch 80, Loss: 0.030673615634441376\n",
      "Epoch 28, Batch 90, Loss: 0.029293816536664963\n",
      "Epoch 28, Batch 100, Loss: 0.028288133442401886\n",
      "Epoch 29, Batch 0, Loss: 0.033119410276412964\n",
      "Epoch 29, Batch 10, Loss: 0.01806320808827877\n",
      "Epoch 29, Batch 20, Loss: 0.02680078148841858\n",
      "Epoch 29, Batch 30, Loss: 0.020958494395017624\n",
      "Epoch 29, Batch 40, Loss: 0.028214575722813606\n",
      "Epoch 29, Batch 50, Loss: 0.02760612964630127\n",
      "Epoch 29, Batch 60, Loss: 0.025078069418668747\n",
      "Epoch 29, Batch 70, Loss: 0.021419323980808258\n",
      "Epoch 29, Batch 80, Loss: 0.02641104720532894\n",
      "Epoch 29, Batch 90, Loss: 0.020596953108906746\n",
      "Epoch 29, Batch 100, Loss: 0.023032521829009056\n",
      "Epoch 30, Batch 0, Loss: 0.025707539170980453\n",
      "Epoch 30, Batch 10, Loss: 0.01861468143761158\n",
      "Epoch 30, Batch 20, Loss: 0.028166675940155983\n",
      "Epoch 30, Batch 30, Loss: 0.028687234967947006\n",
      "Epoch 30, Batch 40, Loss: 0.024295981973409653\n",
      "Epoch 30, Batch 50, Loss: 0.025759991258382797\n",
      "Epoch 30, Batch 60, Loss: 0.024882866069674492\n",
      "Epoch 30, Batch 70, Loss: 0.027399897575378418\n",
      "Epoch 30, Batch 80, Loss: 0.019488804042339325\n",
      "Epoch 30, Batch 90, Loss: 0.0217793807387352\n",
      "Epoch 30, Batch 100, Loss: 0.022490698844194412\n",
      "Epoch 31, Batch 0, Loss: 0.03586811199784279\n",
      "Epoch 31, Batch 10, Loss: 0.03175509721040726\n",
      "Epoch 31, Batch 20, Loss: 0.016924917697906494\n",
      "Epoch 31, Batch 30, Loss: 0.021075347438454628\n",
      "Epoch 31, Batch 40, Loss: 0.023672349750995636\n",
      "Epoch 31, Batch 50, Loss: 0.027998970821499825\n",
      "Epoch 31, Batch 60, Loss: 0.018705422058701515\n",
      "Epoch 31, Batch 70, Loss: 0.025425121188163757\n",
      "Epoch 31, Batch 80, Loss: 0.02753753773868084\n",
      "Epoch 31, Batch 90, Loss: 0.022556694224476814\n",
      "Epoch 31, Batch 100, Loss: 0.027523066848516464\n",
      "Epoch 32, Batch 0, Loss: 0.046770595014095306\n",
      "Epoch 32, Batch 10, Loss: 0.023630289360880852\n",
      "Epoch 32, Batch 20, Loss: 0.020781690254807472\n",
      "Epoch 32, Batch 30, Loss: 0.019038381054997444\n",
      "Epoch 32, Batch 40, Loss: 0.020657088607549667\n",
      "Epoch 32, Batch 50, Loss: 0.028418298810720444\n",
      "Epoch 32, Batch 60, Loss: 0.019819149747490883\n",
      "Epoch 32, Batch 70, Loss: 0.020178623497486115\n",
      "Epoch 32, Batch 80, Loss: 0.03317255154252052\n",
      "Epoch 32, Batch 90, Loss: 0.031033936887979507\n",
      "Epoch 32, Batch 100, Loss: 0.0224334467202425\n",
      "Epoch 33, Batch 0, Loss: 0.027191706001758575\n",
      "Epoch 33, Batch 10, Loss: 0.020171931013464928\n",
      "Epoch 33, Batch 20, Loss: 0.02087537944316864\n",
      "Epoch 33, Batch 30, Loss: 0.026788810268044472\n",
      "Epoch 33, Batch 40, Loss: 0.0313386395573616\n",
      "Epoch 33, Batch 50, Loss: 0.029716983437538147\n",
      "Epoch 33, Batch 60, Loss: 0.024195216596126556\n",
      "Epoch 33, Batch 70, Loss: 0.024762872606515884\n",
      "Epoch 33, Batch 80, Loss: 0.019312622025609016\n",
      "Epoch 33, Batch 90, Loss: 0.02523796260356903\n",
      "Epoch 33, Batch 100, Loss: 0.022254640236496925\n",
      "Epoch 34, Batch 0, Loss: 0.03772268444299698\n",
      "Epoch 34, Batch 10, Loss: 0.023579522967338562\n",
      "Epoch 34, Batch 20, Loss: 0.021421384066343307\n",
      "Epoch 34, Batch 30, Loss: 0.026189938187599182\n",
      "Epoch 34, Batch 40, Loss: 0.026876984164118767\n",
      "Epoch 34, Batch 50, Loss: 0.028179876506328583\n",
      "Epoch 34, Batch 60, Loss: 0.028257472440600395\n",
      "Epoch 34, Batch 70, Loss: 0.023702256381511688\n",
      "Epoch 34, Batch 80, Loss: 0.0318569652736187\n",
      "Epoch 34, Batch 90, Loss: 0.029691359028220177\n",
      "Epoch 34, Batch 100, Loss: 0.018707461655139923\n",
      "Epoch 35, Batch 0, Loss: 0.031230943277478218\n",
      "Epoch 35, Batch 10, Loss: 0.026830334216356277\n",
      "Epoch 35, Batch 20, Loss: 0.029082728549838066\n",
      "Epoch 35, Batch 30, Loss: 0.02128349430859089\n",
      "Epoch 35, Batch 40, Loss: 0.025924386456608772\n",
      "Epoch 35, Batch 50, Loss: 0.01861155778169632\n",
      "Epoch 35, Batch 60, Loss: 0.02507294900715351\n",
      "Epoch 35, Batch 70, Loss: 0.028714651241898537\n",
      "Epoch 35, Batch 80, Loss: 0.02489185333251953\n",
      "Epoch 35, Batch 90, Loss: 0.0282293688505888\n",
      "Epoch 35, Batch 100, Loss: 0.022780507802963257\n",
      "Epoch 36, Batch 0, Loss: 0.03555430471897125\n",
      "Epoch 36, Batch 10, Loss: 0.025307640433311462\n",
      "Epoch 36, Batch 20, Loss: 0.02316218614578247\n",
      "Epoch 36, Batch 30, Loss: 0.026623524725437164\n",
      "Epoch 36, Batch 40, Loss: 0.027326203882694244\n",
      "Epoch 36, Batch 50, Loss: 0.021274609491229057\n",
      "Epoch 36, Batch 60, Loss: 0.020038263872265816\n",
      "Epoch 36, Batch 70, Loss: 0.03171977400779724\n",
      "Epoch 36, Batch 80, Loss: 0.026538429781794548\n",
      "Epoch 36, Batch 90, Loss: 0.0259967353194952\n",
      "Epoch 36, Batch 100, Loss: 0.025635775178670883\n",
      "Epoch 37, Batch 0, Loss: 0.024295011535286903\n",
      "Epoch 37, Batch 10, Loss: 0.027606409043073654\n",
      "Epoch 37, Batch 20, Loss: 0.0273366030305624\n",
      "Epoch 37, Batch 30, Loss: 0.030230607837438583\n",
      "Epoch 37, Batch 40, Loss: 0.023962263017892838\n",
      "Epoch 37, Batch 50, Loss: 0.026408832520246506\n",
      "Epoch 37, Batch 60, Loss: 0.02671811357140541\n",
      "Epoch 37, Batch 70, Loss: 0.027416719123721123\n",
      "Epoch 37, Batch 80, Loss: 0.029728302732110023\n",
      "Epoch 37, Batch 90, Loss: 0.022755393758416176\n",
      "Epoch 37, Batch 100, Loss: 0.03153150528669357\n",
      "Epoch 38, Batch 0, Loss: 0.028576621785759926\n",
      "Epoch 38, Batch 10, Loss: 0.02134392224252224\n",
      "Epoch 38, Batch 20, Loss: 0.020260442048311234\n",
      "Epoch 38, Batch 30, Loss: 0.03569299355149269\n",
      "Epoch 38, Batch 40, Loss: 0.02660278230905533\n",
      "Epoch 38, Batch 50, Loss: 0.018400240689516068\n",
      "Epoch 38, Batch 60, Loss: 0.01807263121008873\n",
      "Epoch 38, Batch 70, Loss: 0.02408887818455696\n",
      "Epoch 38, Batch 80, Loss: 0.023156676441431046\n",
      "Epoch 38, Batch 90, Loss: 0.019514605402946472\n",
      "Epoch 38, Batch 100, Loss: 0.020115956664085388\n",
      "Epoch 39, Batch 0, Loss: 0.023234888911247253\n",
      "Epoch 39, Batch 10, Loss: 0.01739910989999771\n",
      "Epoch 39, Batch 20, Loss: 0.027255279943346977\n",
      "Epoch 39, Batch 30, Loss: 0.024117110297083855\n",
      "Epoch 39, Batch 40, Loss: 0.025266313925385475\n",
      "Epoch 39, Batch 50, Loss: 0.022772574797272682\n",
      "Epoch 39, Batch 60, Loss: 0.033089183270931244\n",
      "Epoch 39, Batch 70, Loss: 0.021940510720014572\n",
      "Epoch 39, Batch 80, Loss: 0.025482553988695145\n",
      "Epoch 39, Batch 90, Loss: 0.027178119868040085\n",
      "Epoch 39, Batch 100, Loss: 0.02416929416358471\n",
      "Epoch 40, Batch 0, Loss: 0.02396497130393982\n",
      "Epoch 40, Batch 10, Loss: 0.02859269082546234\n",
      "Epoch 40, Batch 20, Loss: 0.028441788628697395\n",
      "Epoch 40, Batch 30, Loss: 0.03206882253289223\n",
      "Epoch 40, Batch 40, Loss: 0.022811003029346466\n",
      "Epoch 40, Batch 50, Loss: 0.031786493957042694\n",
      "Epoch 40, Batch 60, Loss: 0.024328581988811493\n",
      "Epoch 40, Batch 70, Loss: 0.030639687553048134\n",
      "Epoch 40, Batch 80, Loss: 0.02085733972489834\n",
      "Epoch 40, Batch 90, Loss: 0.020901896059513092\n",
      "Epoch 40, Batch 100, Loss: 0.0203035157173872\n",
      "Epoch 41, Batch 0, Loss: 0.026032868772745132\n",
      "Epoch 41, Batch 10, Loss: 0.027015581727027893\n",
      "Epoch 41, Batch 20, Loss: 0.02915065921843052\n",
      "Epoch 41, Batch 30, Loss: 0.025952478870749474\n",
      "Epoch 41, Batch 40, Loss: 0.029343798756599426\n",
      "Epoch 41, Batch 50, Loss: 0.019783345982432365\n",
      "Epoch 41, Batch 60, Loss: 0.022212546318769455\n",
      "Epoch 41, Batch 70, Loss: 0.02805309183895588\n",
      "Epoch 41, Batch 80, Loss: 0.023757077753543854\n",
      "Epoch 41, Batch 90, Loss: 0.024664316326379776\n",
      "Epoch 41, Batch 100, Loss: 0.03634978458285332\n",
      "Epoch 42, Batch 0, Loss: 0.031987011432647705\n",
      "Epoch 42, Batch 10, Loss: 0.022583138197660446\n",
      "Epoch 42, Batch 20, Loss: 0.026880081743001938\n",
      "Epoch 42, Batch 30, Loss: 0.027796510607004166\n",
      "Epoch 42, Batch 40, Loss: 0.03598446771502495\n",
      "Epoch 42, Batch 50, Loss: 0.02608301304280758\n",
      "Epoch 42, Batch 60, Loss: 0.027014030143618584\n",
      "Epoch 42, Batch 70, Loss: 0.02767755091190338\n",
      "Epoch 42, Batch 80, Loss: 0.020059119910001755\n",
      "Epoch 42, Batch 90, Loss: 0.021435890346765518\n",
      "Epoch 42, Batch 100, Loss: 0.0254996195435524\n",
      "Epoch 43, Batch 0, Loss: 0.0296239722520113\n",
      "Epoch 43, Batch 10, Loss: 0.030327584594488144\n",
      "Epoch 43, Batch 20, Loss: 0.026597969233989716\n",
      "Epoch 43, Batch 30, Loss: 0.028048962354660034\n",
      "Epoch 43, Batch 40, Loss: 0.026840252801775932\n",
      "Epoch 43, Batch 50, Loss: 0.021543992683291435\n",
      "Epoch 43, Batch 60, Loss: 0.02633107826113701\n",
      "Epoch 43, Batch 70, Loss: 0.028278272598981857\n",
      "Epoch 43, Batch 80, Loss: 0.024281641468405724\n",
      "Epoch 43, Batch 90, Loss: 0.020438581705093384\n",
      "Epoch 43, Batch 100, Loss: 0.02734966203570366\n",
      "Epoch 44, Batch 0, Loss: 0.021838799118995667\n",
      "Epoch 44, Batch 10, Loss: 0.024385705590248108\n",
      "Epoch 44, Batch 20, Loss: 0.03116440773010254\n",
      "Epoch 44, Batch 30, Loss: 0.024315722286701202\n",
      "Epoch 44, Batch 40, Loss: 0.029320284724235535\n",
      "Epoch 44, Batch 50, Loss: 0.019694335758686066\n",
      "Epoch 44, Batch 60, Loss: 0.027040813118219376\n",
      "Epoch 44, Batch 70, Loss: 0.026348788291215897\n",
      "Epoch 44, Batch 80, Loss: 0.02989562228322029\n",
      "Epoch 44, Batch 90, Loss: 0.02680007927119732\n",
      "Epoch 44, Batch 100, Loss: 0.029111899435520172\n",
      "Epoch 45, Batch 0, Loss: 0.024413075298070908\n",
      "Epoch 45, Batch 10, Loss: 0.02758554369211197\n",
      "Epoch 45, Batch 20, Loss: 0.023400407284498215\n",
      "Epoch 45, Batch 30, Loss: 0.02414572611451149\n",
      "Epoch 45, Batch 40, Loss: 0.024930231273174286\n",
      "Epoch 45, Batch 50, Loss: 0.029005376622080803\n",
      "Epoch 45, Batch 60, Loss: 0.02875089831650257\n",
      "Epoch 45, Batch 70, Loss: 0.025450509041547775\n",
      "Epoch 45, Batch 80, Loss: 0.030030183494091034\n",
      "Epoch 45, Batch 90, Loss: 0.028858352452516556\n",
      "Epoch 45, Batch 100, Loss: 0.022121626883745193\n",
      "Epoch 46, Batch 0, Loss: 0.019211940467357635\n",
      "Epoch 46, Batch 10, Loss: 0.023640552535653114\n",
      "Epoch 46, Batch 20, Loss: 0.019796445965766907\n",
      "Epoch 46, Batch 30, Loss: 0.028933580964803696\n",
      "Epoch 46, Batch 40, Loss: 0.023972678929567337\n",
      "Epoch 46, Batch 50, Loss: 0.022352738305926323\n",
      "Epoch 46, Batch 60, Loss: 0.02234751358628273\n",
      "Epoch 46, Batch 70, Loss: 0.023536143824458122\n",
      "Epoch 46, Batch 80, Loss: 0.027186570689082146\n",
      "Epoch 46, Batch 90, Loss: 0.01849786937236786\n",
      "Epoch 46, Batch 100, Loss: 0.028529474511742592\n",
      "Epoch 47, Batch 0, Loss: 0.02055434137582779\n",
      "Epoch 47, Batch 10, Loss: 0.02040151320397854\n",
      "Epoch 47, Batch 20, Loss: 0.024625161662697792\n",
      "Epoch 47, Batch 30, Loss: 0.02271314151585102\n",
      "Epoch 47, Batch 40, Loss: 0.02686694823205471\n",
      "Epoch 47, Batch 50, Loss: 0.02672012709081173\n",
      "Epoch 47, Batch 60, Loss: 0.020758705213665962\n",
      "Epoch 47, Batch 70, Loss: 0.023667970672249794\n",
      "Epoch 47, Batch 80, Loss: 0.032087553292512894\n",
      "Epoch 47, Batch 90, Loss: 0.020155519247055054\n",
      "Epoch 47, Batch 100, Loss: 0.021889008581638336\n",
      "Epoch 48, Batch 0, Loss: 0.021254174411296844\n",
      "Epoch 48, Batch 10, Loss: 0.020491257309913635\n",
      "Epoch 48, Batch 20, Loss: 0.025478340685367584\n",
      "Epoch 48, Batch 30, Loss: 0.02250312641263008\n",
      "Epoch 48, Batch 40, Loss: 0.022043131291866302\n",
      "Epoch 48, Batch 50, Loss: 0.018635621294379234\n",
      "Epoch 48, Batch 60, Loss: 0.029352881014347076\n",
      "Epoch 48, Batch 70, Loss: 0.016720009967684746\n",
      "Epoch 48, Batch 80, Loss: 0.0322316437959671\n",
      "Epoch 48, Batch 90, Loss: 0.022403860464692116\n",
      "Epoch 48, Batch 100, Loss: 0.028206493705511093\n",
      "Epoch 49, Batch 0, Loss: 0.026812434196472168\n",
      "Epoch 49, Batch 10, Loss: 0.02410445176064968\n",
      "Epoch 49, Batch 20, Loss: 0.028729218989610672\n",
      "Epoch 49, Batch 30, Loss: 0.02602093107998371\n",
      "Epoch 49, Batch 40, Loss: 0.028465906158089638\n",
      "Epoch 49, Batch 50, Loss: 0.025714507326483727\n",
      "Epoch 49, Batch 60, Loss: 0.024781443178653717\n",
      "Epoch 49, Batch 70, Loss: 0.020018896088004112\n",
      "Epoch 49, Batch 80, Loss: 0.023601830005645752\n",
      "Epoch 49, Batch 90, Loss: 0.03569550812244415\n",
      "Epoch 49, Batch 100, Loss: 0.019781336188316345\n",
      "Epoch 50, Batch 0, Loss: 0.026290303096175194\n",
      "Epoch 50, Batch 10, Loss: 0.034428972750902176\n",
      "Epoch 50, Batch 20, Loss: 0.025116002187132835\n",
      "Epoch 50, Batch 30, Loss: 0.022459957748651505\n",
      "Epoch 50, Batch 40, Loss: 0.025488898158073425\n",
      "Epoch 50, Batch 50, Loss: 0.022514620795845985\n",
      "Epoch 50, Batch 60, Loss: 0.02182633988559246\n",
      "Epoch 50, Batch 70, Loss: 0.022533200681209564\n",
      "Epoch 50, Batch 80, Loss: 0.031595006585121155\n",
      "Epoch 50, Batch 90, Loss: 0.02759774960577488\n",
      "Epoch 50, Batch 100, Loss: 0.029013855382800102\n",
      "Epoch 51, Batch 0, Loss: 0.04068119823932648\n",
      "Epoch 51, Batch 10, Loss: 0.02794363535940647\n",
      "Epoch 51, Batch 20, Loss: 0.033739253878593445\n",
      "Epoch 51, Batch 30, Loss: 0.025889182463288307\n",
      "Epoch 51, Batch 40, Loss: 0.025255002081394196\n",
      "Epoch 51, Batch 50, Loss: 0.027863483875989914\n",
      "Epoch 51, Batch 60, Loss: 0.04072435200214386\n",
      "Epoch 51, Batch 70, Loss: 0.03539355844259262\n",
      "Epoch 51, Batch 80, Loss: 0.02384376898407936\n",
      "Epoch 51, Batch 90, Loss: 0.0238167904317379\n",
      "Epoch 51, Batch 100, Loss: 0.03266605734825134\n",
      "Epoch 52, Batch 0, Loss: 0.025295764207839966\n",
      "Epoch 52, Batch 10, Loss: 0.02875935658812523\n",
      "Epoch 52, Batch 20, Loss: 0.019877346232533455\n",
      "Epoch 52, Batch 30, Loss: 0.01872202195227146\n",
      "Epoch 52, Batch 40, Loss: 0.01611349731683731\n",
      "Epoch 52, Batch 50, Loss: 0.0266870129853487\n",
      "Epoch 52, Batch 60, Loss: 0.018289001658558846\n",
      "Epoch 52, Batch 70, Loss: 0.024254662916064262\n",
      "Epoch 52, Batch 80, Loss: 0.027817701920866966\n",
      "Epoch 52, Batch 90, Loss: 0.025311198085546494\n",
      "Epoch 52, Batch 100, Loss: 0.03199060633778572\n",
      "Epoch 53, Batch 0, Loss: 0.022211944684386253\n",
      "Epoch 53, Batch 10, Loss: 0.02127915807068348\n",
      "Epoch 53, Batch 20, Loss: 0.029949113726615906\n",
      "Epoch 53, Batch 30, Loss: 0.024902526289224625\n",
      "Epoch 53, Batch 40, Loss: 0.023344019427895546\n",
      "Epoch 53, Batch 50, Loss: 0.02313685603439808\n",
      "Epoch 53, Batch 60, Loss: 0.021736163645982742\n",
      "Epoch 53, Batch 70, Loss: 0.028463978320360184\n",
      "Epoch 53, Batch 80, Loss: 0.022101053968071938\n",
      "Epoch 53, Batch 90, Loss: 0.02742185816168785\n",
      "Epoch 53, Batch 100, Loss: 0.024026483297348022\n",
      "Epoch 54, Batch 0, Loss: 0.014943893998861313\n",
      "Epoch 54, Batch 10, Loss: 0.030504435300827026\n",
      "Epoch 54, Batch 20, Loss: 0.021189549937844276\n",
      "Epoch 54, Batch 30, Loss: 0.02369880862534046\n",
      "Epoch 54, Batch 40, Loss: 0.022465219721198082\n",
      "Epoch 54, Batch 50, Loss: 0.019824635237455368\n",
      "Epoch 54, Batch 60, Loss: 0.020097868517041206\n",
      "Epoch 54, Batch 70, Loss: 0.029171116650104523\n",
      "Epoch 54, Batch 80, Loss: 0.029009804129600525\n",
      "Epoch 54, Batch 90, Loss: 0.029021242633461952\n",
      "Epoch 54, Batch 100, Loss: 0.024234559386968613\n",
      "Epoch 55, Batch 0, Loss: 0.026156876236200333\n",
      "Epoch 55, Batch 10, Loss: 0.025290168821811676\n",
      "Epoch 55, Batch 20, Loss: 0.017449943348765373\n",
      "Epoch 55, Batch 30, Loss: 0.028245769441127777\n",
      "Epoch 55, Batch 40, Loss: 0.028381051495671272\n",
      "Epoch 55, Batch 50, Loss: 0.025730570778250694\n",
      "Epoch 55, Batch 60, Loss: 0.026790553703904152\n",
      "Epoch 55, Batch 70, Loss: 0.026197826489806175\n",
      "Epoch 55, Batch 80, Loss: 0.023655250668525696\n",
      "Epoch 55, Batch 90, Loss: 0.02371460758149624\n",
      "Epoch 55, Batch 100, Loss: 0.024340704083442688\n",
      "Epoch 56, Batch 0, Loss: 0.030127689242362976\n",
      "Epoch 56, Batch 10, Loss: 0.02779335528612137\n",
      "Epoch 56, Batch 20, Loss: 0.019516702741384506\n",
      "Epoch 56, Batch 30, Loss: 0.030104558914899826\n",
      "Epoch 56, Batch 40, Loss: 0.02970573864877224\n",
      "Epoch 56, Batch 50, Loss: 0.025819897651672363\n",
      "Epoch 56, Batch 60, Loss: 0.025065213441848755\n",
      "Epoch 56, Batch 70, Loss: 0.019258901476860046\n",
      "Epoch 56, Batch 80, Loss: 0.02314927987754345\n",
      "Epoch 56, Batch 90, Loss: 0.024713123217225075\n",
      "Epoch 56, Batch 100, Loss: 0.01706877537071705\n",
      "Epoch 57, Batch 0, Loss: 0.032060038298368454\n",
      "Epoch 57, Batch 10, Loss: 0.026154249906539917\n",
      "Epoch 57, Batch 20, Loss: 0.018691785633563995\n",
      "Epoch 57, Batch 30, Loss: 0.027703532949090004\n",
      "Epoch 57, Batch 40, Loss: 0.02541208453476429\n",
      "Epoch 57, Batch 50, Loss: 0.024273581802845\n",
      "Epoch 57, Batch 60, Loss: 0.027056600898504257\n",
      "Epoch 57, Batch 70, Loss: 0.019890183582901955\n",
      "Epoch 57, Batch 80, Loss: 0.024715829640626907\n",
      "Epoch 57, Batch 90, Loss: 0.021544503048062325\n",
      "Epoch 57, Batch 100, Loss: 0.0195535346865654\n",
      "Epoch 58, Batch 0, Loss: 0.02484111674129963\n",
      "Epoch 58, Batch 10, Loss: 0.025361036881804466\n",
      "Epoch 58, Batch 20, Loss: 0.022284114733338356\n",
      "Epoch 58, Batch 30, Loss: 0.02654903382062912\n",
      "Epoch 58, Batch 40, Loss: 0.02180822379887104\n",
      "Epoch 58, Batch 50, Loss: 0.020719802007079124\n",
      "Epoch 58, Batch 60, Loss: 0.031194761395454407\n",
      "Epoch 58, Batch 70, Loss: 0.02285315841436386\n",
      "Epoch 58, Batch 80, Loss: 0.02195727825164795\n",
      "Epoch 58, Batch 90, Loss: 0.023993413895368576\n",
      "Epoch 58, Batch 100, Loss: 0.02300339937210083\n",
      "Epoch 59, Batch 0, Loss: 0.021871933713555336\n",
      "Epoch 59, Batch 10, Loss: 0.02537073940038681\n",
      "Epoch 59, Batch 20, Loss: 0.02500053681433201\n",
      "Epoch 59, Batch 30, Loss: 0.01926441118121147\n",
      "Epoch 59, Batch 40, Loss: 0.02840641513466835\n",
      "Epoch 59, Batch 50, Loss: 0.017496583983302116\n",
      "Epoch 59, Batch 60, Loss: 0.02535165287554264\n",
      "Epoch 59, Batch 70, Loss: 0.025913303717970848\n",
      "Epoch 59, Batch 80, Loss: 0.025248367339372635\n",
      "Epoch 59, Batch 90, Loss: 0.02985880710184574\n",
      "Epoch 59, Batch 100, Loss: 0.024684438481926918\n",
      "Epoch 60, Batch 0, Loss: 0.019397154450416565\n",
      "Epoch 60, Batch 10, Loss: 0.024456432089209557\n",
      "Epoch 60, Batch 20, Loss: 0.02288176119327545\n",
      "Epoch 60, Batch 30, Loss: 0.019978031516075134\n",
      "Epoch 60, Batch 40, Loss: 0.03305647522211075\n",
      "Epoch 60, Batch 50, Loss: 0.02814498543739319\n",
      "Epoch 60, Batch 60, Loss: 0.026711871847510338\n",
      "Epoch 60, Batch 70, Loss: 0.029760578647255898\n",
      "Epoch 60, Batch 80, Loss: 0.02372991293668747\n",
      "Epoch 60, Batch 90, Loss: 0.02269602008163929\n",
      "Epoch 60, Batch 100, Loss: 0.020289935171604156\n",
      "Epoch 61, Batch 0, Loss: 0.027827063575387\n",
      "Epoch 61, Batch 10, Loss: 0.023870347067713737\n",
      "Epoch 61, Batch 20, Loss: 0.03184971958398819\n",
      "Epoch 61, Batch 30, Loss: 0.021272338926792145\n",
      "Epoch 61, Batch 40, Loss: 0.020969904959201813\n",
      "Epoch 61, Batch 50, Loss: 0.01668691635131836\n",
      "Epoch 61, Batch 60, Loss: 0.026107095181941986\n",
      "Epoch 61, Batch 70, Loss: 0.029600245878100395\n",
      "Epoch 61, Batch 80, Loss: 0.01763886585831642\n",
      "Epoch 61, Batch 90, Loss: 0.025595860555768013\n",
      "Epoch 61, Batch 100, Loss: 0.016514195129275322\n",
      "Epoch 62, Batch 0, Loss: 0.022632595151662827\n",
      "Epoch 62, Batch 10, Loss: 0.023543035611510277\n",
      "Epoch 62, Batch 20, Loss: 0.027079351246356964\n",
      "Epoch 62, Batch 30, Loss: 0.026444895192980766\n",
      "Epoch 62, Batch 40, Loss: 0.022697286680340767\n",
      "Epoch 62, Batch 50, Loss: 0.029513919726014137\n",
      "Epoch 62, Batch 60, Loss: 0.020501667633652687\n",
      "Epoch 62, Batch 70, Loss: 0.020441574975848198\n",
      "Epoch 62, Batch 80, Loss: 0.024695638567209244\n",
      "Epoch 62, Batch 90, Loss: 0.03013736568391323\n",
      "Epoch 62, Batch 100, Loss: 0.02102169394493103\n",
      "Epoch 63, Batch 0, Loss: 0.028604518622159958\n",
      "Epoch 63, Batch 10, Loss: 0.024806350469589233\n",
      "Epoch 63, Batch 20, Loss: 0.01767316833138466\n",
      "Epoch 63, Batch 30, Loss: 0.03753367438912392\n",
      "Epoch 63, Batch 40, Loss: 0.02156192809343338\n",
      "Epoch 63, Batch 50, Loss: 0.022008834406733513\n",
      "Epoch 63, Batch 60, Loss: 0.031243111938238144\n",
      "Epoch 63, Batch 70, Loss: 0.021744491532444954\n",
      "Epoch 63, Batch 80, Loss: 0.027867285534739494\n",
      "Epoch 63, Batch 90, Loss: 0.026552805677056313\n",
      "Epoch 63, Batch 100, Loss: 0.02317238040268421\n",
      "Epoch 64, Batch 0, Loss: 0.018942052498459816\n",
      "Epoch 64, Batch 10, Loss: 0.02440091036260128\n",
      "Epoch 64, Batch 20, Loss: 0.03443030267953873\n",
      "Epoch 64, Batch 30, Loss: 0.025973264127969742\n",
      "Epoch 64, Batch 40, Loss: 0.026356741786003113\n",
      "Epoch 64, Batch 50, Loss: 0.025782644748687744\n",
      "Epoch 64, Batch 60, Loss: 0.0238440353423357\n",
      "Epoch 64, Batch 70, Loss: 0.025549622252583504\n",
      "Epoch 64, Batch 80, Loss: 0.027648787945508957\n",
      "Epoch 64, Batch 90, Loss: 0.02195046842098236\n",
      "Epoch 64, Batch 100, Loss: 0.023942889645695686\n",
      "Epoch 65, Batch 0, Loss: 0.018762152642011642\n",
      "Epoch 65, Batch 10, Loss: 0.018543867394328117\n",
      "Epoch 65, Batch 20, Loss: 0.030644310638308525\n",
      "Epoch 65, Batch 30, Loss: 0.02625553123652935\n",
      "Epoch 65, Batch 40, Loss: 0.023086193948984146\n",
      "Epoch 65, Batch 50, Loss: 0.022368205711245537\n",
      "Epoch 65, Batch 60, Loss: 0.023867907002568245\n",
      "Epoch 65, Batch 70, Loss: 0.02543947845697403\n",
      "Epoch 65, Batch 80, Loss: 0.015224652364850044\n",
      "Epoch 65, Batch 90, Loss: 0.022395754233002663\n",
      "Epoch 65, Batch 100, Loss: 0.034969985485076904\n",
      "Epoch 66, Batch 0, Loss: 0.02203008159995079\n",
      "Epoch 66, Batch 10, Loss: 0.017973173409700394\n",
      "Epoch 66, Batch 20, Loss: 0.023255368694663048\n",
      "Epoch 66, Batch 30, Loss: 0.023423684760928154\n",
      "Epoch 66, Batch 40, Loss: 0.024913882836699486\n",
      "Epoch 66, Batch 50, Loss: 0.02386106550693512\n",
      "Epoch 66, Batch 60, Loss: 0.01966916210949421\n",
      "Epoch 66, Batch 70, Loss: 0.02279999665915966\n",
      "Epoch 66, Batch 80, Loss: 0.02833663672208786\n",
      "Epoch 66, Batch 90, Loss: 0.01883941888809204\n",
      "Epoch 66, Batch 100, Loss: 0.022801363840699196\n",
      "Epoch 67, Batch 0, Loss: 0.02362823113799095\n",
      "Epoch 67, Batch 10, Loss: 0.025353427976369858\n",
      "Epoch 67, Batch 20, Loss: 0.024460533633828163\n",
      "Epoch 67, Batch 30, Loss: 0.0170796737074852\n",
      "Epoch 67, Batch 40, Loss: 0.02420421689748764\n",
      "Epoch 67, Batch 50, Loss: 0.027265410870313644\n",
      "Epoch 67, Batch 60, Loss: 0.01943627931177616\n",
      "Epoch 67, Batch 70, Loss: 0.026862885802984238\n",
      "Epoch 67, Batch 80, Loss: 0.025289051234722137\n",
      "Epoch 67, Batch 90, Loss: 0.02456013113260269\n",
      "Epoch 67, Batch 100, Loss: 0.02520330250263214\n",
      "Epoch 68, Batch 0, Loss: 0.018362851813435555\n",
      "Epoch 68, Batch 10, Loss: 0.021734539419412613\n",
      "Epoch 68, Batch 20, Loss: 0.016917455941438675\n",
      "Epoch 68, Batch 30, Loss: 0.022058092057704926\n",
      "Epoch 68, Batch 40, Loss: 0.02452845685184002\n",
      "Epoch 68, Batch 50, Loss: 0.029057838022708893\n",
      "Epoch 68, Batch 60, Loss: 0.02396567538380623\n",
      "Epoch 68, Batch 70, Loss: 0.02866269089281559\n",
      "Epoch 68, Batch 80, Loss: 0.026433944702148438\n",
      "Epoch 68, Batch 90, Loss: 0.024436432868242264\n",
      "Epoch 68, Batch 100, Loss: 0.026066582649946213\n",
      "Epoch 69, Batch 0, Loss: 0.02215486206114292\n",
      "Epoch 69, Batch 10, Loss: 0.025462111458182335\n",
      "Epoch 69, Batch 20, Loss: 0.03244240581989288\n",
      "Epoch 69, Batch 30, Loss: 0.0284925177693367\n",
      "Epoch 69, Batch 40, Loss: 0.026749391108751297\n",
      "Epoch 69, Batch 50, Loss: 0.0235567856580019\n",
      "Epoch 69, Batch 60, Loss: 0.026919007301330566\n",
      "Epoch 69, Batch 70, Loss: 0.0245094895362854\n",
      "Epoch 69, Batch 80, Loss: 0.019935237243771553\n",
      "Epoch 69, Batch 90, Loss: 0.021051907911896706\n",
      "Epoch 69, Batch 100, Loss: 0.015576539561152458\n",
      "Epoch 70, Batch 0, Loss: 0.026199689134955406\n",
      "Epoch 70, Batch 10, Loss: 0.022710507735610008\n",
      "Epoch 70, Batch 20, Loss: 0.028924597427248955\n",
      "Epoch 70, Batch 30, Loss: 0.02431054785847664\n",
      "Epoch 70, Batch 40, Loss: 0.023767957463860512\n",
      "Epoch 70, Batch 50, Loss: 0.02121073380112648\n",
      "Epoch 70, Batch 60, Loss: 0.027058711275458336\n",
      "Epoch 70, Batch 70, Loss: 0.031098417937755585\n",
      "Epoch 70, Batch 80, Loss: 0.027179546654224396\n",
      "Epoch 70, Batch 90, Loss: 0.02534271962940693\n",
      "Epoch 70, Batch 100, Loss: 0.029920628294348717\n",
      "Epoch 71, Batch 0, Loss: 0.02178623527288437\n",
      "Epoch 71, Batch 10, Loss: 0.02283899486064911\n",
      "Epoch 71, Batch 20, Loss: 0.02620263397693634\n",
      "Epoch 71, Batch 30, Loss: 0.02277061715722084\n",
      "Epoch 71, Batch 40, Loss: 0.027792274951934814\n",
      "Epoch 71, Batch 50, Loss: 0.033160559833049774\n",
      "Epoch 71, Batch 60, Loss: 0.02549433708190918\n",
      "Epoch 71, Batch 70, Loss: 0.029334604740142822\n",
      "Epoch 71, Batch 80, Loss: 0.01746845617890358\n",
      "Epoch 71, Batch 90, Loss: 0.0194639191031456\n",
      "Epoch 71, Batch 100, Loss: 0.03572622686624527\n",
      "Epoch 72, Batch 0, Loss: 0.023277033120393753\n",
      "Epoch 72, Batch 10, Loss: 0.023291103541851044\n",
      "Epoch 72, Batch 20, Loss: 0.019493835046887398\n",
      "Epoch 72, Batch 30, Loss: 0.025710156187415123\n",
      "Epoch 72, Batch 40, Loss: 0.026154864579439163\n",
      "Epoch 72, Batch 50, Loss: 0.03245677798986435\n",
      "Epoch 72, Batch 60, Loss: 0.01508417073637247\n",
      "Epoch 72, Batch 70, Loss: 0.02289421856403351\n",
      "Epoch 72, Batch 80, Loss: 0.021609406918287277\n",
      "Epoch 72, Batch 90, Loss: 0.02608650177717209\n",
      "Epoch 72, Batch 100, Loss: 0.02263403870165348\n",
      "Epoch 73, Batch 0, Loss: 0.022689446806907654\n",
      "Epoch 73, Batch 10, Loss: 0.02643674612045288\n",
      "Epoch 73, Batch 20, Loss: 0.01836211420595646\n",
      "Epoch 73, Batch 30, Loss: 0.03131239861249924\n",
      "Epoch 73, Batch 40, Loss: 0.026111703366041183\n",
      "Epoch 73, Batch 50, Loss: 0.02537974901497364\n",
      "Epoch 73, Batch 60, Loss: 0.023809924721717834\n",
      "Epoch 73, Batch 70, Loss: 0.023760046809911728\n",
      "Epoch 73, Batch 80, Loss: 0.03003857284784317\n",
      "Epoch 73, Batch 90, Loss: 0.020853493362665176\n",
      "Epoch 73, Batch 100, Loss: 0.02092813514173031\n",
      "Epoch 74, Batch 0, Loss: 0.02574869990348816\n",
      "Epoch 74, Batch 10, Loss: 0.021086331456899643\n",
      "Epoch 74, Batch 20, Loss: 0.028044067323207855\n",
      "Epoch 74, Batch 30, Loss: 0.021212298423051834\n",
      "Epoch 74, Batch 40, Loss: 0.027829550206661224\n",
      "Epoch 74, Batch 50, Loss: 0.02373676747083664\n",
      "Epoch 74, Batch 60, Loss: 0.02574332058429718\n",
      "Epoch 74, Batch 70, Loss: 0.020295975729823112\n",
      "Epoch 74, Batch 80, Loss: 0.0203699991106987\n",
      "Epoch 74, Batch 90, Loss: 0.02724110335111618\n",
      "Epoch 74, Batch 100, Loss: 0.03416379913687706\n",
      "Epoch 75, Batch 0, Loss: 0.027076099067926407\n",
      "Epoch 75, Batch 10, Loss: 0.020508011803030968\n",
      "Epoch 75, Batch 20, Loss: 0.02123577892780304\n",
      "Epoch 75, Batch 30, Loss: 0.02217242307960987\n",
      "Epoch 75, Batch 40, Loss: 0.031070269644260406\n",
      "Epoch 75, Batch 50, Loss: 0.02043035253882408\n",
      "Epoch 75, Batch 60, Loss: 0.0269378162920475\n",
      "Epoch 75, Batch 70, Loss: 0.023840054869651794\n",
      "Epoch 75, Batch 80, Loss: 0.028276395052671432\n",
      "Epoch 75, Batch 90, Loss: 0.02156214602291584\n",
      "Epoch 75, Batch 100, Loss: 0.019977714866399765\n",
      "Epoch 76, Batch 0, Loss: 0.028990143910050392\n",
      "Epoch 76, Batch 10, Loss: 0.021648872643709183\n",
      "Epoch 76, Batch 20, Loss: 0.028449445962905884\n",
      "Epoch 76, Batch 30, Loss: 0.02197602391242981\n",
      "Epoch 76, Batch 40, Loss: 0.020775573328137398\n",
      "Epoch 76, Batch 50, Loss: 0.032764267176389694\n",
      "Epoch 76, Batch 60, Loss: 0.025378771126270294\n",
      "Epoch 76, Batch 70, Loss: 0.024957552552223206\n",
      "Epoch 76, Batch 80, Loss: 0.021388927474617958\n",
      "Epoch 76, Batch 90, Loss: 0.018819214776158333\n",
      "Epoch 76, Batch 100, Loss: 0.019629454240202904\n",
      "Epoch 77, Batch 0, Loss: 0.025947269052267075\n",
      "Epoch 77, Batch 10, Loss: 0.024047087877988815\n",
      "Epoch 77, Batch 20, Loss: 0.022006701678037643\n",
      "Epoch 77, Batch 30, Loss: 0.022807041183114052\n",
      "Epoch 77, Batch 40, Loss: 0.025329090654850006\n",
      "Epoch 77, Batch 50, Loss: 0.017347540706396103\n",
      "Epoch 77, Batch 60, Loss: 0.02610468678176403\n",
      "Epoch 77, Batch 70, Loss: 0.01851404830813408\n",
      "Epoch 77, Batch 80, Loss: 0.022322451695799828\n",
      "Epoch 77, Batch 90, Loss: 0.02306048572063446\n",
      "Epoch 77, Batch 100, Loss: 0.0261562317609787\n",
      "Epoch 78, Batch 0, Loss: 0.027242379263043404\n",
      "Epoch 78, Batch 10, Loss: 0.02339058183133602\n",
      "Epoch 78, Batch 20, Loss: 0.026508230715990067\n",
      "Epoch 78, Batch 30, Loss: 0.020316559821367264\n",
      "Epoch 78, Batch 40, Loss: 0.01808057725429535\n",
      "Epoch 78, Batch 50, Loss: 0.021312685683369637\n",
      "Epoch 78, Batch 60, Loss: 0.02395426668226719\n",
      "Epoch 78, Batch 70, Loss: 0.023705175146460533\n",
      "Epoch 78, Batch 80, Loss: 0.02206847257912159\n",
      "Epoch 78, Batch 90, Loss: 0.024536702781915665\n",
      "Epoch 78, Batch 100, Loss: 0.02377808839082718\n",
      "Epoch 79, Batch 0, Loss: 0.021217409521341324\n",
      "Epoch 79, Batch 10, Loss: 0.01919453591108322\n",
      "Epoch 79, Batch 20, Loss: 0.022101635113358498\n",
      "Epoch 79, Batch 30, Loss: 0.02723805606365204\n",
      "Epoch 79, Batch 40, Loss: 0.024370426312088966\n",
      "Epoch 79, Batch 50, Loss: 0.026502981781959534\n",
      "Epoch 79, Batch 60, Loss: 0.023748524487018585\n",
      "Epoch 79, Batch 70, Loss: 0.028333820402622223\n",
      "Epoch 79, Batch 80, Loss: 0.02315855585038662\n",
      "Epoch 79, Batch 90, Loss: 0.023295259103178978\n",
      "Epoch 79, Batch 100, Loss: 0.024074578657746315\n",
      "Epoch 80, Batch 0, Loss: 0.022378481924533844\n",
      "Epoch 80, Batch 10, Loss: 0.020703643560409546\n",
      "Epoch 80, Batch 20, Loss: 0.023401731625199318\n",
      "Epoch 80, Batch 30, Loss: 0.02573511376976967\n",
      "Epoch 80, Batch 40, Loss: 0.02297995425760746\n",
      "Epoch 80, Batch 50, Loss: 0.02071237936615944\n",
      "Epoch 80, Batch 60, Loss: 0.026543349027633667\n",
      "Epoch 80, Batch 70, Loss: 0.026773232966661453\n",
      "Epoch 80, Batch 80, Loss: 0.025735536590218544\n",
      "Epoch 80, Batch 90, Loss: 0.02531423792243004\n",
      "Epoch 80, Batch 100, Loss: 0.02203342132270336\n",
      "Epoch 81, Batch 0, Loss: 0.01972660794854164\n",
      "Epoch 81, Batch 10, Loss: 0.02105354517698288\n",
      "Epoch 81, Batch 20, Loss: 0.029982533305883408\n",
      "Epoch 81, Batch 30, Loss: 0.020462393760681152\n",
      "Epoch 81, Batch 40, Loss: 0.017819847911596298\n",
      "Epoch 81, Batch 50, Loss: 0.02291765622794628\n",
      "Epoch 81, Batch 60, Loss: 0.0369986966252327\n",
      "Epoch 81, Batch 70, Loss: 0.019458500668406487\n",
      "Epoch 81, Batch 80, Loss: 0.022289274260401726\n",
      "Epoch 81, Batch 90, Loss: 0.02328825183212757\n",
      "Epoch 81, Batch 100, Loss: 0.03071550466120243\n",
      "Epoch 82, Batch 0, Loss: 0.02212701551616192\n",
      "Epoch 82, Batch 10, Loss: 0.025164984166622162\n",
      "Epoch 82, Batch 20, Loss: 0.022644707933068275\n",
      "Epoch 82, Batch 30, Loss: 0.02617444097995758\n",
      "Epoch 82, Batch 40, Loss: 0.031346485018730164\n",
      "Epoch 82, Batch 50, Loss: 0.02602124586701393\n",
      "Epoch 82, Batch 60, Loss: 0.02424837462604046\n",
      "Epoch 82, Batch 70, Loss: 0.022499144077301025\n",
      "Epoch 82, Batch 80, Loss: 0.018227728083729744\n",
      "Epoch 82, Batch 90, Loss: 0.020642537623643875\n",
      "Epoch 82, Batch 100, Loss: 0.021863453090190887\n",
      "Epoch 83, Batch 0, Loss: 0.022343575954437256\n",
      "Epoch 83, Batch 10, Loss: 0.023263759911060333\n",
      "Epoch 83, Batch 20, Loss: 0.01693126931786537\n",
      "Epoch 83, Batch 30, Loss: 0.022162863984704018\n",
      "Epoch 83, Batch 40, Loss: 0.022293642163276672\n",
      "Epoch 83, Batch 50, Loss: 0.017741424962878227\n",
      "Epoch 83, Batch 60, Loss: 0.02395997755229473\n",
      "Epoch 83, Batch 70, Loss: 0.02625526860356331\n",
      "Epoch 83, Batch 80, Loss: 0.022404927760362625\n",
      "Epoch 83, Batch 90, Loss: 0.023450449109077454\n",
      "Epoch 83, Batch 100, Loss: 0.013415856286883354\n",
      "Epoch 84, Batch 0, Loss: 0.023711297661066055\n",
      "Epoch 84, Batch 10, Loss: 0.021938396617770195\n",
      "Epoch 84, Batch 20, Loss: 0.02154650166630745\n",
      "Epoch 84, Batch 30, Loss: 0.023129481822252274\n",
      "Epoch 84, Batch 40, Loss: 0.024145418778061867\n",
      "Epoch 84, Batch 50, Loss: 0.018617726862430573\n",
      "Epoch 84, Batch 60, Loss: 0.03214697912335396\n",
      "Epoch 84, Batch 70, Loss: 0.022257236763834953\n",
      "Epoch 84, Batch 80, Loss: 0.030373256653547287\n",
      "Epoch 84, Batch 90, Loss: 0.03186582028865814\n",
      "Epoch 84, Batch 100, Loss: 0.027643386274576187\n",
      "Epoch 85, Batch 0, Loss: 0.026299908757209778\n",
      "Epoch 85, Batch 10, Loss: 0.02116304822266102\n",
      "Epoch 85, Batch 20, Loss: 0.01972438581287861\n",
      "Epoch 85, Batch 30, Loss: 0.02354903519153595\n",
      "Epoch 85, Batch 40, Loss: 0.024140439927577972\n",
      "Epoch 85, Batch 50, Loss: 0.02367039956152439\n",
      "Epoch 85, Batch 60, Loss: 0.027842676267027855\n",
      "Epoch 85, Batch 70, Loss: 0.02836948074400425\n",
      "Epoch 85, Batch 80, Loss: 0.01640506461262703\n",
      "Epoch 85, Batch 90, Loss: 0.021374821662902832\n",
      "Epoch 85, Batch 100, Loss: 0.024417314678430557\n",
      "Epoch 86, Batch 0, Loss: 0.02584884688258171\n",
      "Epoch 86, Batch 10, Loss: 0.0285358689725399\n",
      "Epoch 86, Batch 20, Loss: 0.02388942241668701\n",
      "Epoch 86, Batch 30, Loss: 0.022811029106378555\n",
      "Epoch 86, Batch 40, Loss: 0.021008208394050598\n",
      "Epoch 86, Batch 50, Loss: 0.028730615973472595\n",
      "Epoch 86, Batch 60, Loss: 0.025778483599424362\n",
      "Epoch 86, Batch 70, Loss: 0.019377756863832474\n",
      "Epoch 86, Batch 80, Loss: 0.018885653465986252\n",
      "Epoch 86, Batch 90, Loss: 0.02698732353746891\n",
      "Epoch 86, Batch 100, Loss: 0.02231154590845108\n",
      "Epoch 87, Batch 0, Loss: 0.018894467502832413\n",
      "Epoch 87, Batch 10, Loss: 0.029440071433782578\n",
      "Epoch 87, Batch 20, Loss: 0.021377334371209145\n",
      "Epoch 87, Batch 30, Loss: 0.029233599081635475\n",
      "Epoch 87, Batch 40, Loss: 0.02042248100042343\n",
      "Epoch 87, Batch 50, Loss: 0.023311173543334007\n",
      "Epoch 87, Batch 60, Loss: 0.024989396333694458\n",
      "Epoch 87, Batch 70, Loss: 0.026365477591753006\n",
      "Epoch 87, Batch 80, Loss: 0.03271238133311272\n",
      "Epoch 87, Batch 90, Loss: 0.019296297803521156\n",
      "Epoch 87, Batch 100, Loss: 0.02903088927268982\n",
      "Epoch 88, Batch 0, Loss: 0.016311736777424812\n",
      "Epoch 88, Batch 10, Loss: 0.02294117584824562\n",
      "Epoch 88, Batch 20, Loss: 0.03466982766985893\n",
      "Epoch 88, Batch 30, Loss: 0.029344622045755386\n",
      "Epoch 88, Batch 40, Loss: 0.021985553205013275\n",
      "Epoch 88, Batch 50, Loss: 0.022673409432172775\n",
      "Epoch 88, Batch 60, Loss: 0.01911178044974804\n",
      "Epoch 88, Batch 70, Loss: 0.03009004332125187\n",
      "Epoch 88, Batch 80, Loss: 0.0166951734572649\n",
      "Epoch 88, Batch 90, Loss: 0.02924436889588833\n",
      "Epoch 88, Batch 100, Loss: 0.022034205496311188\n",
      "Epoch 89, Batch 0, Loss: 0.014541578479111195\n",
      "Epoch 89, Batch 10, Loss: 0.022348839789628983\n",
      "Epoch 89, Batch 20, Loss: 0.01903906837105751\n",
      "Epoch 89, Batch 30, Loss: 0.028919478878378868\n",
      "Epoch 89, Batch 40, Loss: 0.023285580798983574\n",
      "Epoch 89, Batch 50, Loss: 0.02269657701253891\n",
      "Epoch 89, Batch 60, Loss: 0.02259971760213375\n",
      "Epoch 89, Batch 70, Loss: 0.023334916681051254\n",
      "Epoch 89, Batch 80, Loss: 0.027378937229514122\n",
      "Epoch 89, Batch 90, Loss: 0.024366391822695732\n",
      "Epoch 89, Batch 100, Loss: 0.023009110242128372\n",
      "Epoch 90, Batch 0, Loss: 0.020161492750048637\n",
      "Epoch 90, Batch 10, Loss: 0.01690797135233879\n",
      "Epoch 90, Batch 20, Loss: 0.02918172813951969\n",
      "Epoch 90, Batch 30, Loss: 0.0187810231000185\n",
      "Epoch 90, Batch 40, Loss: 0.021624723449349403\n",
      "Epoch 90, Batch 50, Loss: 0.020843293517827988\n",
      "Epoch 90, Batch 60, Loss: 0.01928068697452545\n",
      "Epoch 90, Batch 70, Loss: 0.02190270647406578\n",
      "Epoch 90, Batch 80, Loss: 0.02087654359638691\n",
      "Epoch 90, Batch 90, Loss: 0.023455558344721794\n",
      "Epoch 90, Batch 100, Loss: 0.023480979725718498\n",
      "Epoch 91, Batch 0, Loss: 0.020641885697841644\n",
      "Epoch 91, Batch 10, Loss: 0.025127679109573364\n",
      "Epoch 91, Batch 20, Loss: 0.01955374702811241\n",
      "Epoch 91, Batch 30, Loss: 0.02073855511844158\n",
      "Epoch 91, Batch 40, Loss: 0.023764193058013916\n",
      "Epoch 91, Batch 50, Loss: 0.021961819380521774\n",
      "Epoch 91, Batch 60, Loss: 0.02369743585586548\n",
      "Epoch 91, Batch 70, Loss: 0.01683836802840233\n",
      "Epoch 91, Batch 80, Loss: 0.019871659576892853\n",
      "Epoch 91, Batch 90, Loss: 0.027290906757116318\n",
      "Epoch 91, Batch 100, Loss: 0.026821447536349297\n",
      "Epoch 92, Batch 0, Loss: 0.028383390977978706\n",
      "Epoch 92, Batch 10, Loss: 0.018205808475613594\n",
      "Epoch 92, Batch 20, Loss: 0.020624466240406036\n",
      "Epoch 92, Batch 30, Loss: 0.022770091891288757\n",
      "Epoch 92, Batch 40, Loss: 0.029917016625404358\n",
      "Epoch 92, Batch 50, Loss: 0.026599837467074394\n",
      "Epoch 92, Batch 60, Loss: 0.03389832004904747\n",
      "Epoch 92, Batch 70, Loss: 0.017676040530204773\n",
      "Epoch 92, Batch 80, Loss: 0.028413057327270508\n",
      "Epoch 92, Batch 90, Loss: 0.023447226732969284\n",
      "Epoch 92, Batch 100, Loss: 0.017939679324626923\n",
      "Epoch 93, Batch 0, Loss: 0.024210430681705475\n",
      "Epoch 93, Batch 10, Loss: 0.020167270675301552\n",
      "Epoch 93, Batch 20, Loss: 0.026404060423374176\n",
      "Epoch 93, Batch 30, Loss: 0.02155478298664093\n",
      "Epoch 93, Batch 40, Loss: 0.01927914097905159\n",
      "Epoch 93, Batch 50, Loss: 0.02083153836429119\n",
      "Epoch 93, Batch 60, Loss: 0.02443264052271843\n",
      "Epoch 93, Batch 70, Loss: 0.026597006246447563\n",
      "Epoch 93, Batch 80, Loss: 0.023522241041064262\n",
      "Epoch 93, Batch 90, Loss: 0.015234210528433323\n",
      "Epoch 93, Batch 100, Loss: 0.028840921819210052\n",
      "Epoch 94, Batch 0, Loss: 0.017754720523953438\n",
      "Epoch 94, Batch 10, Loss: 0.021623428910970688\n",
      "Epoch 94, Batch 20, Loss: 0.027391556650400162\n",
      "Epoch 94, Batch 30, Loss: 0.026429878547787666\n",
      "Epoch 94, Batch 40, Loss: 0.036124493926763535\n",
      "Epoch 94, Batch 50, Loss: 0.019300464540719986\n",
      "Epoch 94, Batch 60, Loss: 0.029224131256341934\n",
      "Epoch 94, Batch 70, Loss: 0.025290818884968758\n",
      "Epoch 94, Batch 80, Loss: 0.02003728412091732\n",
      "Epoch 94, Batch 90, Loss: 0.02625236101448536\n",
      "Epoch 94, Batch 100, Loss: 0.02633950673043728\n",
      "Epoch 95, Batch 0, Loss: 0.02595379948616028\n",
      "Epoch 95, Batch 10, Loss: 0.020013846457004547\n",
      "Epoch 95, Batch 20, Loss: 0.023535024374723434\n",
      "Epoch 95, Batch 30, Loss: 0.016837144270539284\n",
      "Epoch 95, Batch 40, Loss: 0.01961706206202507\n",
      "Epoch 95, Batch 50, Loss: 0.018893152475357056\n",
      "Epoch 95, Batch 60, Loss: 0.02227811887860298\n",
      "Epoch 95, Batch 70, Loss: 0.020691126585006714\n",
      "Epoch 95, Batch 80, Loss: 0.020692873746156693\n",
      "Epoch 95, Batch 90, Loss: 0.02386069856584072\n",
      "Epoch 95, Batch 100, Loss: 0.028663791716098785\n",
      "Epoch 96, Batch 0, Loss: 0.024554627016186714\n",
      "Epoch 96, Batch 10, Loss: 0.0392003133893013\n",
      "Epoch 96, Batch 20, Loss: 0.02310640923678875\n",
      "Epoch 96, Batch 30, Loss: 0.01961805298924446\n",
      "Epoch 96, Batch 40, Loss: 0.021221816539764404\n",
      "Epoch 96, Batch 50, Loss: 0.02062060311436653\n",
      "Epoch 96, Batch 60, Loss: 0.02323332242667675\n",
      "Epoch 96, Batch 70, Loss: 0.02345198765397072\n",
      "Epoch 96, Batch 80, Loss: 0.019507963210344315\n",
      "Epoch 96, Batch 90, Loss: 0.01650388538837433\n",
      "Epoch 96, Batch 100, Loss: 0.031834449619054794\n",
      "Epoch 97, Batch 0, Loss: 0.031655289232730865\n",
      "Epoch 97, Batch 10, Loss: 0.019763439893722534\n",
      "Epoch 97, Batch 20, Loss: 0.01755370944738388\n",
      "Epoch 97, Batch 30, Loss: 0.023406649008393288\n",
      "Epoch 97, Batch 40, Loss: 0.030583307147026062\n",
      "Epoch 97, Batch 50, Loss: 0.02561948075890541\n",
      "Epoch 97, Batch 60, Loss: 0.02037220634520054\n",
      "Epoch 97, Batch 70, Loss: 0.030181575566530228\n",
      "Epoch 97, Batch 80, Loss: 0.020851852372288704\n",
      "Epoch 97, Batch 90, Loss: 0.033482037484645844\n",
      "Epoch 97, Batch 100, Loss: 0.021699445322155952\n",
      "Epoch 98, Batch 0, Loss: 0.017485691234469414\n",
      "Epoch 98, Batch 10, Loss: 0.025182222947478294\n",
      "Epoch 98, Batch 20, Loss: 0.0187140554189682\n",
      "Epoch 98, Batch 30, Loss: 0.028893060982227325\n",
      "Epoch 98, Batch 40, Loss: 0.024670299142599106\n",
      "Epoch 98, Batch 50, Loss: 0.024838579818606377\n",
      "Epoch 98, Batch 60, Loss: 0.018627561628818512\n",
      "Epoch 98, Batch 70, Loss: 0.02750420942902565\n",
      "Epoch 98, Batch 80, Loss: 0.018669290468096733\n",
      "Epoch 98, Batch 90, Loss: 0.020084481686353683\n",
      "Epoch 98, Batch 100, Loss: 0.025689641013741493\n",
      "Epoch 99, Batch 0, Loss: 0.01979503035545349\n",
      "Epoch 99, Batch 10, Loss: 0.018999170511960983\n",
      "Epoch 99, Batch 20, Loss: 0.022633105516433716\n",
      "Epoch 99, Batch 30, Loss: 0.036056116223335266\n",
      "Epoch 99, Batch 40, Loss: 0.02139284834265709\n",
      "Epoch 99, Batch 50, Loss: 0.025989988818764687\n",
      "Epoch 99, Batch 60, Loss: 0.02401270531117916\n",
      "Epoch 99, Batch 70, Loss: 0.019417069852352142\n",
      "Epoch 99, Batch 80, Loss: 0.02384873852133751\n",
      "Epoch 99, Batch 90, Loss: 0.029442081227898598\n",
      "Epoch 99, Batch 100, Loss: 0.02252066880464554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_BiLSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True)\n",
       "    (fc1): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.039798; Test RMSE 40.119644\n",
      "\n",
      "Train  MAE: 0.022829; Test  MAE 28.328704\n",
      "\n",
      "Train  R^2: 0.998414; Test  R^2 0.964476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
