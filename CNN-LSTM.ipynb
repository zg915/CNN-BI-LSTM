{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_preprocessing import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6618, 10, 8]) torch.Size([6618, 1]) torch.Size([490, 10, 8]) (490, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, y_scaler, train_loader, test_loader = get_data(\n",
    "            timestep = 10,\n",
    "            batch_size = 64,\n",
    "            y_name = 'Closing price',\n",
    "            train_size = 6627,\n",
    "            path = \"Data/SSE000001.csv\")\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    print (\"GPU device not found.\")\n",
    "\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer parameters from the table\n",
    "        self.conv1d = nn.Conv1d(in_channels= 8, out_channels= 32, kernel_size= 1, padding=\"same\")\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Pooling\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size= 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layer\n",
    "        x = self.conv1d(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64, lstm_dropout = 0.2, fc1_output_size = 16):\n",
    "        super(BILSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True,\n",
    "                             bidirectional = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size * 2, self.fc1_output_size)\n",
    "        self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        h_fc2 = self.fc2(h_fc1)\n",
    "        output = h_fc2[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64,  fc1_output_size = 16):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        # self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p = self.lstm_dropout)\n",
    "        # self.fc1 = nn.Linear(self.lstm_hidden_size, self.fc1_output_size)\n",
    "        # self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "        # h_dropout = self.dropout(h_lstm)\n",
    "\n",
    "        # h_fc1 = self.fc1(h_lstm)\n",
    "        # h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        # h_fc2 = self.fc2(h_fc1)\n",
    "        # output = h_fc2[:, -1, :]\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        output = h_fc1[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.lstm = LSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x)\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Reshape x to fit BiLSTM input requirements\n",
    "        x = x.contiguous().view(batch_size, seq_len, -1)\n",
    "        x = self.lstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_LSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.8225922584533691\n",
      "Epoch 0, Batch 10, Loss: 0.9547179341316223\n",
      "Epoch 0, Batch 20, Loss: 0.6931480169296265\n",
      "Epoch 0, Batch 30, Loss: 0.9737200736999512\n",
      "Epoch 0, Batch 40, Loss: 0.6662105321884155\n",
      "Epoch 0, Batch 50, Loss: 0.7251437306404114\n",
      "Epoch 0, Batch 60, Loss: 0.7388264536857605\n",
      "Epoch 0, Batch 70, Loss: 0.6835612058639526\n",
      "Epoch 0, Batch 80, Loss: 0.7202428579330444\n",
      "Epoch 0, Batch 90, Loss: 0.5747403502464294\n",
      "Epoch 0, Batch 100, Loss: 0.6590320467948914\n",
      "Epoch 1, Batch 0, Loss: 0.5939779281616211\n",
      "Epoch 1, Batch 10, Loss: 0.48948001861572266\n",
      "Epoch 1, Batch 20, Loss: 0.42448103427886963\n",
      "Epoch 1, Batch 30, Loss: 0.34403321146965027\n",
      "Epoch 1, Batch 40, Loss: 0.2578132450580597\n",
      "Epoch 1, Batch 50, Loss: 0.11058269441127777\n",
      "Epoch 1, Batch 60, Loss: 0.15083293616771698\n",
      "Epoch 1, Batch 70, Loss: 0.13186773657798767\n",
      "Epoch 1, Batch 80, Loss: 0.14412513375282288\n",
      "Epoch 1, Batch 90, Loss: 0.13775332272052765\n",
      "Epoch 1, Batch 100, Loss: 0.11249219626188278\n",
      "Epoch 2, Batch 0, Loss: 0.10952335596084595\n",
      "Epoch 2, Batch 10, Loss: 0.1478879749774933\n",
      "Epoch 2, Batch 20, Loss: 0.06722642481327057\n",
      "Epoch 2, Batch 30, Loss: 0.04328955337405205\n",
      "Epoch 2, Batch 40, Loss: 0.07492069900035858\n",
      "Epoch 2, Batch 50, Loss: 0.11963574588298798\n",
      "Epoch 2, Batch 60, Loss: 0.07177851349115372\n",
      "Epoch 2, Batch 70, Loss: 0.10185229033231735\n",
      "Epoch 2, Batch 80, Loss: 0.09935716539621353\n",
      "Epoch 2, Batch 90, Loss: 0.06680010259151459\n",
      "Epoch 2, Batch 100, Loss: 0.054237231612205505\n",
      "Epoch 3, Batch 0, Loss: 0.04601265862584114\n",
      "Epoch 3, Batch 10, Loss: 0.05474831908941269\n",
      "Epoch 3, Batch 20, Loss: 0.08735755831003189\n",
      "Epoch 3, Batch 30, Loss: 0.08550120145082474\n",
      "Epoch 3, Batch 40, Loss: 0.07086469233036041\n",
      "Epoch 3, Batch 50, Loss: 0.08355559408664703\n",
      "Epoch 3, Batch 60, Loss: 0.045226991176605225\n",
      "Epoch 3, Batch 70, Loss: 0.049859873950481415\n",
      "Epoch 3, Batch 80, Loss: 0.06578368693590164\n",
      "Epoch 3, Batch 90, Loss: 0.04288126900792122\n",
      "Epoch 3, Batch 100, Loss: 0.043995678424835205\n",
      "Epoch 4, Batch 0, Loss: 0.04261156544089317\n",
      "Epoch 4, Batch 10, Loss: 0.06457693874835968\n",
      "Epoch 4, Batch 20, Loss: 0.05782122164964676\n",
      "Epoch 4, Batch 30, Loss: 0.059844788163900375\n",
      "Epoch 4, Batch 40, Loss: 0.044141240417957306\n",
      "Epoch 4, Batch 50, Loss: 0.05947175621986389\n",
      "Epoch 4, Batch 60, Loss: 0.04312360659241676\n",
      "Epoch 4, Batch 70, Loss: 0.03831909969449043\n",
      "Epoch 4, Batch 80, Loss: 0.02710345946252346\n",
      "Epoch 4, Batch 90, Loss: 0.05356688052415848\n",
      "Epoch 4, Batch 100, Loss: 0.05014733970165253\n",
      "Epoch 5, Batch 0, Loss: 0.058435339480638504\n",
      "Epoch 5, Batch 10, Loss: 0.05354994535446167\n",
      "Epoch 5, Batch 20, Loss: 0.04154966399073601\n",
      "Epoch 5, Batch 30, Loss: 0.03495441749691963\n",
      "Epoch 5, Batch 40, Loss: 0.028660491108894348\n",
      "Epoch 5, Batch 50, Loss: 0.032501328736543655\n",
      "Epoch 5, Batch 60, Loss: 0.041633013635873795\n",
      "Epoch 5, Batch 70, Loss: 0.03785306215286255\n",
      "Epoch 5, Batch 80, Loss: 0.04216825217008591\n",
      "Epoch 5, Batch 90, Loss: 0.04620756581425667\n",
      "Epoch 5, Batch 100, Loss: 0.055676285177469254\n",
      "Epoch 6, Batch 0, Loss: 0.027906667441129684\n",
      "Epoch 6, Batch 10, Loss: 0.03901488333940506\n",
      "Epoch 6, Batch 20, Loss: 0.033884044736623764\n",
      "Epoch 6, Batch 30, Loss: 0.03530452400445938\n",
      "Epoch 6, Batch 40, Loss: 0.024260368198156357\n",
      "Epoch 6, Batch 50, Loss: 0.05415652319788933\n",
      "Epoch 6, Batch 60, Loss: 0.04311773180961609\n",
      "Epoch 6, Batch 70, Loss: 0.05493977665901184\n",
      "Epoch 6, Batch 80, Loss: 0.06645946204662323\n",
      "Epoch 6, Batch 90, Loss: 0.052167944610118866\n",
      "Epoch 6, Batch 100, Loss: 0.04615918546915054\n",
      "Epoch 7, Batch 0, Loss: 0.04469708353281021\n",
      "Epoch 7, Batch 10, Loss: 0.04368976503610611\n",
      "Epoch 7, Batch 20, Loss: 0.04244744032621384\n",
      "Epoch 7, Batch 30, Loss: 0.03741426393389702\n",
      "Epoch 7, Batch 40, Loss: 0.051626577973365784\n",
      "Epoch 7, Batch 50, Loss: 0.04279769957065582\n",
      "Epoch 7, Batch 60, Loss: 0.039737775921821594\n",
      "Epoch 7, Batch 70, Loss: 0.023589493706822395\n",
      "Epoch 7, Batch 80, Loss: 0.038679443299770355\n",
      "Epoch 7, Batch 90, Loss: 0.031511418521404266\n",
      "Epoch 7, Batch 100, Loss: 0.02745324932038784\n",
      "Epoch 8, Batch 0, Loss: 0.047364432364702225\n",
      "Epoch 8, Batch 10, Loss: 0.02744428440928459\n",
      "Epoch 8, Batch 20, Loss: 0.04627397656440735\n",
      "Epoch 8, Batch 30, Loss: 0.04999220371246338\n",
      "Epoch 8, Batch 40, Loss: 0.031085478141903877\n",
      "Epoch 8, Batch 50, Loss: 0.03706185519695282\n",
      "Epoch 8, Batch 60, Loss: 0.041672900319099426\n",
      "Epoch 8, Batch 70, Loss: 0.04175310209393501\n",
      "Epoch 8, Batch 80, Loss: 0.027585849165916443\n",
      "Epoch 8, Batch 90, Loss: 0.03187958896160126\n",
      "Epoch 8, Batch 100, Loss: 0.03459247574210167\n",
      "Epoch 9, Batch 0, Loss: 0.028481557965278625\n",
      "Epoch 9, Batch 10, Loss: 0.049602631479501724\n",
      "Epoch 9, Batch 20, Loss: 0.03721471130847931\n",
      "Epoch 9, Batch 30, Loss: 0.040068045258522034\n",
      "Epoch 9, Batch 40, Loss: 0.03976801782846451\n",
      "Epoch 9, Batch 50, Loss: 0.04770561680197716\n",
      "Epoch 9, Batch 60, Loss: 0.03550926223397255\n",
      "Epoch 9, Batch 70, Loss: 0.028445297852158546\n",
      "Epoch 9, Batch 80, Loss: 0.02905413694679737\n",
      "Epoch 9, Batch 90, Loss: 0.030668580904603004\n",
      "Epoch 9, Batch 100, Loss: 0.02729441411793232\n",
      "Epoch 10, Batch 0, Loss: 0.034785352647304535\n",
      "Epoch 10, Batch 10, Loss: 0.02883417159318924\n",
      "Epoch 10, Batch 20, Loss: 0.02393479086458683\n",
      "Epoch 10, Batch 30, Loss: 0.02256806381046772\n",
      "Epoch 10, Batch 40, Loss: 0.04054499417543411\n",
      "Epoch 10, Batch 50, Loss: 0.03344683721661568\n",
      "Epoch 10, Batch 60, Loss: 0.022938739508390427\n",
      "Epoch 10, Batch 70, Loss: 0.029205579310655594\n",
      "Epoch 10, Batch 80, Loss: 0.02342841401696205\n",
      "Epoch 10, Batch 90, Loss: 0.0282828938215971\n",
      "Epoch 10, Batch 100, Loss: 0.03355951979756355\n",
      "Epoch 11, Batch 0, Loss: 0.03416677564382553\n",
      "Epoch 11, Batch 10, Loss: 0.03638593852519989\n",
      "Epoch 11, Batch 20, Loss: 0.03858858346939087\n",
      "Epoch 11, Batch 30, Loss: 0.02967306599020958\n",
      "Epoch 11, Batch 40, Loss: 0.027346663177013397\n",
      "Epoch 11, Batch 50, Loss: 0.027183666825294495\n",
      "Epoch 11, Batch 60, Loss: 0.03375358134508133\n",
      "Epoch 11, Batch 70, Loss: 0.0341796875\n",
      "Epoch 11, Batch 80, Loss: 0.03611596301198006\n",
      "Epoch 11, Batch 90, Loss: 0.03738779574632645\n",
      "Epoch 11, Batch 100, Loss: 0.022330479696393013\n",
      "Epoch 12, Batch 0, Loss: 0.02852964960038662\n",
      "Epoch 12, Batch 10, Loss: 0.03789617493748665\n",
      "Epoch 12, Batch 20, Loss: 0.025437070056796074\n",
      "Epoch 12, Batch 30, Loss: 0.021132033318281174\n",
      "Epoch 12, Batch 40, Loss: 0.028892964124679565\n",
      "Epoch 12, Batch 50, Loss: 0.028763826936483383\n",
      "Epoch 12, Batch 60, Loss: 0.03412472829222679\n",
      "Epoch 12, Batch 70, Loss: 0.030344489961862564\n",
      "Epoch 12, Batch 80, Loss: 0.03470229730010033\n",
      "Epoch 12, Batch 90, Loss: 0.025573205202817917\n",
      "Epoch 12, Batch 100, Loss: 0.03285865858197212\n",
      "Epoch 13, Batch 0, Loss: 0.01960742473602295\n",
      "Epoch 13, Batch 10, Loss: 0.02507753297686577\n",
      "Epoch 13, Batch 20, Loss: 0.03630983829498291\n",
      "Epoch 13, Batch 30, Loss: 0.027463294565677643\n",
      "Epoch 13, Batch 40, Loss: 0.02960129827260971\n",
      "Epoch 13, Batch 50, Loss: 0.0336269773542881\n",
      "Epoch 13, Batch 60, Loss: 0.02433597669005394\n",
      "Epoch 13, Batch 70, Loss: 0.03448627516627312\n",
      "Epoch 13, Batch 80, Loss: 0.03308849781751633\n",
      "Epoch 13, Batch 90, Loss: 0.03241341561079025\n",
      "Epoch 13, Batch 100, Loss: 0.03342540189623833\n",
      "Epoch 14, Batch 0, Loss: 0.027822265401482582\n",
      "Epoch 14, Batch 10, Loss: 0.031002404168248177\n",
      "Epoch 14, Batch 20, Loss: 0.02266499772667885\n",
      "Epoch 14, Batch 30, Loss: 0.02722962573170662\n",
      "Epoch 14, Batch 40, Loss: 0.02918437123298645\n",
      "Epoch 14, Batch 50, Loss: 0.025880958884954453\n",
      "Epoch 14, Batch 60, Loss: 0.02621375024318695\n",
      "Epoch 14, Batch 70, Loss: 0.022581404075026512\n",
      "Epoch 14, Batch 80, Loss: 0.02683456428349018\n",
      "Epoch 14, Batch 90, Loss: 0.036916326731443405\n",
      "Epoch 14, Batch 100, Loss: 0.03496133163571358\n",
      "Epoch 15, Batch 0, Loss: 0.025037411600351334\n",
      "Epoch 15, Batch 10, Loss: 0.03569255396723747\n",
      "Epoch 15, Batch 20, Loss: 0.020258769392967224\n",
      "Epoch 15, Batch 30, Loss: 0.022537557408213615\n",
      "Epoch 15, Batch 40, Loss: 0.03146699443459511\n",
      "Epoch 15, Batch 50, Loss: 0.025801517069339752\n",
      "Epoch 15, Batch 60, Loss: 0.027482163161039352\n",
      "Epoch 15, Batch 70, Loss: 0.030644966289401054\n",
      "Epoch 15, Batch 80, Loss: 0.02623862586915493\n",
      "Epoch 15, Batch 90, Loss: 0.025367984548211098\n",
      "Epoch 15, Batch 100, Loss: 0.028058335185050964\n",
      "Epoch 16, Batch 0, Loss: 0.029046986252069473\n",
      "Epoch 16, Batch 10, Loss: 0.026802243664860725\n",
      "Epoch 16, Batch 20, Loss: 0.035430874675512314\n",
      "Epoch 16, Batch 30, Loss: 0.01936349831521511\n",
      "Epoch 16, Batch 40, Loss: 0.025863012298941612\n",
      "Epoch 16, Batch 50, Loss: 0.020711444318294525\n",
      "Epoch 16, Batch 60, Loss: 0.02794661559164524\n",
      "Epoch 16, Batch 70, Loss: 0.034697141498327255\n",
      "Epoch 16, Batch 80, Loss: 0.035654857754707336\n",
      "Epoch 16, Batch 90, Loss: 0.036402054131031036\n",
      "Epoch 16, Batch 100, Loss: 0.02870996668934822\n",
      "Epoch 17, Batch 0, Loss: 0.03098396584391594\n",
      "Epoch 17, Batch 10, Loss: 0.029614504426717758\n",
      "Epoch 17, Batch 20, Loss: 0.023818964138627052\n",
      "Epoch 17, Batch 30, Loss: 0.025332439690828323\n",
      "Epoch 17, Batch 40, Loss: 0.034223467111587524\n",
      "Epoch 17, Batch 50, Loss: 0.027432728558778763\n",
      "Epoch 17, Batch 60, Loss: 0.02434246614575386\n",
      "Epoch 17, Batch 70, Loss: 0.024818766862154007\n",
      "Epoch 17, Batch 80, Loss: 0.020071247592568398\n",
      "Epoch 17, Batch 90, Loss: 0.024726200848817825\n",
      "Epoch 17, Batch 100, Loss: 0.023918453603982925\n",
      "Epoch 18, Batch 0, Loss: 0.0407780222594738\n",
      "Epoch 18, Batch 10, Loss: 0.02792893908917904\n",
      "Epoch 18, Batch 20, Loss: 0.022717706859111786\n",
      "Epoch 18, Batch 30, Loss: 0.022533638402819633\n",
      "Epoch 18, Batch 40, Loss: 0.028504248708486557\n",
      "Epoch 18, Batch 50, Loss: 0.026856478303670883\n",
      "Epoch 18, Batch 60, Loss: 0.024403635412454605\n",
      "Epoch 18, Batch 70, Loss: 0.03312293812632561\n",
      "Epoch 18, Batch 80, Loss: 0.02984970435500145\n",
      "Epoch 18, Batch 90, Loss: 0.026794303208589554\n",
      "Epoch 18, Batch 100, Loss: 0.026867039501667023\n",
      "Epoch 19, Batch 0, Loss: 0.017508098855614662\n",
      "Epoch 19, Batch 10, Loss: 0.04464554414153099\n",
      "Epoch 19, Batch 20, Loss: 0.01857181452214718\n",
      "Epoch 19, Batch 30, Loss: 0.025129463523626328\n",
      "Epoch 19, Batch 40, Loss: 0.037962332367897034\n",
      "Epoch 19, Batch 50, Loss: 0.02168137952685356\n",
      "Epoch 19, Batch 60, Loss: 0.02063736878335476\n",
      "Epoch 19, Batch 70, Loss: 0.024397237226366997\n",
      "Epoch 19, Batch 80, Loss: 0.04245353490114212\n",
      "Epoch 19, Batch 90, Loss: 0.02863522619009018\n",
      "Epoch 19, Batch 100, Loss: 0.023415634408593178\n",
      "Epoch 20, Batch 0, Loss: 0.01915474608540535\n",
      "Epoch 20, Batch 10, Loss: 0.027045026421546936\n",
      "Epoch 20, Batch 20, Loss: 0.032156579196453094\n",
      "Epoch 20, Batch 30, Loss: 0.021045809611678123\n",
      "Epoch 20, Batch 40, Loss: 0.028548965230584145\n",
      "Epoch 20, Batch 50, Loss: 0.02665276639163494\n",
      "Epoch 20, Batch 60, Loss: 0.03470176085829735\n",
      "Epoch 20, Batch 70, Loss: 0.025571143254637718\n",
      "Epoch 20, Batch 80, Loss: 0.023941898718476295\n",
      "Epoch 20, Batch 90, Loss: 0.03432347625494003\n",
      "Epoch 20, Batch 100, Loss: 0.029591219499707222\n",
      "Epoch 21, Batch 0, Loss: 0.023907678201794624\n",
      "Epoch 21, Batch 10, Loss: 0.033342234790325165\n",
      "Epoch 21, Batch 20, Loss: 0.03350939601659775\n",
      "Epoch 21, Batch 30, Loss: 0.02024219185113907\n",
      "Epoch 21, Batch 40, Loss: 0.02243073098361492\n",
      "Epoch 21, Batch 50, Loss: 0.027336077764630318\n",
      "Epoch 21, Batch 60, Loss: 0.017258256673812866\n",
      "Epoch 21, Batch 70, Loss: 0.020225781947374344\n",
      "Epoch 21, Batch 80, Loss: 0.027648933231830597\n",
      "Epoch 21, Batch 90, Loss: 0.022557850927114487\n",
      "Epoch 21, Batch 100, Loss: 0.027778036892414093\n",
      "Epoch 22, Batch 0, Loss: 0.020877033472061157\n",
      "Epoch 22, Batch 10, Loss: 0.02324768155813217\n",
      "Epoch 22, Batch 20, Loss: 0.0250466950237751\n",
      "Epoch 22, Batch 30, Loss: 0.031859610229730606\n",
      "Epoch 22, Batch 40, Loss: 0.035147830843925476\n",
      "Epoch 22, Batch 50, Loss: 0.0238503310829401\n",
      "Epoch 22, Batch 60, Loss: 0.020760463550686836\n",
      "Epoch 22, Batch 70, Loss: 0.023581236600875854\n",
      "Epoch 22, Batch 80, Loss: 0.02871864102780819\n",
      "Epoch 22, Batch 90, Loss: 0.028720900416374207\n",
      "Epoch 22, Batch 100, Loss: 0.026247158646583557\n",
      "Epoch 23, Batch 0, Loss: 0.033694490790367126\n",
      "Epoch 23, Batch 10, Loss: 0.020440122112631798\n",
      "Epoch 23, Batch 20, Loss: 0.020643409341573715\n",
      "Epoch 23, Batch 30, Loss: 0.01876640133559704\n",
      "Epoch 23, Batch 40, Loss: 0.026812635362148285\n",
      "Epoch 23, Batch 50, Loss: 0.022687554359436035\n",
      "Epoch 23, Batch 60, Loss: 0.02005881443619728\n",
      "Epoch 23, Batch 70, Loss: 0.029063843190670013\n",
      "Epoch 23, Batch 80, Loss: 0.020582962781190872\n",
      "Epoch 23, Batch 90, Loss: 0.025089865550398827\n",
      "Epoch 23, Batch 100, Loss: 0.02817799709737301\n",
      "Epoch 24, Batch 0, Loss: 0.025719257071614265\n",
      "Epoch 24, Batch 10, Loss: 0.025920262560248375\n",
      "Epoch 24, Batch 20, Loss: 0.021860633045434952\n",
      "Epoch 24, Batch 30, Loss: 0.033227793872356415\n",
      "Epoch 24, Batch 40, Loss: 0.025951411575078964\n",
      "Epoch 24, Batch 50, Loss: 0.03859902173280716\n",
      "Epoch 24, Batch 60, Loss: 0.02462795376777649\n",
      "Epoch 24, Batch 70, Loss: 0.023037198930978775\n",
      "Epoch 24, Batch 80, Loss: 0.025404132902622223\n",
      "Epoch 24, Batch 90, Loss: 0.021892592310905457\n",
      "Epoch 24, Batch 100, Loss: 0.031021010130643845\n",
      "Epoch 25, Batch 0, Loss: 0.016418546438217163\n",
      "Epoch 25, Batch 10, Loss: 0.023580031469464302\n",
      "Epoch 25, Batch 20, Loss: 0.025815218687057495\n",
      "Epoch 25, Batch 30, Loss: 0.028392983600497246\n",
      "Epoch 25, Batch 40, Loss: 0.023144574835896492\n",
      "Epoch 25, Batch 50, Loss: 0.028498079627752304\n",
      "Epoch 25, Batch 60, Loss: 0.028285108506679535\n",
      "Epoch 25, Batch 70, Loss: 0.023188326507806778\n",
      "Epoch 25, Batch 80, Loss: 0.023613005876541138\n",
      "Epoch 25, Batch 90, Loss: 0.03128481283783913\n",
      "Epoch 25, Batch 100, Loss: 0.017957784235477448\n",
      "Epoch 26, Batch 0, Loss: 0.02912936732172966\n",
      "Epoch 26, Batch 10, Loss: 0.030734708532691002\n",
      "Epoch 26, Batch 20, Loss: 0.022033464163541794\n",
      "Epoch 26, Batch 30, Loss: 0.02716245874762535\n",
      "Epoch 26, Batch 40, Loss: 0.02358047105371952\n",
      "Epoch 26, Batch 50, Loss: 0.024744248017668724\n",
      "Epoch 26, Batch 60, Loss: 0.01983955316245556\n",
      "Epoch 26, Batch 70, Loss: 0.03159409761428833\n",
      "Epoch 26, Batch 80, Loss: 0.023982159793376923\n",
      "Epoch 26, Batch 90, Loss: 0.029672687873244286\n",
      "Epoch 26, Batch 100, Loss: 0.030928364023566246\n",
      "Epoch 27, Batch 0, Loss: 0.02923014387488365\n",
      "Epoch 27, Batch 10, Loss: 0.020811649039387703\n",
      "Epoch 27, Batch 20, Loss: 0.018732957541942596\n",
      "Epoch 27, Batch 30, Loss: 0.028880786150693893\n",
      "Epoch 27, Batch 40, Loss: 0.0364353321492672\n",
      "Epoch 27, Batch 50, Loss: 0.02760254591703415\n",
      "Epoch 27, Batch 60, Loss: 0.01958969607949257\n",
      "Epoch 27, Batch 70, Loss: 0.02488180622458458\n",
      "Epoch 27, Batch 80, Loss: 0.02139834128320217\n",
      "Epoch 27, Batch 90, Loss: 0.025571754202246666\n",
      "Epoch 27, Batch 100, Loss: 0.027493854984641075\n",
      "Epoch 28, Batch 0, Loss: 0.026924632489681244\n",
      "Epoch 28, Batch 10, Loss: 0.029946399852633476\n",
      "Epoch 28, Batch 20, Loss: 0.02238164283335209\n",
      "Epoch 28, Batch 30, Loss: 0.022143175825476646\n",
      "Epoch 28, Batch 40, Loss: 0.026232577860355377\n",
      "Epoch 28, Batch 50, Loss: 0.03491498529911041\n",
      "Epoch 28, Batch 60, Loss: 0.020763445645570755\n",
      "Epoch 28, Batch 70, Loss: 0.028221193701028824\n",
      "Epoch 28, Batch 80, Loss: 0.028551680967211723\n",
      "Epoch 28, Batch 90, Loss: 0.026495641097426414\n",
      "Epoch 28, Batch 100, Loss: 0.02226569689810276\n",
      "Epoch 29, Batch 0, Loss: 0.018626296892762184\n",
      "Epoch 29, Batch 10, Loss: 0.030215328559279442\n",
      "Epoch 29, Batch 20, Loss: 0.02405577152967453\n",
      "Epoch 29, Batch 30, Loss: 0.02519982121884823\n",
      "Epoch 29, Batch 40, Loss: 0.027605630457401276\n",
      "Epoch 29, Batch 50, Loss: 0.017771681770682335\n",
      "Epoch 29, Batch 60, Loss: 0.023783212527632713\n",
      "Epoch 29, Batch 70, Loss: 0.030242227017879486\n",
      "Epoch 29, Batch 80, Loss: 0.02926524542272091\n",
      "Epoch 29, Batch 90, Loss: 0.018676716834306717\n",
      "Epoch 29, Batch 100, Loss: 0.02323763817548752\n",
      "Epoch 30, Batch 0, Loss: 0.021737180650234222\n",
      "Epoch 30, Batch 10, Loss: 0.024507056921720505\n",
      "Epoch 30, Batch 20, Loss: 0.019423440098762512\n",
      "Epoch 30, Batch 30, Loss: 0.023884844034910202\n",
      "Epoch 30, Batch 40, Loss: 0.020429551601409912\n",
      "Epoch 30, Batch 50, Loss: 0.0282643660902977\n",
      "Epoch 30, Batch 60, Loss: 0.02490891143679619\n",
      "Epoch 30, Batch 70, Loss: 0.020413381978869438\n",
      "Epoch 30, Batch 80, Loss: 0.02343239262700081\n",
      "Epoch 30, Batch 90, Loss: 0.02622220106422901\n",
      "Epoch 30, Batch 100, Loss: 0.029290705919265747\n",
      "Epoch 31, Batch 0, Loss: 0.02501131407916546\n",
      "Epoch 31, Batch 10, Loss: 0.018434572964906693\n",
      "Epoch 31, Batch 20, Loss: 0.020751524716615677\n",
      "Epoch 31, Batch 30, Loss: 0.025449858978390694\n",
      "Epoch 31, Batch 40, Loss: 0.02940160036087036\n",
      "Epoch 31, Batch 50, Loss: 0.024055644869804382\n",
      "Epoch 31, Batch 60, Loss: 0.025456666946411133\n",
      "Epoch 31, Batch 70, Loss: 0.026413248851895332\n",
      "Epoch 31, Batch 80, Loss: 0.02306336723268032\n",
      "Epoch 31, Batch 90, Loss: 0.024929553270339966\n",
      "Epoch 31, Batch 100, Loss: 0.019502470269799232\n",
      "Epoch 32, Batch 0, Loss: 0.020728904753923416\n",
      "Epoch 32, Batch 10, Loss: 0.033775851130485535\n",
      "Epoch 32, Batch 20, Loss: 0.02214367687702179\n",
      "Epoch 32, Batch 30, Loss: 0.028516098856925964\n",
      "Epoch 32, Batch 40, Loss: 0.0256002489477396\n",
      "Epoch 32, Batch 50, Loss: 0.027547672390937805\n",
      "Epoch 32, Batch 60, Loss: 0.021417880430817604\n",
      "Epoch 32, Batch 70, Loss: 0.023960169404745102\n",
      "Epoch 32, Batch 80, Loss: 0.026030030101537704\n",
      "Epoch 32, Batch 90, Loss: 0.021593313664197922\n",
      "Epoch 32, Batch 100, Loss: 0.028667233884334564\n",
      "Epoch 33, Batch 0, Loss: 0.026634177193045616\n",
      "Epoch 33, Batch 10, Loss: 0.025506075471639633\n",
      "Epoch 33, Batch 20, Loss: 0.026207210496068\n",
      "Epoch 33, Batch 30, Loss: 0.019280044361948967\n",
      "Epoch 33, Batch 40, Loss: 0.020767133682966232\n",
      "Epoch 33, Batch 50, Loss: 0.027335522696375847\n",
      "Epoch 33, Batch 60, Loss: 0.018856797367334366\n",
      "Epoch 33, Batch 70, Loss: 0.024184463545680046\n",
      "Epoch 33, Batch 80, Loss: 0.026071159169077873\n",
      "Epoch 33, Batch 90, Loss: 0.026892337948083878\n",
      "Epoch 33, Batch 100, Loss: 0.024389367550611496\n",
      "Epoch 34, Batch 0, Loss: 0.01907462440431118\n",
      "Epoch 34, Batch 10, Loss: 0.021077731624245644\n",
      "Epoch 34, Batch 20, Loss: 0.03373975679278374\n",
      "Epoch 34, Batch 30, Loss: 0.02700454369187355\n",
      "Epoch 34, Batch 40, Loss: 0.018487192690372467\n",
      "Epoch 34, Batch 50, Loss: 0.02511611580848694\n",
      "Epoch 34, Batch 60, Loss: 0.021771475672721863\n",
      "Epoch 34, Batch 70, Loss: 0.024800511077046394\n",
      "Epoch 34, Batch 80, Loss: 0.019839763641357422\n",
      "Epoch 34, Batch 90, Loss: 0.027019893750548363\n",
      "Epoch 34, Batch 100, Loss: 0.03131873160600662\n",
      "Epoch 35, Batch 0, Loss: 0.02322942577302456\n",
      "Epoch 35, Batch 10, Loss: 0.0198184996843338\n",
      "Epoch 35, Batch 20, Loss: 0.0238823052495718\n",
      "Epoch 35, Batch 30, Loss: 0.023054739460349083\n",
      "Epoch 35, Batch 40, Loss: 0.023183438926935196\n",
      "Epoch 35, Batch 50, Loss: 0.026531144976615906\n",
      "Epoch 35, Batch 60, Loss: 0.022749772295355797\n",
      "Epoch 35, Batch 70, Loss: 0.030391257256269455\n",
      "Epoch 35, Batch 80, Loss: 0.02951672486960888\n",
      "Epoch 35, Batch 90, Loss: 0.030183056369423866\n",
      "Epoch 35, Batch 100, Loss: 0.02847035974264145\n",
      "Epoch 36, Batch 0, Loss: 0.0199133288115263\n",
      "Epoch 36, Batch 10, Loss: 0.019192002713680267\n",
      "Epoch 36, Batch 20, Loss: 0.02301909402012825\n",
      "Epoch 36, Batch 30, Loss: 0.021969838067889214\n",
      "Epoch 36, Batch 40, Loss: 0.028529100120067596\n",
      "Epoch 36, Batch 50, Loss: 0.01829778030514717\n",
      "Epoch 36, Batch 60, Loss: 0.025097480043768883\n",
      "Epoch 36, Batch 70, Loss: 0.020351791754364967\n",
      "Epoch 36, Batch 80, Loss: 0.02164621278643608\n",
      "Epoch 36, Batch 90, Loss: 0.021078389137983322\n",
      "Epoch 36, Batch 100, Loss: 0.027034636586904526\n",
      "Epoch 37, Batch 0, Loss: 0.023576630279421806\n",
      "Epoch 37, Batch 10, Loss: 0.03342205658555031\n",
      "Epoch 37, Batch 20, Loss: 0.0313476137816906\n",
      "Epoch 37, Batch 30, Loss: 0.024799536913633347\n",
      "Epoch 37, Batch 40, Loss: 0.01691836677491665\n",
      "Epoch 37, Batch 50, Loss: 0.027043912559747696\n",
      "Epoch 37, Batch 60, Loss: 0.018729370087385178\n",
      "Epoch 37, Batch 70, Loss: 0.02336587943136692\n",
      "Epoch 37, Batch 80, Loss: 0.024334821850061417\n",
      "Epoch 37, Batch 90, Loss: 0.01629299484193325\n",
      "Epoch 37, Batch 100, Loss: 0.023969462141394615\n",
      "Epoch 38, Batch 0, Loss: 0.020623264834284782\n",
      "Epoch 38, Batch 10, Loss: 0.03253813832998276\n",
      "Epoch 38, Batch 20, Loss: 0.02082265540957451\n",
      "Epoch 38, Batch 30, Loss: 0.016639526933431625\n",
      "Epoch 38, Batch 40, Loss: 0.025411512702703476\n",
      "Epoch 38, Batch 50, Loss: 0.022981766611337662\n",
      "Epoch 38, Batch 60, Loss: 0.027768846601247787\n",
      "Epoch 38, Batch 70, Loss: 0.028965258970856667\n",
      "Epoch 38, Batch 80, Loss: 0.019721653312444687\n",
      "Epoch 38, Batch 90, Loss: 0.023974282667040825\n",
      "Epoch 38, Batch 100, Loss: 0.02572629600763321\n",
      "Epoch 39, Batch 0, Loss: 0.022441022098064423\n",
      "Epoch 39, Batch 10, Loss: 0.021649161353707314\n",
      "Epoch 39, Batch 20, Loss: 0.019067568704485893\n",
      "Epoch 39, Batch 30, Loss: 0.02187475934624672\n",
      "Epoch 39, Batch 40, Loss: 0.023805469274520874\n",
      "Epoch 39, Batch 50, Loss: 0.02560235932469368\n",
      "Epoch 39, Batch 60, Loss: 0.022658824920654297\n",
      "Epoch 39, Batch 70, Loss: 0.02239779382944107\n",
      "Epoch 39, Batch 80, Loss: 0.021625889465212822\n",
      "Epoch 39, Batch 90, Loss: 0.02283962443470955\n",
      "Epoch 39, Batch 100, Loss: 0.021341519430279732\n",
      "Epoch 40, Batch 0, Loss: 0.02030310407280922\n",
      "Epoch 40, Batch 10, Loss: 0.029501521959900856\n",
      "Epoch 40, Batch 20, Loss: 0.026590976864099503\n",
      "Epoch 40, Batch 30, Loss: 0.04533062130212784\n",
      "Epoch 40, Batch 40, Loss: 0.02205893211066723\n",
      "Epoch 40, Batch 50, Loss: 0.029459558427333832\n",
      "Epoch 40, Batch 60, Loss: 0.030860424041748047\n",
      "Epoch 40, Batch 70, Loss: 0.023542385548353195\n",
      "Epoch 40, Batch 80, Loss: 0.01815447025001049\n",
      "Epoch 40, Batch 90, Loss: 0.02068723365664482\n",
      "Epoch 40, Batch 100, Loss: 0.019002487882971764\n",
      "Epoch 41, Batch 0, Loss: 0.02558152750134468\n",
      "Epoch 41, Batch 10, Loss: 0.025157339870929718\n",
      "Epoch 41, Batch 20, Loss: 0.017691567540168762\n",
      "Epoch 41, Batch 30, Loss: 0.03230047971010208\n",
      "Epoch 41, Batch 40, Loss: 0.030378632247447968\n",
      "Epoch 41, Batch 50, Loss: 0.021773602813482285\n",
      "Epoch 41, Batch 60, Loss: 0.028737038373947144\n",
      "Epoch 41, Batch 70, Loss: 0.021765245124697685\n",
      "Epoch 41, Batch 80, Loss: 0.02234039083123207\n",
      "Epoch 41, Batch 90, Loss: 0.02251308038830757\n",
      "Epoch 41, Batch 100, Loss: 0.023097947239875793\n",
      "Epoch 42, Batch 0, Loss: 0.023148292675614357\n",
      "Epoch 42, Batch 10, Loss: 0.03363446518778801\n",
      "Epoch 42, Batch 20, Loss: 0.02652139961719513\n",
      "Epoch 42, Batch 30, Loss: 0.020882315933704376\n",
      "Epoch 42, Batch 40, Loss: 0.021057842299342155\n",
      "Epoch 42, Batch 50, Loss: 0.026906393468379974\n",
      "Epoch 42, Batch 60, Loss: 0.01698813959956169\n",
      "Epoch 42, Batch 70, Loss: 0.02385086566209793\n",
      "Epoch 42, Batch 80, Loss: 0.023642124608159065\n",
      "Epoch 42, Batch 90, Loss: 0.02283908985555172\n",
      "Epoch 42, Batch 100, Loss: 0.027959313243627548\n",
      "Epoch 43, Batch 0, Loss: 0.02148670144379139\n",
      "Epoch 43, Batch 10, Loss: 0.028303096070885658\n",
      "Epoch 43, Batch 20, Loss: 0.028599150478839874\n",
      "Epoch 43, Batch 30, Loss: 0.02149614691734314\n",
      "Epoch 43, Batch 40, Loss: 0.026080206036567688\n",
      "Epoch 43, Batch 50, Loss: 0.03020070679485798\n",
      "Epoch 43, Batch 60, Loss: 0.02169092930853367\n",
      "Epoch 43, Batch 70, Loss: 0.018592990934848785\n",
      "Epoch 43, Batch 80, Loss: 0.024041449651122093\n",
      "Epoch 43, Batch 90, Loss: 0.024390416219830513\n",
      "Epoch 43, Batch 100, Loss: 0.031589411199092865\n",
      "Epoch 44, Batch 0, Loss: 0.020230518653988838\n",
      "Epoch 44, Batch 10, Loss: 0.023683570325374603\n",
      "Epoch 44, Batch 20, Loss: 0.024599120020866394\n",
      "Epoch 44, Batch 30, Loss: 0.035407740622758865\n",
      "Epoch 44, Batch 40, Loss: 0.03350251168012619\n",
      "Epoch 44, Batch 50, Loss: 0.02074574865400791\n",
      "Epoch 44, Batch 60, Loss: 0.027545519173145294\n",
      "Epoch 44, Batch 70, Loss: 0.020175160840153694\n",
      "Epoch 44, Batch 80, Loss: 0.019679799675941467\n",
      "Epoch 44, Batch 90, Loss: 0.02645738795399666\n",
      "Epoch 44, Batch 100, Loss: 0.028548823669552803\n",
      "Epoch 45, Batch 0, Loss: 0.029084082692861557\n",
      "Epoch 45, Batch 10, Loss: 0.026144251227378845\n",
      "Epoch 45, Batch 20, Loss: 0.02844206430017948\n",
      "Epoch 45, Batch 30, Loss: 0.027116624638438225\n",
      "Epoch 45, Batch 40, Loss: 0.021100478246808052\n",
      "Epoch 45, Batch 50, Loss: 0.022417603060603142\n",
      "Epoch 45, Batch 60, Loss: 0.027858253568410873\n",
      "Epoch 45, Batch 70, Loss: 0.022334372624754906\n",
      "Epoch 45, Batch 80, Loss: 0.023562252521514893\n",
      "Epoch 45, Batch 90, Loss: 0.025275813415646553\n",
      "Epoch 45, Batch 100, Loss: 0.022844405844807625\n",
      "Epoch 46, Batch 0, Loss: 0.026550326496362686\n",
      "Epoch 46, Batch 10, Loss: 0.02539835497736931\n",
      "Epoch 46, Batch 20, Loss: 0.01796378754079342\n",
      "Epoch 46, Batch 30, Loss: 0.019731061533093452\n",
      "Epoch 46, Batch 40, Loss: 0.025110770016908646\n",
      "Epoch 46, Batch 50, Loss: 0.019470708444714546\n",
      "Epoch 46, Batch 60, Loss: 0.029110565781593323\n",
      "Epoch 46, Batch 70, Loss: 0.030324086546897888\n",
      "Epoch 46, Batch 80, Loss: 0.028558753430843353\n",
      "Epoch 46, Batch 90, Loss: 0.028059998527169228\n",
      "Epoch 46, Batch 100, Loss: 0.017895318567752838\n",
      "Epoch 47, Batch 0, Loss: 0.023663509637117386\n",
      "Epoch 47, Batch 10, Loss: 0.030314579606056213\n",
      "Epoch 47, Batch 20, Loss: 0.03432542830705643\n",
      "Epoch 47, Batch 30, Loss: 0.026873964816331863\n",
      "Epoch 47, Batch 40, Loss: 0.022977540269494057\n",
      "Epoch 47, Batch 50, Loss: 0.026661466807127\n",
      "Epoch 47, Batch 60, Loss: 0.040018197149038315\n",
      "Epoch 47, Batch 70, Loss: 0.02040431648492813\n",
      "Epoch 47, Batch 80, Loss: 0.0239834263920784\n",
      "Epoch 47, Batch 90, Loss: 0.02668253518640995\n",
      "Epoch 47, Batch 100, Loss: 0.026889363303780556\n",
      "Epoch 48, Batch 0, Loss: 0.03362651914358139\n",
      "Epoch 48, Batch 10, Loss: 0.024526074528694153\n",
      "Epoch 48, Batch 20, Loss: 0.024116147309541702\n",
      "Epoch 48, Batch 30, Loss: 0.0233920905739069\n",
      "Epoch 48, Batch 40, Loss: 0.018905680626630783\n",
      "Epoch 48, Batch 50, Loss: 0.0307293813675642\n",
      "Epoch 48, Batch 60, Loss: 0.02338200993835926\n",
      "Epoch 48, Batch 70, Loss: 0.020555702969431877\n",
      "Epoch 48, Batch 80, Loss: 0.03279925882816315\n",
      "Epoch 48, Batch 90, Loss: 0.026056939736008644\n",
      "Epoch 48, Batch 100, Loss: 0.029187124222517014\n",
      "Epoch 49, Batch 0, Loss: 0.02675267495214939\n",
      "Epoch 49, Batch 10, Loss: 0.02586239203810692\n",
      "Epoch 49, Batch 20, Loss: 0.018927350640296936\n",
      "Epoch 49, Batch 30, Loss: 0.017149822786450386\n",
      "Epoch 49, Batch 40, Loss: 0.01839999295771122\n",
      "Epoch 49, Batch 50, Loss: 0.03442685678601265\n",
      "Epoch 49, Batch 60, Loss: 0.021627184003591537\n",
      "Epoch 49, Batch 70, Loss: 0.029885847121477127\n",
      "Epoch 49, Batch 80, Loss: 0.0243991632014513\n",
      "Epoch 49, Batch 90, Loss: 0.02338404953479767\n",
      "Epoch 49, Batch 100, Loss: 0.024421287700533867\n",
      "Epoch 50, Batch 0, Loss: 0.021187271922826767\n",
      "Epoch 50, Batch 10, Loss: 0.02675766497850418\n",
      "Epoch 50, Batch 20, Loss: 0.01860484853386879\n",
      "Epoch 50, Batch 30, Loss: 0.018645690754055977\n",
      "Epoch 50, Batch 40, Loss: 0.022233998402953148\n",
      "Epoch 50, Batch 50, Loss: 0.023105407133698463\n",
      "Epoch 50, Batch 60, Loss: 0.024842403829097748\n",
      "Epoch 50, Batch 70, Loss: 0.027087295427918434\n",
      "Epoch 50, Batch 80, Loss: 0.026726387441158295\n",
      "Epoch 50, Batch 90, Loss: 0.02137080393731594\n",
      "Epoch 50, Batch 100, Loss: 0.02678270824253559\n",
      "Epoch 51, Batch 0, Loss: 0.02763698250055313\n",
      "Epoch 51, Batch 10, Loss: 0.022386537864804268\n",
      "Epoch 51, Batch 20, Loss: 0.02522893436253071\n",
      "Epoch 51, Batch 30, Loss: 0.02342851087450981\n",
      "Epoch 51, Batch 40, Loss: 0.017277870327234268\n",
      "Epoch 51, Batch 50, Loss: 0.019710518419742584\n",
      "Epoch 51, Batch 60, Loss: 0.02690078690648079\n",
      "Epoch 51, Batch 70, Loss: 0.01826442778110504\n",
      "Epoch 51, Batch 80, Loss: 0.026570850983262062\n",
      "Epoch 51, Batch 90, Loss: 0.02637208253145218\n",
      "Epoch 51, Batch 100, Loss: 0.020150691270828247\n",
      "Epoch 52, Batch 0, Loss: 0.021196730434894562\n",
      "Epoch 52, Batch 10, Loss: 0.024750348180532455\n",
      "Epoch 52, Batch 20, Loss: 0.02810017392039299\n",
      "Epoch 52, Batch 30, Loss: 0.027000539004802704\n",
      "Epoch 52, Batch 40, Loss: 0.018246646970510483\n",
      "Epoch 52, Batch 50, Loss: 0.026291923597455025\n",
      "Epoch 52, Batch 60, Loss: 0.018440386280417442\n",
      "Epoch 52, Batch 70, Loss: 0.026112157851457596\n",
      "Epoch 52, Batch 80, Loss: 0.02228383719921112\n",
      "Epoch 52, Batch 90, Loss: 0.025283941999077797\n",
      "Epoch 52, Batch 100, Loss: 0.016994955018162727\n",
      "Epoch 53, Batch 0, Loss: 0.0233351718634367\n",
      "Epoch 53, Batch 10, Loss: 0.019826387986540794\n",
      "Epoch 53, Batch 20, Loss: 0.017945390194654465\n",
      "Epoch 53, Batch 30, Loss: 0.023999854922294617\n",
      "Epoch 53, Batch 40, Loss: 0.028126880526542664\n",
      "Epoch 53, Batch 50, Loss: 0.027929246425628662\n",
      "Epoch 53, Batch 60, Loss: 0.020309703424572945\n",
      "Epoch 53, Batch 70, Loss: 0.027595022693276405\n",
      "Epoch 53, Batch 80, Loss: 0.02299930527806282\n",
      "Epoch 53, Batch 90, Loss: 0.03392670303583145\n",
      "Epoch 53, Batch 100, Loss: 0.02736969292163849\n",
      "Epoch 54, Batch 0, Loss: 0.02767397090792656\n",
      "Epoch 54, Batch 10, Loss: 0.026185523718595505\n",
      "Epoch 54, Batch 20, Loss: 0.02132616937160492\n",
      "Epoch 54, Batch 30, Loss: 0.020791171118617058\n",
      "Epoch 54, Batch 40, Loss: 0.02030813694000244\n",
      "Epoch 54, Batch 50, Loss: 0.02971191331744194\n",
      "Epoch 54, Batch 60, Loss: 0.03222865238785744\n",
      "Epoch 54, Batch 70, Loss: 0.040227025747299194\n",
      "Epoch 54, Batch 80, Loss: 0.026381080970168114\n",
      "Epoch 54, Batch 90, Loss: 0.020586395636200905\n",
      "Epoch 54, Batch 100, Loss: 0.020507246255874634\n",
      "Epoch 55, Batch 0, Loss: 0.022486664354801178\n",
      "Epoch 55, Batch 10, Loss: 0.02070261351764202\n",
      "Epoch 55, Batch 20, Loss: 0.021459316834807396\n",
      "Epoch 55, Batch 30, Loss: 0.02061481401324272\n",
      "Epoch 55, Batch 40, Loss: 0.022381771355867386\n",
      "Epoch 55, Batch 50, Loss: 0.02382173016667366\n",
      "Epoch 55, Batch 60, Loss: 0.017833147197961807\n",
      "Epoch 55, Batch 70, Loss: 0.03394915536046028\n",
      "Epoch 55, Batch 80, Loss: 0.02308492176234722\n",
      "Epoch 55, Batch 90, Loss: 0.027124930173158646\n",
      "Epoch 55, Batch 100, Loss: 0.030890949070453644\n",
      "Epoch 56, Batch 0, Loss: 0.02342139184474945\n",
      "Epoch 56, Batch 10, Loss: 0.03081468865275383\n",
      "Epoch 56, Batch 20, Loss: 0.019315870478749275\n",
      "Epoch 56, Batch 30, Loss: 0.02254120633006096\n",
      "Epoch 56, Batch 40, Loss: 0.020923057571053505\n",
      "Epoch 56, Batch 50, Loss: 0.0212121419608593\n",
      "Epoch 56, Batch 60, Loss: 0.0166191253811121\n",
      "Epoch 56, Batch 70, Loss: 0.026407472789287567\n",
      "Epoch 56, Batch 80, Loss: 0.028996115550398827\n",
      "Epoch 56, Batch 90, Loss: 0.01886150613427162\n",
      "Epoch 56, Batch 100, Loss: 0.028916357085108757\n",
      "Epoch 57, Batch 0, Loss: 0.02359173819422722\n",
      "Epoch 57, Batch 10, Loss: 0.02596842125058174\n",
      "Epoch 57, Batch 20, Loss: 0.023478027433156967\n",
      "Epoch 57, Batch 30, Loss: 0.02463115192949772\n",
      "Epoch 57, Batch 40, Loss: 0.022198358550667763\n",
      "Epoch 57, Batch 50, Loss: 0.020748194307088852\n",
      "Epoch 57, Batch 60, Loss: 0.024663187563419342\n",
      "Epoch 57, Batch 70, Loss: 0.01862397789955139\n",
      "Epoch 57, Batch 80, Loss: 0.01769448257982731\n",
      "Epoch 57, Batch 90, Loss: 0.026484960690140724\n",
      "Epoch 57, Batch 100, Loss: 0.023448782041668892\n",
      "Epoch 58, Batch 0, Loss: 0.021690137684345245\n",
      "Epoch 58, Batch 10, Loss: 0.02044094167649746\n",
      "Epoch 58, Batch 20, Loss: 0.02574883960187435\n",
      "Epoch 58, Batch 30, Loss: 0.020010804757475853\n",
      "Epoch 58, Batch 40, Loss: 0.030313465744256973\n",
      "Epoch 58, Batch 50, Loss: 0.024980146437883377\n",
      "Epoch 58, Batch 60, Loss: 0.020299836993217468\n",
      "Epoch 58, Batch 70, Loss: 0.02504757046699524\n",
      "Epoch 58, Batch 80, Loss: 0.022801140323281288\n",
      "Epoch 58, Batch 90, Loss: 0.026547137647867203\n",
      "Epoch 58, Batch 100, Loss: 0.02277436852455139\n",
      "Epoch 59, Batch 0, Loss: 0.024523938074707985\n",
      "Epoch 59, Batch 10, Loss: 0.01973845064640045\n",
      "Epoch 59, Batch 20, Loss: 0.026331007480621338\n",
      "Epoch 59, Batch 30, Loss: 0.024405362084507942\n",
      "Epoch 59, Batch 40, Loss: 0.020480450242757797\n",
      "Epoch 59, Batch 50, Loss: 0.0235917828977108\n",
      "Epoch 59, Batch 60, Loss: 0.021209407597780228\n",
      "Epoch 59, Batch 70, Loss: 0.02892710454761982\n",
      "Epoch 59, Batch 80, Loss: 0.0275440476834774\n",
      "Epoch 59, Batch 90, Loss: 0.03354707732796669\n",
      "Epoch 59, Batch 100, Loss: 0.02863733097910881\n",
      "Epoch 60, Batch 0, Loss: 0.023204613476991653\n",
      "Epoch 60, Batch 10, Loss: 0.030665969476103783\n",
      "Epoch 60, Batch 20, Loss: 0.01936580426990986\n",
      "Epoch 60, Batch 30, Loss: 0.03231032192707062\n",
      "Epoch 60, Batch 40, Loss: 0.01660667546093464\n",
      "Epoch 60, Batch 50, Loss: 0.025167420506477356\n",
      "Epoch 60, Batch 60, Loss: 0.01994038186967373\n",
      "Epoch 60, Batch 70, Loss: 0.022519350051879883\n",
      "Epoch 60, Batch 80, Loss: 0.025907261297106743\n",
      "Epoch 60, Batch 90, Loss: 0.019101060926914215\n",
      "Epoch 60, Batch 100, Loss: 0.019270461052656174\n",
      "Epoch 61, Batch 0, Loss: 0.025889340788125992\n",
      "Epoch 61, Batch 10, Loss: 0.01958145573735237\n",
      "Epoch 61, Batch 20, Loss: 0.02058972604572773\n",
      "Epoch 61, Batch 30, Loss: 0.024743657559156418\n",
      "Epoch 61, Batch 40, Loss: 0.0231282040476799\n",
      "Epoch 61, Batch 50, Loss: 0.03186510503292084\n",
      "Epoch 61, Batch 60, Loss: 0.025293493643403053\n",
      "Epoch 61, Batch 70, Loss: 0.022112995386123657\n",
      "Epoch 61, Batch 80, Loss: 0.027550026774406433\n",
      "Epoch 61, Batch 90, Loss: 0.019979942589998245\n",
      "Epoch 61, Batch 100, Loss: 0.026055462658405304\n",
      "Epoch 62, Batch 0, Loss: 0.021540269255638123\n",
      "Epoch 62, Batch 10, Loss: 0.021409180015325546\n",
      "Epoch 62, Batch 20, Loss: 0.02295837365090847\n",
      "Epoch 62, Batch 30, Loss: 0.02736857905983925\n",
      "Epoch 62, Batch 40, Loss: 0.02182336337864399\n",
      "Epoch 62, Batch 50, Loss: 0.03080175444483757\n",
      "Epoch 62, Batch 60, Loss: 0.02090580202639103\n",
      "Epoch 62, Batch 70, Loss: 0.024728938937187195\n",
      "Epoch 62, Batch 80, Loss: 0.02338915504515171\n",
      "Epoch 62, Batch 90, Loss: 0.02165800705552101\n",
      "Epoch 62, Batch 100, Loss: 0.023130374029278755\n",
      "Epoch 63, Batch 0, Loss: 0.019861498847603798\n",
      "Epoch 63, Batch 10, Loss: 0.024944014847278595\n",
      "Epoch 63, Batch 20, Loss: 0.020861754193902016\n",
      "Epoch 63, Batch 30, Loss: 0.029480963945388794\n",
      "Epoch 63, Batch 40, Loss: 0.03130993992090225\n",
      "Epoch 63, Batch 50, Loss: 0.020003173500299454\n",
      "Epoch 63, Batch 60, Loss: 0.026956137269735336\n",
      "Epoch 63, Batch 70, Loss: 0.024853331968188286\n",
      "Epoch 63, Batch 80, Loss: 0.02241476997733116\n",
      "Epoch 63, Batch 90, Loss: 0.024674728512763977\n",
      "Epoch 63, Batch 100, Loss: 0.022456642240285873\n",
      "Epoch 64, Batch 0, Loss: 0.024886228144168854\n",
      "Epoch 64, Batch 10, Loss: 0.020697513595223427\n",
      "Epoch 64, Batch 20, Loss: 0.023587031289935112\n",
      "Epoch 64, Batch 30, Loss: 0.02399139106273651\n",
      "Epoch 64, Batch 40, Loss: 0.025330623611807823\n",
      "Epoch 64, Batch 50, Loss: 0.027157073840498924\n",
      "Epoch 64, Batch 60, Loss: 0.030832644551992416\n",
      "Epoch 64, Batch 70, Loss: 0.03336550295352936\n",
      "Epoch 64, Batch 80, Loss: 0.025814788416028023\n",
      "Epoch 64, Batch 90, Loss: 0.023813258856534958\n",
      "Epoch 64, Batch 100, Loss: 0.021460693329572678\n",
      "Epoch 65, Batch 0, Loss: 0.02289273403584957\n",
      "Epoch 65, Batch 10, Loss: 0.021425757557153702\n",
      "Epoch 65, Batch 20, Loss: 0.023340580984950066\n",
      "Epoch 65, Batch 30, Loss: 0.01541692391037941\n",
      "Epoch 65, Batch 40, Loss: 0.022134438157081604\n",
      "Epoch 65, Batch 50, Loss: 0.02263886108994484\n",
      "Epoch 65, Batch 60, Loss: 0.027464089915156364\n",
      "Epoch 65, Batch 70, Loss: 0.02527255192399025\n",
      "Epoch 65, Batch 80, Loss: 0.025843879207968712\n",
      "Epoch 65, Batch 90, Loss: 0.029253650456666946\n",
      "Epoch 65, Batch 100, Loss: 0.035625167191028595\n",
      "Epoch 66, Batch 0, Loss: 0.022164005786180496\n",
      "Epoch 66, Batch 10, Loss: 0.02314828895032406\n",
      "Epoch 66, Batch 20, Loss: 0.02752302959561348\n",
      "Epoch 66, Batch 30, Loss: 0.01696123369038105\n",
      "Epoch 66, Batch 40, Loss: 0.016670655459165573\n",
      "Epoch 66, Batch 50, Loss: 0.025045093148946762\n",
      "Epoch 66, Batch 60, Loss: 0.024996347725391388\n",
      "Epoch 66, Batch 70, Loss: 0.023031845688819885\n",
      "Epoch 66, Batch 80, Loss: 0.023077480494976044\n",
      "Epoch 66, Batch 90, Loss: 0.026125900447368622\n",
      "Epoch 66, Batch 100, Loss: 0.018745949491858482\n",
      "Epoch 67, Batch 0, Loss: 0.021190326660871506\n",
      "Epoch 67, Batch 10, Loss: 0.025403598323464394\n",
      "Epoch 67, Batch 20, Loss: 0.032730359584093094\n",
      "Epoch 67, Batch 30, Loss: 0.01873481646180153\n",
      "Epoch 67, Batch 40, Loss: 0.023615961894392967\n",
      "Epoch 67, Batch 50, Loss: 0.02425716444849968\n",
      "Epoch 67, Batch 60, Loss: 0.022623766213655472\n",
      "Epoch 67, Batch 70, Loss: 0.02642427571117878\n",
      "Epoch 67, Batch 80, Loss: 0.03690529242157936\n",
      "Epoch 67, Batch 90, Loss: 0.02981455996632576\n",
      "Epoch 67, Batch 100, Loss: 0.02449364773929119\n",
      "Epoch 68, Batch 0, Loss: 0.024230409413576126\n",
      "Epoch 68, Batch 10, Loss: 0.022299770265817642\n",
      "Epoch 68, Batch 20, Loss: 0.019672442227602005\n",
      "Epoch 68, Batch 30, Loss: 0.02326725423336029\n",
      "Epoch 68, Batch 40, Loss: 0.018664509057998657\n",
      "Epoch 68, Batch 50, Loss: 0.0225976575165987\n",
      "Epoch 68, Batch 60, Loss: 0.025644434615969658\n",
      "Epoch 68, Batch 70, Loss: 0.019545622169971466\n",
      "Epoch 68, Batch 80, Loss: 0.02243679389357567\n",
      "Epoch 68, Batch 90, Loss: 0.028164153918623924\n",
      "Epoch 68, Batch 100, Loss: 0.030061857774853706\n",
      "Epoch 69, Batch 0, Loss: 0.01692950911819935\n",
      "Epoch 69, Batch 10, Loss: 0.024561289697885513\n",
      "Epoch 69, Batch 20, Loss: 0.02528768964111805\n",
      "Epoch 69, Batch 30, Loss: 0.025003818795084953\n",
      "Epoch 69, Batch 40, Loss: 0.020694870501756668\n",
      "Epoch 69, Batch 50, Loss: 0.030992062762379646\n",
      "Epoch 69, Batch 60, Loss: 0.02194988541305065\n",
      "Epoch 69, Batch 70, Loss: 0.02201238088309765\n",
      "Epoch 69, Batch 80, Loss: 0.026048514991998672\n",
      "Epoch 69, Batch 90, Loss: 0.02834795042872429\n",
      "Epoch 69, Batch 100, Loss: 0.023783020675182343\n",
      "Epoch 70, Batch 0, Loss: 0.027135521173477173\n",
      "Epoch 70, Batch 10, Loss: 0.018418041989207268\n",
      "Epoch 70, Batch 20, Loss: 0.019698690623044968\n",
      "Epoch 70, Batch 30, Loss: 0.02780335396528244\n",
      "Epoch 70, Batch 40, Loss: 0.025461465120315552\n",
      "Epoch 70, Batch 50, Loss: 0.025989055633544922\n",
      "Epoch 70, Batch 60, Loss: 0.02692747302353382\n",
      "Epoch 70, Batch 70, Loss: 0.027482178062200546\n",
      "Epoch 70, Batch 80, Loss: 0.025319363921880722\n",
      "Epoch 70, Batch 90, Loss: 0.021092427894473076\n",
      "Epoch 70, Batch 100, Loss: 0.021360989660024643\n",
      "Epoch 71, Batch 0, Loss: 0.02970529906451702\n",
      "Epoch 71, Batch 10, Loss: 0.02291344851255417\n",
      "Epoch 71, Batch 20, Loss: 0.031056605279445648\n",
      "Epoch 71, Batch 30, Loss: 0.022703906521201134\n",
      "Epoch 71, Batch 40, Loss: 0.0157450083643198\n",
      "Epoch 71, Batch 50, Loss: 0.02346908114850521\n",
      "Epoch 71, Batch 60, Loss: 0.01841874048113823\n",
      "Epoch 71, Batch 70, Loss: 0.026307249441742897\n",
      "Epoch 71, Batch 80, Loss: 0.027236584573984146\n",
      "Epoch 71, Batch 90, Loss: 0.02315456047654152\n",
      "Epoch 71, Batch 100, Loss: 0.024017468094825745\n",
      "Epoch 72, Batch 0, Loss: 0.020788490772247314\n",
      "Epoch 72, Batch 10, Loss: 0.03079497069120407\n",
      "Epoch 72, Batch 20, Loss: 0.02346004918217659\n",
      "Epoch 72, Batch 30, Loss: 0.019318081438541412\n",
      "Epoch 72, Batch 40, Loss: 0.02688278816640377\n",
      "Epoch 72, Batch 50, Loss: 0.016645371913909912\n",
      "Epoch 72, Batch 60, Loss: 0.02734636515378952\n",
      "Epoch 72, Batch 70, Loss: 0.025372428819537163\n",
      "Epoch 72, Batch 80, Loss: 0.026736415922641754\n",
      "Epoch 72, Batch 90, Loss: 0.022066058591008186\n",
      "Epoch 72, Batch 100, Loss: 0.01820180006325245\n",
      "Epoch 73, Batch 0, Loss: 0.02291889116168022\n",
      "Epoch 73, Batch 10, Loss: 0.020259739831089973\n",
      "Epoch 73, Batch 20, Loss: 0.024687938392162323\n",
      "Epoch 73, Batch 30, Loss: 0.027226733043789864\n",
      "Epoch 73, Batch 40, Loss: 0.021752309054136276\n",
      "Epoch 73, Batch 50, Loss: 0.02213115803897381\n",
      "Epoch 73, Batch 60, Loss: 0.023594215512275696\n",
      "Epoch 73, Batch 70, Loss: 0.026946164667606354\n",
      "Epoch 73, Batch 80, Loss: 0.023611625656485558\n",
      "Epoch 73, Batch 90, Loss: 0.02124713733792305\n",
      "Epoch 73, Batch 100, Loss: 0.024668393656611443\n",
      "Epoch 74, Batch 0, Loss: 0.025477919727563858\n",
      "Epoch 74, Batch 10, Loss: 0.02853662520647049\n",
      "Epoch 74, Batch 20, Loss: 0.02925817295908928\n",
      "Epoch 74, Batch 30, Loss: 0.02044856920838356\n",
      "Epoch 74, Batch 40, Loss: 0.032265596091747284\n",
      "Epoch 74, Batch 50, Loss: 0.03709070384502411\n",
      "Epoch 74, Batch 60, Loss: 0.025440379977226257\n",
      "Epoch 74, Batch 70, Loss: 0.020716967061161995\n",
      "Epoch 74, Batch 80, Loss: 0.023152941837906837\n",
      "Epoch 74, Batch 90, Loss: 0.020207254216074944\n",
      "Epoch 74, Batch 100, Loss: 0.025123994797468185\n",
      "Epoch 75, Batch 0, Loss: 0.023688064888119698\n",
      "Epoch 75, Batch 10, Loss: 0.03288080915808678\n",
      "Epoch 75, Batch 20, Loss: 0.021649152040481567\n",
      "Epoch 75, Batch 30, Loss: 0.02653537318110466\n",
      "Epoch 75, Batch 40, Loss: 0.03137384355068207\n",
      "Epoch 75, Batch 50, Loss: 0.023497186601161957\n",
      "Epoch 75, Batch 60, Loss: 0.02114110440015793\n",
      "Epoch 75, Batch 70, Loss: 0.01811533235013485\n",
      "Epoch 75, Batch 80, Loss: 0.017190203070640564\n",
      "Epoch 75, Batch 90, Loss: 0.019920174032449722\n",
      "Epoch 75, Batch 100, Loss: 0.01789798215031624\n",
      "Epoch 76, Batch 0, Loss: 0.026131603866815567\n",
      "Epoch 76, Batch 10, Loss: 0.024651173502206802\n",
      "Epoch 76, Batch 20, Loss: 0.024325042963027954\n",
      "Epoch 76, Batch 30, Loss: 0.024458134546875954\n",
      "Epoch 76, Batch 40, Loss: 0.03271230682730675\n",
      "Epoch 76, Batch 50, Loss: 0.025047361850738525\n",
      "Epoch 76, Batch 60, Loss: 0.03219789266586304\n",
      "Epoch 76, Batch 70, Loss: 0.027936168015003204\n",
      "Epoch 76, Batch 80, Loss: 0.03136930242180824\n",
      "Epoch 76, Batch 90, Loss: 0.01740698143839836\n",
      "Epoch 76, Batch 100, Loss: 0.023288564756512642\n",
      "Epoch 77, Batch 0, Loss: 0.021855439990758896\n",
      "Epoch 77, Batch 10, Loss: 0.03616056591272354\n",
      "Epoch 77, Batch 20, Loss: 0.020059457048773766\n",
      "Epoch 77, Batch 30, Loss: 0.019934283569455147\n",
      "Epoch 77, Batch 40, Loss: 0.023125045001506805\n",
      "Epoch 77, Batch 50, Loss: 0.02468695305287838\n",
      "Epoch 77, Batch 60, Loss: 0.02523152157664299\n",
      "Epoch 77, Batch 70, Loss: 0.020165374502539635\n",
      "Epoch 77, Batch 80, Loss: 0.022517919540405273\n",
      "Epoch 77, Batch 90, Loss: 0.03165767341852188\n",
      "Epoch 77, Batch 100, Loss: 0.02380700409412384\n",
      "Epoch 78, Batch 0, Loss: 0.024189230054616928\n",
      "Epoch 78, Batch 10, Loss: 0.020832562819123268\n",
      "Epoch 78, Batch 20, Loss: 0.01973358355462551\n",
      "Epoch 78, Batch 30, Loss: 0.02108282968401909\n",
      "Epoch 78, Batch 40, Loss: 0.023429393768310547\n",
      "Epoch 78, Batch 50, Loss: 0.019053775817155838\n",
      "Epoch 78, Batch 60, Loss: 0.021035809069871902\n",
      "Epoch 78, Batch 70, Loss: 0.02668110467493534\n",
      "Epoch 78, Batch 80, Loss: 0.024125969037413597\n",
      "Epoch 78, Batch 90, Loss: 0.023412995040416718\n",
      "Epoch 78, Batch 100, Loss: 0.02606610395014286\n",
      "Epoch 79, Batch 0, Loss: 0.022313963621854782\n",
      "Epoch 79, Batch 10, Loss: 0.020424267277121544\n",
      "Epoch 79, Batch 20, Loss: 0.02596341073513031\n",
      "Epoch 79, Batch 30, Loss: 0.020512228831648827\n",
      "Epoch 79, Batch 40, Loss: 0.027123592793941498\n",
      "Epoch 79, Batch 50, Loss: 0.026682600378990173\n",
      "Epoch 79, Batch 60, Loss: 0.02050492726266384\n",
      "Epoch 79, Batch 70, Loss: 0.01555817760527134\n",
      "Epoch 79, Batch 80, Loss: 0.022215843200683594\n",
      "Epoch 79, Batch 90, Loss: 0.026059824973344803\n",
      "Epoch 79, Batch 100, Loss: 0.01935863494873047\n",
      "Epoch 80, Batch 0, Loss: 0.02643369510769844\n",
      "Epoch 80, Batch 10, Loss: 0.021845411509275436\n",
      "Epoch 80, Batch 20, Loss: 0.022223414853215218\n",
      "Epoch 80, Batch 30, Loss: 0.02977018803358078\n",
      "Epoch 80, Batch 40, Loss: 0.024226926267147064\n",
      "Epoch 80, Batch 50, Loss: 0.027154138311743736\n",
      "Epoch 80, Batch 60, Loss: 0.02120950259268284\n",
      "Epoch 80, Batch 70, Loss: 0.02568104676902294\n",
      "Epoch 80, Batch 80, Loss: 0.017853453755378723\n",
      "Epoch 80, Batch 90, Loss: 0.021496687084436417\n",
      "Epoch 80, Batch 100, Loss: 0.03894031047821045\n",
      "Epoch 81, Batch 0, Loss: 0.044821541756391525\n",
      "Epoch 81, Batch 10, Loss: 0.02090299129486084\n",
      "Epoch 81, Batch 20, Loss: 0.025790054351091385\n",
      "Epoch 81, Batch 30, Loss: 0.019830312579870224\n",
      "Epoch 81, Batch 40, Loss: 0.02072213776409626\n",
      "Epoch 81, Batch 50, Loss: 0.020511819049715996\n",
      "Epoch 81, Batch 60, Loss: 0.026904478669166565\n",
      "Epoch 81, Batch 70, Loss: 0.02442294918000698\n",
      "Epoch 81, Batch 80, Loss: 0.02478594332933426\n",
      "Epoch 81, Batch 90, Loss: 0.018739385530352592\n",
      "Epoch 81, Batch 100, Loss: 0.02341248095035553\n",
      "Epoch 82, Batch 0, Loss: 0.02035537175834179\n",
      "Epoch 82, Batch 10, Loss: 0.020809773355722427\n",
      "Epoch 82, Batch 20, Loss: 0.025167949497699738\n",
      "Epoch 82, Batch 30, Loss: 0.018371064215898514\n",
      "Epoch 82, Batch 40, Loss: 0.026422660797834396\n",
      "Epoch 82, Batch 50, Loss: 0.027721956372261047\n",
      "Epoch 82, Batch 60, Loss: 0.0179606806486845\n",
      "Epoch 82, Batch 70, Loss: 0.0218217596411705\n",
      "Epoch 82, Batch 80, Loss: 0.025113016366958618\n",
      "Epoch 82, Batch 90, Loss: 0.02625049278140068\n",
      "Epoch 82, Batch 100, Loss: 0.024173414334654808\n",
      "Epoch 83, Batch 0, Loss: 0.02173958159983158\n",
      "Epoch 83, Batch 10, Loss: 0.022401822730898857\n",
      "Epoch 83, Batch 20, Loss: 0.02468760311603546\n",
      "Epoch 83, Batch 30, Loss: 0.022406451404094696\n",
      "Epoch 83, Batch 40, Loss: 0.023401275277137756\n",
      "Epoch 83, Batch 50, Loss: 0.028514182195067406\n",
      "Epoch 83, Batch 60, Loss: 0.016847150400280952\n",
      "Epoch 83, Batch 70, Loss: 0.02394353412091732\n",
      "Epoch 83, Batch 80, Loss: 0.019662009552121162\n",
      "Epoch 83, Batch 90, Loss: 0.025137439370155334\n",
      "Epoch 83, Batch 100, Loss: 0.025167139247059822\n",
      "Epoch 84, Batch 0, Loss: 0.021331679075956345\n",
      "Epoch 84, Batch 10, Loss: 0.018134262412786484\n",
      "Epoch 84, Batch 20, Loss: 0.014843527227640152\n",
      "Epoch 84, Batch 30, Loss: 0.024232761934399605\n",
      "Epoch 84, Batch 40, Loss: 0.018527066335082054\n",
      "Epoch 84, Batch 50, Loss: 0.026623772457242012\n",
      "Epoch 84, Batch 60, Loss: 0.021548479795455933\n",
      "Epoch 84, Batch 70, Loss: 0.024333123117685318\n",
      "Epoch 84, Batch 80, Loss: 0.024913828819990158\n",
      "Epoch 84, Batch 90, Loss: 0.018448030576109886\n",
      "Epoch 84, Batch 100, Loss: 0.0301295667886734\n",
      "Epoch 85, Batch 0, Loss: 0.02528384141623974\n",
      "Epoch 85, Batch 10, Loss: 0.021652014926075935\n",
      "Epoch 85, Batch 20, Loss: 0.022732924669981003\n",
      "Epoch 85, Batch 30, Loss: 0.030291344970464706\n",
      "Epoch 85, Batch 40, Loss: 0.0255458764731884\n",
      "Epoch 85, Batch 50, Loss: 0.015990687534213066\n",
      "Epoch 85, Batch 60, Loss: 0.026578467339277267\n",
      "Epoch 85, Batch 70, Loss: 0.018055113032460213\n",
      "Epoch 85, Batch 80, Loss: 0.022477859631180763\n",
      "Epoch 85, Batch 90, Loss: 0.02729346975684166\n",
      "Epoch 85, Batch 100, Loss: 0.025433288887143135\n",
      "Epoch 86, Batch 0, Loss: 0.03407970070838928\n",
      "Epoch 86, Batch 10, Loss: 0.032817695289850235\n",
      "Epoch 86, Batch 20, Loss: 0.019252881407737732\n",
      "Epoch 86, Batch 30, Loss: 0.027716517448425293\n",
      "Epoch 86, Batch 40, Loss: 0.024815170094370842\n",
      "Epoch 86, Batch 50, Loss: 0.022740375250577927\n",
      "Epoch 86, Batch 60, Loss: 0.019748950377106667\n",
      "Epoch 86, Batch 70, Loss: 0.01835460588335991\n",
      "Epoch 86, Batch 80, Loss: 0.020639874041080475\n",
      "Epoch 86, Batch 90, Loss: 0.02403043955564499\n",
      "Epoch 86, Batch 100, Loss: 0.02531970478594303\n",
      "Epoch 87, Batch 0, Loss: 0.018962781876325607\n",
      "Epoch 87, Batch 10, Loss: 0.029709279537200928\n",
      "Epoch 87, Batch 20, Loss: 0.02680862694978714\n",
      "Epoch 87, Batch 30, Loss: 0.030199717730283737\n",
      "Epoch 87, Batch 40, Loss: 0.028644541278481483\n",
      "Epoch 87, Batch 50, Loss: 0.024956336244940758\n",
      "Epoch 87, Batch 60, Loss: 0.024802187457680702\n",
      "Epoch 87, Batch 70, Loss: 0.021732084453105927\n",
      "Epoch 87, Batch 80, Loss: 0.024297980591654778\n",
      "Epoch 87, Batch 90, Loss: 0.021350592374801636\n",
      "Epoch 87, Batch 100, Loss: 0.025742126628756523\n",
      "Epoch 88, Batch 0, Loss: 0.025839626789093018\n",
      "Epoch 88, Batch 10, Loss: 0.03419943153858185\n",
      "Epoch 88, Batch 20, Loss: 0.03149472922086716\n",
      "Epoch 88, Batch 30, Loss: 0.033834636211395264\n",
      "Epoch 88, Batch 40, Loss: 0.0244264118373394\n",
      "Epoch 88, Batch 50, Loss: 0.026102591305971146\n",
      "Epoch 88, Batch 60, Loss: 0.02080129086971283\n",
      "Epoch 88, Batch 70, Loss: 0.026488181203603745\n",
      "Epoch 88, Batch 80, Loss: 0.022341128438711166\n",
      "Epoch 88, Batch 90, Loss: 0.024542447179555893\n",
      "Epoch 88, Batch 100, Loss: 0.02162143401801586\n",
      "Epoch 89, Batch 0, Loss: 0.023517191410064697\n",
      "Epoch 89, Batch 10, Loss: 0.01988152042031288\n",
      "Epoch 89, Batch 20, Loss: 0.024722140282392502\n",
      "Epoch 89, Batch 30, Loss: 0.026477059349417686\n",
      "Epoch 89, Batch 40, Loss: 0.020571989938616753\n",
      "Epoch 89, Batch 50, Loss: 0.024375729262828827\n",
      "Epoch 89, Batch 60, Loss: 0.026252973824739456\n",
      "Epoch 89, Batch 70, Loss: 0.021173272281885147\n",
      "Epoch 89, Batch 80, Loss: 0.02080266922712326\n",
      "Epoch 89, Batch 90, Loss: 0.017996104434132576\n",
      "Epoch 89, Batch 100, Loss: 0.02157306671142578\n",
      "Epoch 90, Batch 0, Loss: 0.024731621146202087\n",
      "Epoch 90, Batch 10, Loss: 0.021336030215024948\n",
      "Epoch 90, Batch 20, Loss: 0.027103392407298088\n",
      "Epoch 90, Batch 30, Loss: 0.025406382977962494\n",
      "Epoch 90, Batch 40, Loss: 0.02405494824051857\n",
      "Epoch 90, Batch 50, Loss: 0.022491324692964554\n",
      "Epoch 90, Batch 60, Loss: 0.01771741546690464\n",
      "Epoch 90, Batch 70, Loss: 0.026604026556015015\n",
      "Epoch 90, Batch 80, Loss: 0.02348928153514862\n",
      "Epoch 90, Batch 90, Loss: 0.02684895507991314\n",
      "Epoch 90, Batch 100, Loss: 0.018441092222929\n",
      "Epoch 91, Batch 0, Loss: 0.0222767386585474\n",
      "Epoch 91, Batch 10, Loss: 0.017453307285904884\n",
      "Epoch 91, Batch 20, Loss: 0.02530447021126747\n",
      "Epoch 91, Batch 30, Loss: 0.02387799136340618\n",
      "Epoch 91, Batch 40, Loss: 0.01926463097333908\n",
      "Epoch 91, Batch 50, Loss: 0.024361958727240562\n",
      "Epoch 91, Batch 60, Loss: 0.01875603199005127\n",
      "Epoch 91, Batch 70, Loss: 0.028810439631342888\n",
      "Epoch 91, Batch 80, Loss: 0.020476963371038437\n",
      "Epoch 91, Batch 90, Loss: 0.01687062345445156\n",
      "Epoch 91, Batch 100, Loss: 0.02281976118683815\n",
      "Epoch 92, Batch 0, Loss: 0.022897187620401382\n",
      "Epoch 92, Batch 10, Loss: 0.024591224268078804\n",
      "Epoch 92, Batch 20, Loss: 0.023183433338999748\n",
      "Epoch 92, Batch 30, Loss: 0.022615309804677963\n",
      "Epoch 92, Batch 40, Loss: 0.029808036983013153\n",
      "Epoch 92, Batch 50, Loss: 0.026211196556687355\n",
      "Epoch 92, Batch 60, Loss: 0.024614457041025162\n",
      "Epoch 92, Batch 70, Loss: 0.02761276438832283\n",
      "Epoch 92, Batch 80, Loss: 0.023017896339297295\n",
      "Epoch 92, Batch 90, Loss: 0.026637909933924675\n",
      "Epoch 92, Batch 100, Loss: 0.019415829330682755\n",
      "Epoch 93, Batch 0, Loss: 0.02777812071144581\n",
      "Epoch 93, Batch 10, Loss: 0.019607063382864\n",
      "Epoch 93, Batch 20, Loss: 0.023672528564929962\n",
      "Epoch 93, Batch 30, Loss: 0.021509965881705284\n",
      "Epoch 93, Batch 40, Loss: 0.025520779192447662\n",
      "Epoch 93, Batch 50, Loss: 0.023764587938785553\n",
      "Epoch 93, Batch 60, Loss: 0.026836689561605453\n",
      "Epoch 93, Batch 70, Loss: 0.02430916205048561\n",
      "Epoch 93, Batch 80, Loss: 0.01874540001153946\n",
      "Epoch 93, Batch 90, Loss: 0.02402813732624054\n",
      "Epoch 93, Batch 100, Loss: 0.024707317352294922\n",
      "Epoch 94, Batch 0, Loss: 0.019163953140378\n",
      "Epoch 94, Batch 10, Loss: 0.02658798173069954\n",
      "Epoch 94, Batch 20, Loss: 0.027590785175561905\n",
      "Epoch 94, Batch 30, Loss: 0.024008603766560555\n",
      "Epoch 94, Batch 40, Loss: 0.02501625567674637\n",
      "Epoch 94, Batch 50, Loss: 0.03027074597775936\n",
      "Epoch 94, Batch 60, Loss: 0.03300748020410538\n",
      "Epoch 94, Batch 70, Loss: 0.027305113151669502\n",
      "Epoch 94, Batch 80, Loss: 0.01665477082133293\n",
      "Epoch 94, Batch 90, Loss: 0.02260841801762581\n",
      "Epoch 94, Batch 100, Loss: 0.017783084884285927\n",
      "Epoch 95, Batch 0, Loss: 0.028413698077201843\n",
      "Epoch 95, Batch 10, Loss: 0.027142509818077087\n",
      "Epoch 95, Batch 20, Loss: 0.021675975993275642\n",
      "Epoch 95, Batch 30, Loss: 0.019569329917430878\n",
      "Epoch 95, Batch 40, Loss: 0.024273423478007317\n",
      "Epoch 95, Batch 50, Loss: 0.02420451119542122\n",
      "Epoch 95, Batch 60, Loss: 0.023296067491173744\n",
      "Epoch 95, Batch 70, Loss: 0.022689543664455414\n",
      "Epoch 95, Batch 80, Loss: 0.020398471504449844\n",
      "Epoch 95, Batch 90, Loss: 0.022730745375156403\n",
      "Epoch 95, Batch 100, Loss: 0.02969236858189106\n",
      "Epoch 96, Batch 0, Loss: 0.026221955195069313\n",
      "Epoch 96, Batch 10, Loss: 0.026214174926280975\n",
      "Epoch 96, Batch 20, Loss: 0.02215251699090004\n",
      "Epoch 96, Batch 30, Loss: 0.02430119179189205\n",
      "Epoch 96, Batch 40, Loss: 0.021595697849988937\n",
      "Epoch 96, Batch 50, Loss: 0.023525100201368332\n",
      "Epoch 96, Batch 60, Loss: 0.024281371384859085\n",
      "Epoch 96, Batch 70, Loss: 0.02785535901784897\n",
      "Epoch 96, Batch 80, Loss: 0.021211281418800354\n",
      "Epoch 96, Batch 90, Loss: 0.019495634362101555\n",
      "Epoch 96, Batch 100, Loss: 0.027072684839367867\n",
      "Epoch 97, Batch 0, Loss: 0.027248306199908257\n",
      "Epoch 97, Batch 10, Loss: 0.02170902118086815\n",
      "Epoch 97, Batch 20, Loss: 0.024504633620381355\n",
      "Epoch 97, Batch 30, Loss: 0.025263585150241852\n",
      "Epoch 97, Batch 40, Loss: 0.025504309684038162\n",
      "Epoch 97, Batch 50, Loss: 0.02503531239926815\n",
      "Epoch 97, Batch 60, Loss: 0.024379588663578033\n",
      "Epoch 97, Batch 70, Loss: 0.014657193794846535\n",
      "Epoch 97, Batch 80, Loss: 0.02629043161869049\n",
      "Epoch 97, Batch 90, Loss: 0.02090628072619438\n",
      "Epoch 97, Batch 100, Loss: 0.026346631348133087\n",
      "Epoch 98, Batch 0, Loss: 0.023866305127739906\n",
      "Epoch 98, Batch 10, Loss: 0.02155030146241188\n",
      "Epoch 98, Batch 20, Loss: 0.024834370240569115\n",
      "Epoch 98, Batch 30, Loss: 0.03126409649848938\n",
      "Epoch 98, Batch 40, Loss: 0.030195636674761772\n",
      "Epoch 98, Batch 50, Loss: 0.02493385225534439\n",
      "Epoch 98, Batch 60, Loss: 0.02511173114180565\n",
      "Epoch 98, Batch 70, Loss: 0.024460110813379288\n",
      "Epoch 98, Batch 80, Loss: 0.017824240028858185\n",
      "Epoch 98, Batch 90, Loss: 0.019292576238512993\n",
      "Epoch 98, Batch 100, Loss: 0.02458161488175392\n",
      "Epoch 99, Batch 0, Loss: 0.026728663593530655\n",
      "Epoch 99, Batch 10, Loss: 0.021977990865707397\n",
      "Epoch 99, Batch 20, Loss: 0.023358967155218124\n",
      "Epoch 99, Batch 30, Loss: 0.01926085166633129\n",
      "Epoch 99, Batch 40, Loss: 0.02712438814342022\n",
      "Epoch 99, Batch 50, Loss: 0.01805899664759636\n",
      "Epoch 99, Batch 60, Loss: 0.018977241590619087\n",
      "Epoch 99, Batch 70, Loss: 0.018064752221107483\n",
      "Epoch 99, Batch 80, Loss: 0.023910684511065483\n",
      "Epoch 99, Batch 90, Loss: 0.02582702226936817\n",
      "Epoch 99, Batch 100, Loss: 0.025209231302142143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_BiLSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True)\n",
       "    (fc1): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.040860; Test RMSE 43.385241\n",
      "\n",
      "Train  MAE: 0.023309; Test  MAE 30.753394\n",
      "\n",
      "Train  R^2: 0.998347; Test  R^2 0.957258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BILSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BILSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.bilstm = BILSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x)\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Reshape x to fit BiLSTM input requirements\n",
    "        x = x.contiguous().view(batch_size, seq_len, -1)\n",
    "        x = self.bilstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_BILSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.06291728466749191\n",
      "Epoch 0, Batch 10, Loss: 0.08043981343507767\n",
      "Epoch 0, Batch 20, Loss: 0.08701319992542267\n",
      "Epoch 0, Batch 30, Loss: 0.09361810982227325\n",
      "Epoch 0, Batch 40, Loss: 0.06424490362405777\n",
      "Epoch 0, Batch 50, Loss: 0.04788050800561905\n",
      "Epoch 0, Batch 60, Loss: 0.08356991410255432\n",
      "Epoch 0, Batch 70, Loss: 0.10480913519859314\n",
      "Epoch 0, Batch 80, Loss: 0.060703881084918976\n",
      "Epoch 0, Batch 90, Loss: 0.07559864223003387\n",
      "Epoch 0, Batch 100, Loss: 0.04077320545911789\n",
      "Epoch 1, Batch 0, Loss: 0.06718764454126358\n",
      "Epoch 1, Batch 10, Loss: 0.03783310949802399\n",
      "Epoch 1, Batch 20, Loss: 0.029751121997833252\n",
      "Epoch 1, Batch 30, Loss: 0.0717022716999054\n",
      "Epoch 1, Batch 40, Loss: 0.07472895085811615\n",
      "Epoch 1, Batch 50, Loss: 0.05690589174628258\n",
      "Epoch 1, Batch 60, Loss: 0.055589087307453156\n",
      "Epoch 1, Batch 70, Loss: 0.06037123128771782\n",
      "Epoch 1, Batch 80, Loss: 0.032200418412685394\n",
      "Epoch 1, Batch 90, Loss: 0.05787580832839012\n",
      "Epoch 1, Batch 100, Loss: 0.07136482000350952\n",
      "Epoch 2, Batch 0, Loss: 0.03838171437382698\n",
      "Epoch 2, Batch 10, Loss: 0.046300023794174194\n",
      "Epoch 2, Batch 20, Loss: 0.03950824588537216\n",
      "Epoch 2, Batch 30, Loss: 0.04951029643416405\n",
      "Epoch 2, Batch 40, Loss: 0.06809112429618835\n",
      "Epoch 2, Batch 50, Loss: 0.05421034246683121\n",
      "Epoch 2, Batch 60, Loss: 0.04339058697223663\n",
      "Epoch 2, Batch 70, Loss: 0.03131823241710663\n",
      "Epoch 2, Batch 80, Loss: 0.04130277782678604\n",
      "Epoch 2, Batch 90, Loss: 0.05821618065237999\n",
      "Epoch 2, Batch 100, Loss: 0.08939504623413086\n",
      "Epoch 3, Batch 0, Loss: 0.035729628056287766\n",
      "Epoch 3, Batch 10, Loss: 0.036631759256124496\n",
      "Epoch 3, Batch 20, Loss: 0.04882831871509552\n",
      "Epoch 3, Batch 30, Loss: 0.0553140752017498\n",
      "Epoch 3, Batch 40, Loss: 0.04735960438847542\n",
      "Epoch 3, Batch 50, Loss: 0.040636807680130005\n",
      "Epoch 3, Batch 60, Loss: 0.02991102635860443\n",
      "Epoch 3, Batch 70, Loss: 0.03226836770772934\n",
      "Epoch 3, Batch 80, Loss: 0.0496496856212616\n",
      "Epoch 3, Batch 90, Loss: 0.03712855651974678\n",
      "Epoch 3, Batch 100, Loss: 0.05055244639515877\n",
      "Epoch 4, Batch 0, Loss: 0.037902869284152985\n",
      "Epoch 4, Batch 10, Loss: 0.03003716468811035\n",
      "Epoch 4, Batch 20, Loss: 0.0520973838865757\n",
      "Epoch 4, Batch 30, Loss: 0.04009389877319336\n",
      "Epoch 4, Batch 40, Loss: 0.030868008732795715\n",
      "Epoch 4, Batch 50, Loss: 0.039219461381435394\n",
      "Epoch 4, Batch 60, Loss: 0.032342832535505295\n",
      "Epoch 4, Batch 70, Loss: 0.05636608973145485\n",
      "Epoch 4, Batch 80, Loss: 0.02262265235185623\n",
      "Epoch 4, Batch 90, Loss: 0.04912639781832695\n",
      "Epoch 4, Batch 100, Loss: 0.03702310845255852\n",
      "Epoch 5, Batch 0, Loss: 0.04066671058535576\n",
      "Epoch 5, Batch 10, Loss: 0.035959288477897644\n",
      "Epoch 5, Batch 20, Loss: 0.030916091054677963\n",
      "Epoch 5, Batch 30, Loss: 0.03427601978182793\n",
      "Epoch 5, Batch 40, Loss: 0.03404880687594414\n",
      "Epoch 5, Batch 50, Loss: 0.03140219300985336\n",
      "Epoch 5, Batch 60, Loss: 0.05597894266247749\n",
      "Epoch 5, Batch 70, Loss: 0.039155371487140656\n",
      "Epoch 5, Batch 80, Loss: 0.030895262956619263\n",
      "Epoch 5, Batch 90, Loss: 0.03342229872941971\n",
      "Epoch 5, Batch 100, Loss: 0.024474557489156723\n",
      "Epoch 6, Batch 0, Loss: 0.02538927085697651\n",
      "Epoch 6, Batch 10, Loss: 0.03309009224176407\n",
      "Epoch 6, Batch 20, Loss: 0.03541318699717522\n",
      "Epoch 6, Batch 30, Loss: 0.03389132022857666\n",
      "Epoch 6, Batch 40, Loss: 0.034835413098335266\n",
      "Epoch 6, Batch 50, Loss: 0.03382231667637825\n",
      "Epoch 6, Batch 60, Loss: 0.029405565932393074\n",
      "Epoch 6, Batch 70, Loss: 0.03111216612160206\n",
      "Epoch 6, Batch 80, Loss: 0.03191271051764488\n",
      "Epoch 6, Batch 90, Loss: 0.04030440002679825\n",
      "Epoch 6, Batch 100, Loss: 0.028829533606767654\n",
      "Epoch 7, Batch 0, Loss: 0.03180749714374542\n",
      "Epoch 7, Batch 10, Loss: 0.02899797447025776\n",
      "Epoch 7, Batch 20, Loss: 0.0288276057690382\n",
      "Epoch 7, Batch 30, Loss: 0.022649630904197693\n",
      "Epoch 7, Batch 40, Loss: 0.025065170601010323\n",
      "Epoch 7, Batch 50, Loss: 0.032398879528045654\n",
      "Epoch 7, Batch 60, Loss: 0.031713999807834625\n",
      "Epoch 7, Batch 70, Loss: 0.031437743455171585\n",
      "Epoch 7, Batch 80, Loss: 0.0357837900519371\n",
      "Epoch 7, Batch 90, Loss: 0.02927009016275406\n",
      "Epoch 7, Batch 100, Loss: 0.021787162870168686\n",
      "Epoch 8, Batch 0, Loss: 0.03453729301691055\n",
      "Epoch 8, Batch 10, Loss: 0.026600534096360207\n",
      "Epoch 8, Batch 20, Loss: 0.04528379067778587\n",
      "Epoch 8, Batch 30, Loss: 0.026597723364830017\n",
      "Epoch 8, Batch 40, Loss: 0.034663546830415726\n",
      "Epoch 8, Batch 50, Loss: 0.027800653129816055\n",
      "Epoch 8, Batch 60, Loss: 0.02503983862698078\n",
      "Epoch 8, Batch 70, Loss: 0.03678935393691063\n",
      "Epoch 8, Batch 80, Loss: 0.035739440470933914\n",
      "Epoch 8, Batch 90, Loss: 0.028591670095920563\n",
      "Epoch 8, Batch 100, Loss: 0.03350329399108887\n",
      "Epoch 9, Batch 0, Loss: 0.025154300034046173\n",
      "Epoch 9, Batch 10, Loss: 0.03174898773431778\n",
      "Epoch 9, Batch 20, Loss: 0.02505432441830635\n",
      "Epoch 9, Batch 30, Loss: 0.03193262219429016\n",
      "Epoch 9, Batch 40, Loss: 0.0390416756272316\n",
      "Epoch 9, Batch 50, Loss: 0.02586890012025833\n",
      "Epoch 9, Batch 60, Loss: 0.024665163829922676\n",
      "Epoch 9, Batch 70, Loss: 0.03627021983265877\n",
      "Epoch 9, Batch 80, Loss: 0.034809328615665436\n",
      "Epoch 9, Batch 90, Loss: 0.034144796431064606\n",
      "Epoch 9, Batch 100, Loss: 0.02648061327636242\n",
      "Epoch 10, Batch 0, Loss: 0.0245764572173357\n",
      "Epoch 10, Batch 10, Loss: 0.02740337699651718\n",
      "Epoch 10, Batch 20, Loss: 0.027570955455303192\n",
      "Epoch 10, Batch 30, Loss: 0.0274889525026083\n",
      "Epoch 10, Batch 40, Loss: 0.03998168930411339\n",
      "Epoch 10, Batch 50, Loss: 0.028929639607667923\n",
      "Epoch 10, Batch 60, Loss: 0.03570334240794182\n",
      "Epoch 10, Batch 70, Loss: 0.024043520912528038\n",
      "Epoch 10, Batch 80, Loss: 0.03807539865374565\n",
      "Epoch 10, Batch 90, Loss: 0.029645351693034172\n",
      "Epoch 10, Batch 100, Loss: 0.022785812616348267\n",
      "Epoch 11, Batch 0, Loss: 0.022226151078939438\n",
      "Epoch 11, Batch 10, Loss: 0.023999910801649094\n",
      "Epoch 11, Batch 20, Loss: 0.03562355786561966\n",
      "Epoch 11, Batch 30, Loss: 0.047863662242889404\n",
      "Epoch 11, Batch 40, Loss: 0.02691149339079857\n",
      "Epoch 11, Batch 50, Loss: 0.032276399433612823\n",
      "Epoch 11, Batch 60, Loss: 0.029675230383872986\n",
      "Epoch 11, Batch 70, Loss: 0.028431331738829613\n",
      "Epoch 11, Batch 80, Loss: 0.03282395005226135\n",
      "Epoch 11, Batch 90, Loss: 0.028126651421189308\n",
      "Epoch 11, Batch 100, Loss: 0.033531688153743744\n",
      "Epoch 12, Batch 0, Loss: 0.021417783573269844\n",
      "Epoch 12, Batch 10, Loss: 0.03443116694688797\n",
      "Epoch 12, Batch 20, Loss: 0.0240846648812294\n",
      "Epoch 12, Batch 30, Loss: 0.025194603949785233\n",
      "Epoch 12, Batch 40, Loss: 0.02200094424188137\n",
      "Epoch 12, Batch 50, Loss: 0.024858007207512856\n",
      "Epoch 12, Batch 60, Loss: 0.02854696474969387\n",
      "Epoch 12, Batch 70, Loss: 0.025988858193159103\n",
      "Epoch 12, Batch 80, Loss: 0.02368338778614998\n",
      "Epoch 12, Batch 90, Loss: 0.02740050107240677\n",
      "Epoch 12, Batch 100, Loss: 0.028013985604047775\n",
      "Epoch 13, Batch 0, Loss: 0.028875410556793213\n",
      "Epoch 13, Batch 10, Loss: 0.027800481766462326\n",
      "Epoch 13, Batch 20, Loss: 0.01941826567053795\n",
      "Epoch 13, Batch 30, Loss: 0.023320810869336128\n",
      "Epoch 13, Batch 40, Loss: 0.0345696285367012\n",
      "Epoch 13, Batch 50, Loss: 0.03531194478273392\n",
      "Epoch 13, Batch 60, Loss: 0.023145200684666634\n",
      "Epoch 13, Batch 70, Loss: 0.028741760179400444\n",
      "Epoch 13, Batch 80, Loss: 0.033334292471408844\n",
      "Epoch 13, Batch 90, Loss: 0.02197358012199402\n",
      "Epoch 13, Batch 100, Loss: 0.02360743097960949\n",
      "Epoch 14, Batch 0, Loss: 0.03097056970000267\n",
      "Epoch 14, Batch 10, Loss: 0.03336396440863609\n",
      "Epoch 14, Batch 20, Loss: 0.027762504294514656\n",
      "Epoch 14, Batch 30, Loss: 0.039221808314323425\n",
      "Epoch 14, Batch 40, Loss: 0.026126442477107048\n",
      "Epoch 14, Batch 50, Loss: 0.024891532957553864\n",
      "Epoch 14, Batch 60, Loss: 0.02518211305141449\n",
      "Epoch 14, Batch 70, Loss: 0.03551250323653221\n",
      "Epoch 14, Batch 80, Loss: 0.0320788249373436\n",
      "Epoch 14, Batch 90, Loss: 0.03900568187236786\n",
      "Epoch 14, Batch 100, Loss: 0.022745149210095406\n",
      "Epoch 15, Batch 0, Loss: 0.025840137153863907\n",
      "Epoch 15, Batch 10, Loss: 0.025354817509651184\n",
      "Epoch 15, Batch 20, Loss: 0.035129301249980927\n",
      "Epoch 15, Batch 30, Loss: 0.02573096752166748\n",
      "Epoch 15, Batch 40, Loss: 0.03389303386211395\n",
      "Epoch 15, Batch 50, Loss: 0.02147345058619976\n",
      "Epoch 15, Batch 60, Loss: 0.02845095470547676\n",
      "Epoch 15, Batch 70, Loss: 0.023304952308535576\n",
      "Epoch 15, Batch 80, Loss: 0.030470671132206917\n",
      "Epoch 15, Batch 90, Loss: 0.0348316989839077\n",
      "Epoch 15, Batch 100, Loss: 0.030809879302978516\n",
      "Epoch 16, Batch 0, Loss: 0.039717897772789\n",
      "Epoch 16, Batch 10, Loss: 0.019395116716623306\n",
      "Epoch 16, Batch 20, Loss: 0.029685260728001595\n",
      "Epoch 16, Batch 30, Loss: 0.03634537011384964\n",
      "Epoch 16, Batch 40, Loss: 0.02594086527824402\n",
      "Epoch 16, Batch 50, Loss: 0.025431325659155846\n",
      "Epoch 16, Batch 60, Loss: 0.02921188250184059\n",
      "Epoch 16, Batch 70, Loss: 0.02552497759461403\n",
      "Epoch 16, Batch 80, Loss: 0.031195109710097313\n",
      "Epoch 16, Batch 90, Loss: 0.028750181198120117\n",
      "Epoch 16, Batch 100, Loss: 0.02758045867085457\n",
      "Epoch 17, Batch 0, Loss: 0.02656659483909607\n",
      "Epoch 17, Batch 10, Loss: 0.031363215297460556\n",
      "Epoch 17, Batch 20, Loss: 0.021698694676160812\n",
      "Epoch 17, Batch 30, Loss: 0.027838725596666336\n",
      "Epoch 17, Batch 40, Loss: 0.030612189322710037\n",
      "Epoch 17, Batch 50, Loss: 0.02863798849284649\n",
      "Epoch 17, Batch 60, Loss: 0.026713725179433823\n",
      "Epoch 17, Batch 70, Loss: 0.023211542516946793\n",
      "Epoch 17, Batch 80, Loss: 0.025745239108800888\n",
      "Epoch 17, Batch 90, Loss: 0.0304594486951828\n",
      "Epoch 17, Batch 100, Loss: 0.022585373371839523\n",
      "Epoch 18, Batch 0, Loss: 0.02132755145430565\n",
      "Epoch 18, Batch 10, Loss: 0.027179574593901634\n",
      "Epoch 18, Batch 20, Loss: 0.019349241629242897\n",
      "Epoch 18, Batch 30, Loss: 0.03261275216937065\n",
      "Epoch 18, Batch 40, Loss: 0.02855966053903103\n",
      "Epoch 18, Batch 50, Loss: 0.019458413124084473\n",
      "Epoch 18, Batch 60, Loss: 0.02587830275297165\n",
      "Epoch 18, Batch 70, Loss: 0.022939201444387436\n",
      "Epoch 18, Batch 80, Loss: 0.028345413506031036\n",
      "Epoch 18, Batch 90, Loss: 0.023879963904619217\n",
      "Epoch 18, Batch 100, Loss: 0.03210914134979248\n",
      "Epoch 19, Batch 0, Loss: 0.029780996963381767\n",
      "Epoch 19, Batch 10, Loss: 0.025415681302547455\n",
      "Epoch 19, Batch 20, Loss: 0.01798040047287941\n",
      "Epoch 19, Batch 30, Loss: 0.029457977041602135\n",
      "Epoch 19, Batch 40, Loss: 0.021731387823820114\n",
      "Epoch 19, Batch 50, Loss: 0.025573235005140305\n",
      "Epoch 19, Batch 60, Loss: 0.026588594540953636\n",
      "Epoch 19, Batch 70, Loss: 0.025239862501621246\n",
      "Epoch 19, Batch 80, Loss: 0.0256681926548481\n",
      "Epoch 19, Batch 90, Loss: 0.02516370639204979\n",
      "Epoch 19, Batch 100, Loss: 0.031320005655288696\n",
      "Epoch 20, Batch 0, Loss: 0.023264721035957336\n",
      "Epoch 20, Batch 10, Loss: 0.028485454618930817\n",
      "Epoch 20, Batch 20, Loss: 0.02897767536342144\n",
      "Epoch 20, Batch 30, Loss: 0.031215673312544823\n",
      "Epoch 20, Batch 40, Loss: 0.027161233127117157\n",
      "Epoch 20, Batch 50, Loss: 0.03169748932123184\n",
      "Epoch 20, Batch 60, Loss: 0.02709546685218811\n",
      "Epoch 20, Batch 70, Loss: 0.03285551071166992\n",
      "Epoch 20, Batch 80, Loss: 0.02179103158414364\n",
      "Epoch 20, Batch 90, Loss: 0.03522544726729393\n",
      "Epoch 20, Batch 100, Loss: 0.025256920605897903\n",
      "Epoch 21, Batch 0, Loss: 0.027251949533820152\n",
      "Epoch 21, Batch 10, Loss: 0.021438684314489365\n",
      "Epoch 21, Batch 20, Loss: 0.021138101816177368\n",
      "Epoch 21, Batch 30, Loss: 0.029872246086597443\n",
      "Epoch 21, Batch 40, Loss: 0.02697266824543476\n",
      "Epoch 21, Batch 50, Loss: 0.02294524945318699\n",
      "Epoch 21, Batch 60, Loss: 0.019584134221076965\n",
      "Epoch 21, Batch 70, Loss: 0.02765277773141861\n",
      "Epoch 21, Batch 80, Loss: 0.025702137500047684\n",
      "Epoch 21, Batch 90, Loss: 0.024879397824406624\n",
      "Epoch 21, Batch 100, Loss: 0.0298248790204525\n",
      "Epoch 22, Batch 0, Loss: 0.021409405395388603\n",
      "Epoch 22, Batch 10, Loss: 0.01937226392328739\n",
      "Epoch 22, Batch 20, Loss: 0.02897653728723526\n",
      "Epoch 22, Batch 30, Loss: 0.03095901943743229\n",
      "Epoch 22, Batch 40, Loss: 0.023478001356124878\n",
      "Epoch 22, Batch 50, Loss: 0.01859334297478199\n",
      "Epoch 22, Batch 60, Loss: 0.03154211491346359\n",
      "Epoch 22, Batch 70, Loss: 0.01684444025158882\n",
      "Epoch 22, Batch 80, Loss: 0.0383288599550724\n",
      "Epoch 22, Batch 90, Loss: 0.029552295804023743\n",
      "Epoch 22, Batch 100, Loss: 0.022969845682382584\n",
      "Epoch 23, Batch 0, Loss: 0.022363029420375824\n",
      "Epoch 23, Batch 10, Loss: 0.01854819804430008\n",
      "Epoch 23, Batch 20, Loss: 0.024658044800162315\n",
      "Epoch 23, Batch 30, Loss: 0.03797576576471329\n",
      "Epoch 23, Batch 40, Loss: 0.02702144719660282\n",
      "Epoch 23, Batch 50, Loss: 0.026202144101262093\n",
      "Epoch 23, Batch 60, Loss: 0.024209629744291306\n",
      "Epoch 23, Batch 70, Loss: 0.0221098605543375\n",
      "Epoch 23, Batch 80, Loss: 0.04068707674741745\n",
      "Epoch 23, Batch 90, Loss: 0.02964974381029606\n",
      "Epoch 23, Batch 100, Loss: 0.025199247524142265\n",
      "Epoch 24, Batch 0, Loss: 0.02520408295094967\n",
      "Epoch 24, Batch 10, Loss: 0.01776614412665367\n",
      "Epoch 24, Batch 20, Loss: 0.028450816869735718\n",
      "Epoch 24, Batch 30, Loss: 0.0259762741625309\n",
      "Epoch 24, Batch 40, Loss: 0.02440049685537815\n",
      "Epoch 24, Batch 50, Loss: 0.020405113697052002\n",
      "Epoch 24, Batch 60, Loss: 0.026999592781066895\n",
      "Epoch 24, Batch 70, Loss: 0.02394070290029049\n",
      "Epoch 24, Batch 80, Loss: 0.029657214879989624\n",
      "Epoch 24, Batch 90, Loss: 0.020118465647101402\n",
      "Epoch 24, Batch 100, Loss: 0.026764675974845886\n",
      "Epoch 25, Batch 0, Loss: 0.021570220589637756\n",
      "Epoch 25, Batch 10, Loss: 0.020146872848272324\n",
      "Epoch 25, Batch 20, Loss: 0.016391785815358162\n",
      "Epoch 25, Batch 30, Loss: 0.026346329599618912\n",
      "Epoch 25, Batch 40, Loss: 0.020521635189652443\n",
      "Epoch 25, Batch 50, Loss: 0.033806025981903076\n",
      "Epoch 25, Batch 60, Loss: 0.020881110802292824\n",
      "Epoch 25, Batch 70, Loss: 0.02285350300371647\n",
      "Epoch 25, Batch 80, Loss: 0.02403399348258972\n",
      "Epoch 25, Batch 90, Loss: 0.037192024290561676\n",
      "Epoch 25, Batch 100, Loss: 0.024970756843686104\n",
      "Epoch 26, Batch 0, Loss: 0.024901654571294785\n",
      "Epoch 26, Batch 10, Loss: 0.029498349875211716\n",
      "Epoch 26, Batch 20, Loss: 0.023027658462524414\n",
      "Epoch 26, Batch 30, Loss: 0.01634339801967144\n",
      "Epoch 26, Batch 40, Loss: 0.018699325621128082\n",
      "Epoch 26, Batch 50, Loss: 0.02775096707046032\n",
      "Epoch 26, Batch 60, Loss: 0.021490486338734627\n",
      "Epoch 26, Batch 70, Loss: 0.03159809485077858\n",
      "Epoch 26, Batch 80, Loss: 0.024482984095811844\n",
      "Epoch 26, Batch 90, Loss: 0.029909541830420494\n",
      "Epoch 26, Batch 100, Loss: 0.03151543065905571\n",
      "Epoch 27, Batch 0, Loss: 0.02066059596836567\n",
      "Epoch 27, Batch 10, Loss: 0.019452601671218872\n",
      "Epoch 27, Batch 20, Loss: 0.027080196887254715\n",
      "Epoch 27, Batch 30, Loss: 0.021903175860643387\n",
      "Epoch 27, Batch 40, Loss: 0.0324971005320549\n",
      "Epoch 27, Batch 50, Loss: 0.030147215351462364\n",
      "Epoch 27, Batch 60, Loss: 0.022075075656175613\n",
      "Epoch 27, Batch 70, Loss: 0.020151175558567047\n",
      "Epoch 27, Batch 80, Loss: 0.031486865133047104\n",
      "Epoch 27, Batch 90, Loss: 0.026257097721099854\n",
      "Epoch 27, Batch 100, Loss: 0.022206854075193405\n",
      "Epoch 28, Batch 0, Loss: 0.0325680747628212\n",
      "Epoch 28, Batch 10, Loss: 0.027684958651661873\n",
      "Epoch 28, Batch 20, Loss: 0.02536740154027939\n",
      "Epoch 28, Batch 30, Loss: 0.02449692226946354\n",
      "Epoch 28, Batch 40, Loss: 0.024522310122847557\n",
      "Epoch 28, Batch 50, Loss: 0.028468916192650795\n",
      "Epoch 28, Batch 60, Loss: 0.03008103184401989\n",
      "Epoch 28, Batch 70, Loss: 0.022379634901881218\n",
      "Epoch 28, Batch 80, Loss: 0.018422609195113182\n",
      "Epoch 28, Batch 90, Loss: 0.02312014438211918\n",
      "Epoch 28, Batch 100, Loss: 0.02466621622443199\n",
      "Epoch 29, Batch 0, Loss: 0.023828832432627678\n",
      "Epoch 29, Batch 10, Loss: 0.018830427899956703\n",
      "Epoch 29, Batch 20, Loss: 0.029724333435297012\n",
      "Epoch 29, Batch 30, Loss: 0.0204287339001894\n",
      "Epoch 29, Batch 40, Loss: 0.023813070729374886\n",
      "Epoch 29, Batch 50, Loss: 0.031621042639017105\n",
      "Epoch 29, Batch 60, Loss: 0.025404050946235657\n",
      "Epoch 29, Batch 70, Loss: 0.03291750326752663\n",
      "Epoch 29, Batch 80, Loss: 0.034930452704429626\n",
      "Epoch 29, Batch 90, Loss: 0.02823009341955185\n",
      "Epoch 29, Batch 100, Loss: 0.0285823754966259\n",
      "Epoch 30, Batch 0, Loss: 0.026745371520519257\n",
      "Epoch 30, Batch 10, Loss: 0.024910278618335724\n",
      "Epoch 30, Batch 20, Loss: 0.027158614248037338\n",
      "Epoch 30, Batch 30, Loss: 0.01775023713707924\n",
      "Epoch 30, Batch 40, Loss: 0.016604920849204063\n",
      "Epoch 30, Batch 50, Loss: 0.02013625204563141\n",
      "Epoch 30, Batch 60, Loss: 0.018771739676594734\n",
      "Epoch 30, Batch 70, Loss: 0.023837052285671234\n",
      "Epoch 30, Batch 80, Loss: 0.03109114244580269\n",
      "Epoch 30, Batch 90, Loss: 0.020046591758728027\n",
      "Epoch 30, Batch 100, Loss: 0.02200418896973133\n",
      "Epoch 31, Batch 0, Loss: 0.0191566850990057\n",
      "Epoch 31, Batch 10, Loss: 0.022616740316152573\n",
      "Epoch 31, Batch 20, Loss: 0.01912572793662548\n",
      "Epoch 31, Batch 30, Loss: 0.02144896797835827\n",
      "Epoch 31, Batch 40, Loss: 0.02379489503800869\n",
      "Epoch 31, Batch 50, Loss: 0.030259674414992332\n",
      "Epoch 31, Batch 60, Loss: 0.02742158994078636\n",
      "Epoch 31, Batch 70, Loss: 0.026761576533317566\n",
      "Epoch 31, Batch 80, Loss: 0.03143613040447235\n",
      "Epoch 31, Batch 90, Loss: 0.02589457668364048\n",
      "Epoch 31, Batch 100, Loss: 0.023812899366021156\n",
      "Epoch 32, Batch 0, Loss: 0.026567265391349792\n",
      "Epoch 32, Batch 10, Loss: 0.02816157601773739\n",
      "Epoch 32, Batch 20, Loss: 0.019292887300252914\n",
      "Epoch 32, Batch 30, Loss: 0.021948734298348427\n",
      "Epoch 32, Batch 40, Loss: 0.021368185058236122\n",
      "Epoch 32, Batch 50, Loss: 0.022405225783586502\n",
      "Epoch 32, Batch 60, Loss: 0.022083483636379242\n",
      "Epoch 32, Batch 70, Loss: 0.022310294210910797\n",
      "Epoch 32, Batch 80, Loss: 0.02567143924534321\n",
      "Epoch 32, Batch 90, Loss: 0.025330474600195885\n",
      "Epoch 32, Batch 100, Loss: 0.018645508214831352\n",
      "Epoch 33, Batch 0, Loss: 0.02923542447388172\n",
      "Epoch 33, Batch 10, Loss: 0.024188287556171417\n",
      "Epoch 33, Batch 20, Loss: 0.032097235321998596\n",
      "Epoch 33, Batch 30, Loss: 0.021905498579144478\n",
      "Epoch 33, Batch 40, Loss: 0.017325446009635925\n",
      "Epoch 33, Batch 50, Loss: 0.025287708267569542\n",
      "Epoch 33, Batch 60, Loss: 0.018029842525720596\n",
      "Epoch 33, Batch 70, Loss: 0.02114834077656269\n",
      "Epoch 33, Batch 80, Loss: 0.01889774389564991\n",
      "Epoch 33, Batch 90, Loss: 0.03432340547442436\n",
      "Epoch 33, Batch 100, Loss: 0.01689385436475277\n",
      "Epoch 34, Batch 0, Loss: 0.024305310100317\n",
      "Epoch 34, Batch 10, Loss: 0.0304931178689003\n",
      "Epoch 34, Batch 20, Loss: 0.02464769221842289\n",
      "Epoch 34, Batch 30, Loss: 0.023174025118350983\n",
      "Epoch 34, Batch 40, Loss: 0.024515828117728233\n",
      "Epoch 34, Batch 50, Loss: 0.018019573763012886\n",
      "Epoch 34, Batch 60, Loss: 0.035704389214515686\n",
      "Epoch 34, Batch 70, Loss: 0.030155258253216743\n",
      "Epoch 34, Batch 80, Loss: 0.019083712249994278\n",
      "Epoch 34, Batch 90, Loss: 0.025742843747138977\n",
      "Epoch 34, Batch 100, Loss: 0.022848905995488167\n",
      "Epoch 35, Batch 0, Loss: 0.024376079440116882\n",
      "Epoch 35, Batch 10, Loss: 0.024198509752750397\n",
      "Epoch 35, Batch 20, Loss: 0.025748638436198235\n",
      "Epoch 35, Batch 30, Loss: 0.020139038562774658\n",
      "Epoch 35, Batch 40, Loss: 0.02339133433997631\n",
      "Epoch 35, Batch 50, Loss: 0.031954213976860046\n",
      "Epoch 35, Batch 60, Loss: 0.022025147452950478\n",
      "Epoch 35, Batch 70, Loss: 0.024224333465099335\n",
      "Epoch 35, Batch 80, Loss: 0.020824871957302094\n",
      "Epoch 35, Batch 90, Loss: 0.025123445317149162\n",
      "Epoch 35, Batch 100, Loss: 0.02853737212717533\n",
      "Epoch 36, Batch 0, Loss: 0.023754319176077843\n",
      "Epoch 36, Batch 10, Loss: 0.021177208051085472\n",
      "Epoch 36, Batch 20, Loss: 0.019432198256254196\n",
      "Epoch 36, Batch 30, Loss: 0.023446913808584213\n",
      "Epoch 36, Batch 40, Loss: 0.023554012179374695\n",
      "Epoch 36, Batch 50, Loss: 0.01551127154380083\n",
      "Epoch 36, Batch 60, Loss: 0.03960878401994705\n",
      "Epoch 36, Batch 70, Loss: 0.023352278396487236\n",
      "Epoch 36, Batch 80, Loss: 0.019123097881674767\n",
      "Epoch 36, Batch 90, Loss: 0.02678222768008709\n",
      "Epoch 36, Batch 100, Loss: 0.02266663871705532\n",
      "Epoch 37, Batch 0, Loss: 0.030116360634565353\n",
      "Epoch 37, Batch 10, Loss: 0.018403619527816772\n",
      "Epoch 37, Batch 20, Loss: 0.030894795432686806\n",
      "Epoch 37, Batch 30, Loss: 0.021167607977986336\n",
      "Epoch 37, Batch 40, Loss: 0.02402360737323761\n",
      "Epoch 37, Batch 50, Loss: 0.031882211565971375\n",
      "Epoch 37, Batch 60, Loss: 0.02035810612142086\n",
      "Epoch 37, Batch 70, Loss: 0.020689358934760094\n",
      "Epoch 37, Batch 80, Loss: 0.023303598165512085\n",
      "Epoch 37, Batch 90, Loss: 0.029188761487603188\n",
      "Epoch 37, Batch 100, Loss: 0.029786158353090286\n",
      "Epoch 38, Batch 0, Loss: 0.023477638140320778\n",
      "Epoch 38, Batch 10, Loss: 0.022085636854171753\n",
      "Epoch 38, Batch 20, Loss: 0.019859755411744118\n",
      "Epoch 38, Batch 30, Loss: 0.024551577866077423\n",
      "Epoch 38, Batch 40, Loss: 0.027277423068881035\n",
      "Epoch 38, Batch 50, Loss: 0.02069186232984066\n",
      "Epoch 38, Batch 60, Loss: 0.024381490424275398\n",
      "Epoch 38, Batch 70, Loss: 0.028457028791308403\n",
      "Epoch 38, Batch 80, Loss: 0.029555652290582657\n",
      "Epoch 38, Batch 90, Loss: 0.02508755587041378\n",
      "Epoch 38, Batch 100, Loss: 0.024023203179240227\n",
      "Epoch 39, Batch 0, Loss: 0.026348615065217018\n",
      "Epoch 39, Batch 10, Loss: 0.022128963842988014\n",
      "Epoch 39, Batch 20, Loss: 0.022394921630620956\n",
      "Epoch 39, Batch 30, Loss: 0.01795983500778675\n",
      "Epoch 39, Batch 40, Loss: 0.023824483156204224\n",
      "Epoch 39, Batch 50, Loss: 0.02734266221523285\n",
      "Epoch 39, Batch 60, Loss: 0.023433787748217583\n",
      "Epoch 39, Batch 70, Loss: 0.019874950870871544\n",
      "Epoch 39, Batch 80, Loss: 0.02407011017203331\n",
      "Epoch 39, Batch 90, Loss: 0.018796639516949654\n",
      "Epoch 39, Batch 100, Loss: 0.02156802825629711\n",
      "Epoch 40, Batch 0, Loss: 0.02840590663254261\n",
      "Epoch 40, Batch 10, Loss: 0.03777782991528511\n",
      "Epoch 40, Batch 20, Loss: 0.023369349539279938\n",
      "Epoch 40, Batch 30, Loss: 0.02336965873837471\n",
      "Epoch 40, Batch 40, Loss: 0.027934137731790543\n",
      "Epoch 40, Batch 50, Loss: 0.023471403867006302\n",
      "Epoch 40, Batch 60, Loss: 0.01803671568632126\n",
      "Epoch 40, Batch 70, Loss: 0.022245684638619423\n",
      "Epoch 40, Batch 80, Loss: 0.020419521257281303\n",
      "Epoch 40, Batch 90, Loss: 0.017007287591695786\n",
      "Epoch 40, Batch 100, Loss: 0.032978400588035583\n",
      "Epoch 41, Batch 0, Loss: 0.01794317364692688\n",
      "Epoch 41, Batch 10, Loss: 0.027889486402273178\n",
      "Epoch 41, Batch 20, Loss: 0.031308237463235855\n",
      "Epoch 41, Batch 30, Loss: 0.025902502238750458\n",
      "Epoch 41, Batch 40, Loss: 0.01901630125939846\n",
      "Epoch 41, Batch 50, Loss: 0.02433887869119644\n",
      "Epoch 41, Batch 60, Loss: 0.023948580026626587\n",
      "Epoch 41, Batch 70, Loss: 0.02308403141796589\n",
      "Epoch 41, Batch 80, Loss: 0.03114897571504116\n",
      "Epoch 41, Batch 90, Loss: 0.02172171138226986\n",
      "Epoch 41, Batch 100, Loss: 0.02044852450489998\n",
      "Epoch 42, Batch 0, Loss: 0.02093787118792534\n",
      "Epoch 42, Batch 10, Loss: 0.02237829566001892\n",
      "Epoch 42, Batch 20, Loss: 0.025011954829096794\n",
      "Epoch 42, Batch 30, Loss: 0.029398556798696518\n",
      "Epoch 42, Batch 40, Loss: 0.017101380974054337\n",
      "Epoch 42, Batch 50, Loss: 0.021563218906521797\n",
      "Epoch 42, Batch 60, Loss: 0.024390269070863724\n",
      "Epoch 42, Batch 70, Loss: 0.029889319092035294\n",
      "Epoch 42, Batch 80, Loss: 0.01831774227321148\n",
      "Epoch 42, Batch 90, Loss: 0.03445364162325859\n",
      "Epoch 42, Batch 100, Loss: 0.021786436438560486\n",
      "Epoch 43, Batch 0, Loss: 0.019869554787874222\n",
      "Epoch 43, Batch 10, Loss: 0.027132228016853333\n",
      "Epoch 43, Batch 20, Loss: 0.028048589825630188\n",
      "Epoch 43, Batch 30, Loss: 0.02333054132759571\n",
      "Epoch 43, Batch 40, Loss: 0.02025667577981949\n",
      "Epoch 43, Batch 50, Loss: 0.03451601415872574\n",
      "Epoch 43, Batch 60, Loss: 0.03178507462143898\n",
      "Epoch 43, Batch 70, Loss: 0.02569824643433094\n",
      "Epoch 43, Batch 80, Loss: 0.026399465277791023\n",
      "Epoch 43, Batch 90, Loss: 0.027333607897162437\n",
      "Epoch 43, Batch 100, Loss: 0.02713662199676037\n",
      "Epoch 44, Batch 0, Loss: 0.021955588832497597\n",
      "Epoch 44, Batch 10, Loss: 0.03465599566698074\n",
      "Epoch 44, Batch 20, Loss: 0.02901356853544712\n",
      "Epoch 44, Batch 30, Loss: 0.028339611366391182\n",
      "Epoch 44, Batch 40, Loss: 0.02614694833755493\n",
      "Epoch 44, Batch 50, Loss: 0.02603520080447197\n",
      "Epoch 44, Batch 60, Loss: 0.02239270880818367\n",
      "Epoch 44, Batch 70, Loss: 0.029540708288550377\n",
      "Epoch 44, Batch 80, Loss: 0.025850269943475723\n",
      "Epoch 44, Batch 90, Loss: 0.03161362558603287\n",
      "Epoch 44, Batch 100, Loss: 0.020381275564432144\n",
      "Epoch 45, Batch 0, Loss: 0.027113383635878563\n",
      "Epoch 45, Batch 10, Loss: 0.02610834129154682\n",
      "Epoch 45, Batch 20, Loss: 0.023166920989751816\n",
      "Epoch 45, Batch 30, Loss: 0.025307688862085342\n",
      "Epoch 45, Batch 40, Loss: 0.025126580148935318\n",
      "Epoch 45, Batch 50, Loss: 0.01996038481593132\n",
      "Epoch 45, Batch 60, Loss: 0.022892244160175323\n",
      "Epoch 45, Batch 70, Loss: 0.02241680398583412\n",
      "Epoch 45, Batch 80, Loss: 0.027076933532953262\n",
      "Epoch 45, Batch 90, Loss: 0.02919899672269821\n",
      "Epoch 45, Batch 100, Loss: 0.032985832542181015\n",
      "Epoch 46, Batch 0, Loss: 0.018714768812060356\n",
      "Epoch 46, Batch 10, Loss: 0.02039005234837532\n",
      "Epoch 46, Batch 20, Loss: 0.022940251976251602\n",
      "Epoch 46, Batch 30, Loss: 0.021261505782604218\n",
      "Epoch 46, Batch 40, Loss: 0.02008185163140297\n",
      "Epoch 46, Batch 50, Loss: 0.021741235628724098\n",
      "Epoch 46, Batch 60, Loss: 0.026431528851389885\n",
      "Epoch 46, Batch 70, Loss: 0.021819084882736206\n",
      "Epoch 46, Batch 80, Loss: 0.02040090411901474\n",
      "Epoch 46, Batch 90, Loss: 0.022048231214284897\n",
      "Epoch 46, Batch 100, Loss: 0.02376878447830677\n",
      "Epoch 47, Batch 0, Loss: 0.01812673732638359\n",
      "Epoch 47, Batch 10, Loss: 0.025399893522262573\n",
      "Epoch 47, Batch 20, Loss: 0.022090893238782883\n",
      "Epoch 47, Batch 30, Loss: 0.03167979046702385\n",
      "Epoch 47, Batch 40, Loss: 0.02454506605863571\n",
      "Epoch 47, Batch 50, Loss: 0.023196088150143623\n",
      "Epoch 47, Batch 60, Loss: 0.028384508565068245\n",
      "Epoch 47, Batch 70, Loss: 0.025431759655475616\n",
      "Epoch 47, Batch 80, Loss: 0.03073865734040737\n",
      "Epoch 47, Batch 90, Loss: 0.02200062945485115\n",
      "Epoch 47, Batch 100, Loss: 0.025517569854855537\n",
      "Epoch 48, Batch 0, Loss: 0.029275037348270416\n",
      "Epoch 48, Batch 10, Loss: 0.021055612713098526\n",
      "Epoch 48, Batch 20, Loss: 0.02709113247692585\n",
      "Epoch 48, Batch 30, Loss: 0.026609936729073524\n",
      "Epoch 48, Batch 40, Loss: 0.0228238794952631\n",
      "Epoch 48, Batch 50, Loss: 0.02241990715265274\n",
      "Epoch 48, Batch 60, Loss: 0.024392662569880486\n",
      "Epoch 48, Batch 70, Loss: 0.025174632668495178\n",
      "Epoch 48, Batch 80, Loss: 0.023190250620245934\n",
      "Epoch 48, Batch 90, Loss: 0.02155853994190693\n",
      "Epoch 48, Batch 100, Loss: 0.02348790504038334\n",
      "Epoch 49, Batch 0, Loss: 0.02790079265832901\n",
      "Epoch 49, Batch 10, Loss: 0.030201949179172516\n",
      "Epoch 49, Batch 20, Loss: 0.023142799735069275\n",
      "Epoch 49, Batch 30, Loss: 0.02038729563355446\n",
      "Epoch 49, Batch 40, Loss: 0.023322079330682755\n",
      "Epoch 49, Batch 50, Loss: 0.022761914879083633\n",
      "Epoch 49, Batch 60, Loss: 0.029207779094576836\n",
      "Epoch 49, Batch 70, Loss: 0.026059355586767197\n",
      "Epoch 49, Batch 80, Loss: 0.01916259154677391\n",
      "Epoch 49, Batch 90, Loss: 0.01842506416141987\n",
      "Epoch 49, Batch 100, Loss: 0.02472032606601715\n",
      "Epoch 50, Batch 0, Loss: 0.02281046472489834\n",
      "Epoch 50, Batch 10, Loss: 0.028729723766446114\n",
      "Epoch 50, Batch 20, Loss: 0.031931180506944656\n",
      "Epoch 50, Batch 30, Loss: 0.021343756467103958\n",
      "Epoch 50, Batch 40, Loss: 0.02373911812901497\n",
      "Epoch 50, Batch 50, Loss: 0.017553037032485008\n",
      "Epoch 50, Batch 60, Loss: 0.03051818534731865\n",
      "Epoch 50, Batch 70, Loss: 0.02308838628232479\n",
      "Epoch 50, Batch 80, Loss: 0.024653274565935135\n",
      "Epoch 50, Batch 90, Loss: 0.022840287536382675\n",
      "Epoch 50, Batch 100, Loss: 0.02761218324303627\n",
      "Epoch 51, Batch 0, Loss: 0.024629779160022736\n",
      "Epoch 51, Batch 10, Loss: 0.028404761105775833\n",
      "Epoch 51, Batch 20, Loss: 0.023501744493842125\n",
      "Epoch 51, Batch 30, Loss: 0.028042994439601898\n",
      "Epoch 51, Batch 40, Loss: 0.027421781793236732\n",
      "Epoch 51, Batch 50, Loss: 0.02698482945561409\n",
      "Epoch 51, Batch 60, Loss: 0.021466702222824097\n",
      "Epoch 51, Batch 70, Loss: 0.03258225694298744\n",
      "Epoch 51, Batch 80, Loss: 0.021802231669425964\n",
      "Epoch 51, Batch 90, Loss: 0.027715470641851425\n",
      "Epoch 51, Batch 100, Loss: 0.021925542503595352\n",
      "Epoch 52, Batch 0, Loss: 0.023120557889342308\n",
      "Epoch 52, Batch 10, Loss: 0.023086510598659515\n",
      "Epoch 52, Batch 20, Loss: 0.027453921735286713\n",
      "Epoch 52, Batch 30, Loss: 0.023808840662240982\n",
      "Epoch 52, Batch 40, Loss: 0.02152552641928196\n",
      "Epoch 52, Batch 50, Loss: 0.01658458262681961\n",
      "Epoch 52, Batch 60, Loss: 0.027693424373865128\n",
      "Epoch 52, Batch 70, Loss: 0.021720830351114273\n",
      "Epoch 52, Batch 80, Loss: 0.025939596816897392\n",
      "Epoch 52, Batch 90, Loss: 0.0301376860588789\n",
      "Epoch 52, Batch 100, Loss: 0.02679543010890484\n",
      "Epoch 53, Batch 0, Loss: 0.02042226679623127\n",
      "Epoch 53, Batch 10, Loss: 0.021799933165311813\n",
      "Epoch 53, Batch 20, Loss: 0.023074891418218613\n",
      "Epoch 53, Batch 30, Loss: 0.02048151008784771\n",
      "Epoch 53, Batch 40, Loss: 0.017591172829270363\n",
      "Epoch 53, Batch 50, Loss: 0.02889143116772175\n",
      "Epoch 53, Batch 60, Loss: 0.022608021274209023\n",
      "Epoch 53, Batch 70, Loss: 0.03480508551001549\n",
      "Epoch 53, Batch 80, Loss: 0.026610977947711945\n",
      "Epoch 53, Batch 90, Loss: 0.025952905416488647\n",
      "Epoch 53, Batch 100, Loss: 0.02822084166109562\n",
      "Epoch 54, Batch 0, Loss: 0.021277472376823425\n",
      "Epoch 54, Batch 10, Loss: 0.027386292815208435\n",
      "Epoch 54, Batch 20, Loss: 0.01636509783565998\n",
      "Epoch 54, Batch 30, Loss: 0.016805358231067657\n",
      "Epoch 54, Batch 40, Loss: 0.019124003127217293\n",
      "Epoch 54, Batch 50, Loss: 0.029458897188305855\n",
      "Epoch 54, Batch 60, Loss: 0.026529327034950256\n",
      "Epoch 54, Batch 70, Loss: 0.023460976779460907\n",
      "Epoch 54, Batch 80, Loss: 0.024779431521892548\n",
      "Epoch 54, Batch 90, Loss: 0.02042398974299431\n",
      "Epoch 54, Batch 100, Loss: 0.023666037246584892\n",
      "Epoch 55, Batch 0, Loss: 0.030801238492131233\n",
      "Epoch 55, Batch 10, Loss: 0.02786956913769245\n",
      "Epoch 55, Batch 20, Loss: 0.020591212436556816\n",
      "Epoch 55, Batch 30, Loss: 0.025249455124139786\n",
      "Epoch 55, Batch 40, Loss: 0.02148563787341118\n",
      "Epoch 55, Batch 50, Loss: 0.028921380639076233\n",
      "Epoch 55, Batch 60, Loss: 0.018488125875592232\n",
      "Epoch 55, Batch 70, Loss: 0.020771106705069542\n",
      "Epoch 55, Batch 80, Loss: 0.03248635679483414\n",
      "Epoch 55, Batch 90, Loss: 0.020957259461283684\n",
      "Epoch 55, Batch 100, Loss: 0.01985008642077446\n",
      "Epoch 56, Batch 0, Loss: 0.02599823847413063\n",
      "Epoch 56, Batch 10, Loss: 0.020960383117198944\n",
      "Epoch 56, Batch 20, Loss: 0.027006106451153755\n",
      "Epoch 56, Batch 30, Loss: 0.028073836117982864\n",
      "Epoch 56, Batch 40, Loss: 0.01995757780969143\n",
      "Epoch 56, Batch 50, Loss: 0.021259315311908722\n",
      "Epoch 56, Batch 60, Loss: 0.019689291715621948\n",
      "Epoch 56, Batch 70, Loss: 0.021551400423049927\n",
      "Epoch 56, Batch 80, Loss: 0.02332531288266182\n",
      "Epoch 56, Batch 90, Loss: 0.0253780297935009\n",
      "Epoch 56, Batch 100, Loss: 0.023372653871774673\n",
      "Epoch 57, Batch 0, Loss: 0.0281643345952034\n",
      "Epoch 57, Batch 10, Loss: 0.020789530128240585\n",
      "Epoch 57, Batch 20, Loss: 0.021996716037392616\n",
      "Epoch 57, Batch 30, Loss: 0.025496220216155052\n",
      "Epoch 57, Batch 40, Loss: 0.0199043620377779\n",
      "Epoch 57, Batch 50, Loss: 0.020937953144311905\n",
      "Epoch 57, Batch 60, Loss: 0.022525984793901443\n",
      "Epoch 57, Batch 70, Loss: 0.026821954175829887\n",
      "Epoch 57, Batch 80, Loss: 0.019992098212242126\n",
      "Epoch 57, Batch 90, Loss: 0.030728546902537346\n",
      "Epoch 57, Batch 100, Loss: 0.020634382963180542\n",
      "Epoch 58, Batch 0, Loss: 0.028225991874933243\n",
      "Epoch 58, Batch 10, Loss: 0.024862270802259445\n",
      "Epoch 58, Batch 20, Loss: 0.02427240088582039\n",
      "Epoch 58, Batch 30, Loss: 0.031966764479875565\n",
      "Epoch 58, Batch 40, Loss: 0.02735755778849125\n",
      "Epoch 58, Batch 50, Loss: 0.0230697188526392\n",
      "Epoch 58, Batch 60, Loss: 0.017732344567775726\n",
      "Epoch 58, Batch 70, Loss: 0.024540606886148453\n",
      "Epoch 58, Batch 80, Loss: 0.027081117033958435\n",
      "Epoch 58, Batch 90, Loss: 0.02756991982460022\n",
      "Epoch 58, Batch 100, Loss: 0.02089725062251091\n",
      "Epoch 59, Batch 0, Loss: 0.039105918258428574\n",
      "Epoch 59, Batch 10, Loss: 0.02703874744474888\n",
      "Epoch 59, Batch 20, Loss: 0.027182865887880325\n",
      "Epoch 59, Batch 30, Loss: 0.03291630744934082\n",
      "Epoch 59, Batch 40, Loss: 0.022711414843797684\n",
      "Epoch 59, Batch 50, Loss: 0.021635111421346664\n",
      "Epoch 59, Batch 60, Loss: 0.021989068016409874\n",
      "Epoch 59, Batch 70, Loss: 0.019262656569480896\n",
      "Epoch 59, Batch 80, Loss: 0.030101723968982697\n",
      "Epoch 59, Batch 90, Loss: 0.023196300491690636\n",
      "Epoch 59, Batch 100, Loss: 0.017193734645843506\n",
      "Epoch 60, Batch 0, Loss: 0.01712694577872753\n",
      "Epoch 60, Batch 10, Loss: 0.023331880569458008\n",
      "Epoch 60, Batch 20, Loss: 0.026337863877415657\n",
      "Epoch 60, Batch 30, Loss: 0.026725132018327713\n",
      "Epoch 60, Batch 40, Loss: 0.023885764181613922\n",
      "Epoch 60, Batch 50, Loss: 0.020562482997775078\n",
      "Epoch 60, Batch 60, Loss: 0.019000761210918427\n",
      "Epoch 60, Batch 70, Loss: 0.019986506551504135\n",
      "Epoch 60, Batch 80, Loss: 0.02272571250796318\n",
      "Epoch 60, Batch 90, Loss: 0.027792705222964287\n",
      "Epoch 60, Batch 100, Loss: 0.021488960832357407\n",
      "Epoch 61, Batch 0, Loss: 0.02730533853173256\n",
      "Epoch 61, Batch 10, Loss: 0.02200920321047306\n",
      "Epoch 61, Batch 20, Loss: 0.020910391584038734\n",
      "Epoch 61, Batch 30, Loss: 0.02081114612519741\n",
      "Epoch 61, Batch 40, Loss: 0.022651471197605133\n",
      "Epoch 61, Batch 50, Loss: 0.022968214005231857\n",
      "Epoch 61, Batch 60, Loss: 0.02009980008006096\n",
      "Epoch 61, Batch 70, Loss: 0.026077792048454285\n",
      "Epoch 61, Batch 80, Loss: 0.021397631615400314\n",
      "Epoch 61, Batch 90, Loss: 0.02686895802617073\n",
      "Epoch 61, Batch 100, Loss: 0.025382302701473236\n",
      "Epoch 62, Batch 0, Loss: 0.018950536847114563\n",
      "Epoch 62, Batch 10, Loss: 0.02443520352244377\n",
      "Epoch 62, Batch 20, Loss: 0.02881639450788498\n",
      "Epoch 62, Batch 30, Loss: 0.022128775715827942\n",
      "Epoch 62, Batch 40, Loss: 0.02023499272763729\n",
      "Epoch 62, Batch 50, Loss: 0.02484455332159996\n",
      "Epoch 62, Batch 60, Loss: 0.02549491636455059\n",
      "Epoch 62, Batch 70, Loss: 0.01894449070096016\n",
      "Epoch 62, Batch 80, Loss: 0.029009636491537094\n",
      "Epoch 62, Batch 90, Loss: 0.025630276650190353\n",
      "Epoch 62, Batch 100, Loss: 0.01870185323059559\n",
      "Epoch 63, Batch 0, Loss: 0.025310100987553596\n",
      "Epoch 63, Batch 10, Loss: 0.020372668281197548\n",
      "Epoch 63, Batch 20, Loss: 0.021288705989718437\n",
      "Epoch 63, Batch 30, Loss: 0.03452685847878456\n",
      "Epoch 63, Batch 40, Loss: 0.02034849487245083\n",
      "Epoch 63, Batch 50, Loss: 0.02536298707127571\n",
      "Epoch 63, Batch 60, Loss: 0.024614281952381134\n",
      "Epoch 63, Batch 70, Loss: 0.021701255813241005\n",
      "Epoch 63, Batch 80, Loss: 0.028347186744213104\n",
      "Epoch 63, Batch 90, Loss: 0.028267275542020798\n",
      "Epoch 63, Batch 100, Loss: 0.02262202650308609\n",
      "Epoch 64, Batch 0, Loss: 0.02829793654382229\n",
      "Epoch 64, Batch 10, Loss: 0.02433512918651104\n",
      "Epoch 64, Batch 20, Loss: 0.02412535808980465\n",
      "Epoch 64, Batch 30, Loss: 0.0270058773458004\n",
      "Epoch 64, Batch 40, Loss: 0.02262609452009201\n",
      "Epoch 64, Batch 50, Loss: 0.025090957060456276\n",
      "Epoch 64, Batch 60, Loss: 0.01664574444293976\n",
      "Epoch 64, Batch 70, Loss: 0.022701235488057137\n",
      "Epoch 64, Batch 80, Loss: 0.022375477477908134\n",
      "Epoch 64, Batch 90, Loss: 0.02774268202483654\n",
      "Epoch 64, Batch 100, Loss: 0.03136831894516945\n",
      "Epoch 65, Batch 0, Loss: 0.02353755384683609\n",
      "Epoch 65, Batch 10, Loss: 0.019522450864315033\n",
      "Epoch 65, Batch 20, Loss: 0.026132827624678612\n",
      "Epoch 65, Batch 30, Loss: 0.020443826913833618\n",
      "Epoch 65, Batch 40, Loss: 0.023876022547483444\n",
      "Epoch 65, Batch 50, Loss: 0.03652457520365715\n",
      "Epoch 65, Batch 60, Loss: 0.0338556244969368\n",
      "Epoch 65, Batch 70, Loss: 0.0352909192442894\n",
      "Epoch 65, Batch 80, Loss: 0.02029665932059288\n",
      "Epoch 65, Batch 90, Loss: 0.03168083354830742\n",
      "Epoch 65, Batch 100, Loss: 0.02320227399468422\n",
      "Epoch 66, Batch 0, Loss: 0.018714170902967453\n",
      "Epoch 66, Batch 10, Loss: 0.02915617637336254\n",
      "Epoch 66, Batch 20, Loss: 0.02360396832227707\n",
      "Epoch 66, Batch 30, Loss: 0.018910735845565796\n",
      "Epoch 66, Batch 40, Loss: 0.028910456225275993\n",
      "Epoch 66, Batch 50, Loss: 0.019864549860358238\n",
      "Epoch 66, Batch 60, Loss: 0.02205744758248329\n",
      "Epoch 66, Batch 70, Loss: 0.027902450412511826\n",
      "Epoch 66, Batch 80, Loss: 0.02931082434952259\n",
      "Epoch 66, Batch 90, Loss: 0.026897739619016647\n",
      "Epoch 66, Batch 100, Loss: 0.022569002583622932\n",
      "Epoch 67, Batch 0, Loss: 0.01962043158710003\n",
      "Epoch 67, Batch 10, Loss: 0.02650798112154007\n",
      "Epoch 67, Batch 20, Loss: 0.02470921352505684\n",
      "Epoch 67, Batch 30, Loss: 0.02610066905617714\n",
      "Epoch 67, Batch 40, Loss: 0.027945227921009064\n",
      "Epoch 67, Batch 50, Loss: 0.019471127539873123\n",
      "Epoch 67, Batch 60, Loss: 0.021520495414733887\n",
      "Epoch 67, Batch 70, Loss: 0.022054538130760193\n",
      "Epoch 67, Batch 80, Loss: 0.02775021456182003\n",
      "Epoch 67, Batch 90, Loss: 0.022930653765797615\n",
      "Epoch 67, Batch 100, Loss: 0.028663111850619316\n",
      "Epoch 68, Batch 0, Loss: 0.019877338781952858\n",
      "Epoch 68, Batch 10, Loss: 0.013067342340946198\n",
      "Epoch 68, Batch 20, Loss: 0.030130553990602493\n",
      "Epoch 68, Batch 30, Loss: 0.025268474593758583\n",
      "Epoch 68, Batch 40, Loss: 0.025475990027189255\n",
      "Epoch 68, Batch 50, Loss: 0.022341925650835037\n",
      "Epoch 68, Batch 60, Loss: 0.019831161946058273\n",
      "Epoch 68, Batch 70, Loss: 0.02279016375541687\n",
      "Epoch 68, Batch 80, Loss: 0.02697131037712097\n",
      "Epoch 68, Batch 90, Loss: 0.019281312823295593\n",
      "Epoch 68, Batch 100, Loss: 0.013647172600030899\n",
      "Epoch 69, Batch 0, Loss: 0.018548455089330673\n",
      "Epoch 69, Batch 10, Loss: 0.022052351385354996\n",
      "Epoch 69, Batch 20, Loss: 0.024836644530296326\n",
      "Epoch 69, Batch 30, Loss: 0.02895568124949932\n",
      "Epoch 69, Batch 40, Loss: 0.02345365844666958\n",
      "Epoch 69, Batch 50, Loss: 0.01782163791358471\n",
      "Epoch 69, Batch 60, Loss: 0.03012758307158947\n",
      "Epoch 69, Batch 70, Loss: 0.02122090384364128\n",
      "Epoch 69, Batch 80, Loss: 0.024878108873963356\n",
      "Epoch 69, Batch 90, Loss: 0.025271225720643997\n",
      "Epoch 69, Batch 100, Loss: 0.022395994514226913\n",
      "Epoch 70, Batch 0, Loss: 0.019451599568128586\n",
      "Epoch 70, Batch 10, Loss: 0.021027084439992905\n",
      "Epoch 70, Batch 20, Loss: 0.022790279239416122\n",
      "Epoch 70, Batch 30, Loss: 0.019813163205981255\n",
      "Epoch 70, Batch 40, Loss: 0.02134392410516739\n",
      "Epoch 70, Batch 50, Loss: 0.03229953721165657\n",
      "Epoch 70, Batch 60, Loss: 0.02547072060406208\n",
      "Epoch 70, Batch 70, Loss: 0.0254235677421093\n",
      "Epoch 70, Batch 80, Loss: 0.023994648829102516\n",
      "Epoch 70, Batch 90, Loss: 0.02033364027738571\n",
      "Epoch 70, Batch 100, Loss: 0.023370513692498207\n",
      "Epoch 71, Batch 0, Loss: 0.021609799936413765\n",
      "Epoch 71, Batch 10, Loss: 0.02106507122516632\n",
      "Epoch 71, Batch 20, Loss: 0.01793822832405567\n",
      "Epoch 71, Batch 30, Loss: 0.026340942829847336\n",
      "Epoch 71, Batch 40, Loss: 0.026395058259367943\n",
      "Epoch 71, Batch 50, Loss: 0.02982190251350403\n",
      "Epoch 71, Batch 60, Loss: 0.020121563225984573\n",
      "Epoch 71, Batch 70, Loss: 0.023321963846683502\n",
      "Epoch 71, Batch 80, Loss: 0.01797639951109886\n",
      "Epoch 71, Batch 90, Loss: 0.028442969545722008\n",
      "Epoch 71, Batch 100, Loss: 0.027518603950738907\n",
      "Epoch 72, Batch 0, Loss: 0.01937796361744404\n",
      "Epoch 72, Batch 10, Loss: 0.02517007477581501\n",
      "Epoch 72, Batch 20, Loss: 0.023998701944947243\n",
      "Epoch 72, Batch 30, Loss: 0.02860284596681595\n",
      "Epoch 72, Batch 40, Loss: 0.026593942195177078\n",
      "Epoch 72, Batch 50, Loss: 0.02144254371523857\n",
      "Epoch 72, Batch 60, Loss: 0.022550929337739944\n",
      "Epoch 72, Batch 70, Loss: 0.038087714463472366\n",
      "Epoch 72, Batch 80, Loss: 0.01954358071088791\n",
      "Epoch 72, Batch 90, Loss: 0.025357896462082863\n",
      "Epoch 72, Batch 100, Loss: 0.03347252309322357\n",
      "Epoch 73, Batch 0, Loss: 0.023404672741889954\n",
      "Epoch 73, Batch 10, Loss: 0.025540214031934738\n",
      "Epoch 73, Batch 20, Loss: 0.02080424129962921\n",
      "Epoch 73, Batch 30, Loss: 0.020141232758760452\n",
      "Epoch 73, Batch 40, Loss: 0.033764079213142395\n",
      "Epoch 73, Batch 50, Loss: 0.025804512202739716\n",
      "Epoch 73, Batch 60, Loss: 0.02183682657778263\n",
      "Epoch 73, Batch 70, Loss: 0.02606111951172352\n",
      "Epoch 73, Batch 80, Loss: 0.0186915323138237\n",
      "Epoch 73, Batch 90, Loss: 0.022605108097195625\n",
      "Epoch 73, Batch 100, Loss: 0.023948730900883675\n",
      "Epoch 74, Batch 0, Loss: 0.029303867369890213\n",
      "Epoch 74, Batch 10, Loss: 0.025880318135023117\n",
      "Epoch 74, Batch 20, Loss: 0.028126686811447144\n",
      "Epoch 74, Batch 30, Loss: 0.02266889624297619\n",
      "Epoch 74, Batch 40, Loss: 0.025452857837080956\n",
      "Epoch 74, Batch 50, Loss: 0.024828538298606873\n",
      "Epoch 74, Batch 60, Loss: 0.02461864799261093\n",
      "Epoch 74, Batch 70, Loss: 0.03444924205541611\n",
      "Epoch 74, Batch 80, Loss: 0.023337826132774353\n",
      "Epoch 74, Batch 90, Loss: 0.025416897609829903\n",
      "Epoch 74, Batch 100, Loss: 0.03379411995410919\n",
      "Epoch 75, Batch 0, Loss: 0.03205416351556778\n",
      "Epoch 75, Batch 10, Loss: 0.022187836468219757\n",
      "Epoch 75, Batch 20, Loss: 0.02973409928381443\n",
      "Epoch 75, Batch 30, Loss: 0.02301633544266224\n",
      "Epoch 75, Batch 40, Loss: 0.01363969687372446\n",
      "Epoch 75, Batch 50, Loss: 0.026854079216718674\n",
      "Epoch 75, Batch 60, Loss: 0.020865721628069878\n",
      "Epoch 75, Batch 70, Loss: 0.02352125197649002\n",
      "Epoch 75, Batch 80, Loss: 0.02236996963620186\n",
      "Epoch 75, Batch 90, Loss: 0.019621748477220535\n",
      "Epoch 75, Batch 100, Loss: 0.02109229564666748\n",
      "Epoch 76, Batch 0, Loss: 0.02488407865166664\n",
      "Epoch 76, Batch 10, Loss: 0.01741577312350273\n",
      "Epoch 76, Batch 20, Loss: 0.02139855921268463\n",
      "Epoch 76, Batch 30, Loss: 0.018770316615700722\n",
      "Epoch 76, Batch 40, Loss: 0.02452220395207405\n",
      "Epoch 76, Batch 50, Loss: 0.026960406452417374\n",
      "Epoch 76, Batch 60, Loss: 0.021507229655981064\n",
      "Epoch 76, Batch 70, Loss: 0.023247482255101204\n",
      "Epoch 76, Batch 80, Loss: 0.01548760011792183\n",
      "Epoch 76, Batch 90, Loss: 0.025274481624364853\n",
      "Epoch 76, Batch 100, Loss: 0.022608086466789246\n",
      "Epoch 77, Batch 0, Loss: 0.027456272393465042\n",
      "Epoch 77, Batch 10, Loss: 0.021011007949709892\n",
      "Epoch 77, Batch 20, Loss: 0.027498995885252953\n",
      "Epoch 77, Batch 30, Loss: 0.018670905381441116\n",
      "Epoch 77, Batch 40, Loss: 0.024585533887147903\n",
      "Epoch 77, Batch 50, Loss: 0.024145787581801414\n",
      "Epoch 77, Batch 60, Loss: 0.025365501642227173\n",
      "Epoch 77, Batch 70, Loss: 0.02569747343659401\n",
      "Epoch 77, Batch 80, Loss: 0.025097869336605072\n",
      "Epoch 77, Batch 90, Loss: 0.01903839223086834\n",
      "Epoch 77, Batch 100, Loss: 0.02602744847536087\n",
      "Epoch 78, Batch 0, Loss: 0.02982536517083645\n",
      "Epoch 78, Batch 10, Loss: 0.03407522290945053\n",
      "Epoch 78, Batch 20, Loss: 0.02251303568482399\n",
      "Epoch 78, Batch 30, Loss: 0.02686844952404499\n",
      "Epoch 78, Batch 40, Loss: 0.018745755776762962\n",
      "Epoch 78, Batch 50, Loss: 0.021999895572662354\n",
      "Epoch 78, Batch 60, Loss: 0.023480333387851715\n",
      "Epoch 78, Batch 70, Loss: 0.023799799382686615\n",
      "Epoch 78, Batch 80, Loss: 0.02740803174674511\n",
      "Epoch 78, Batch 90, Loss: 0.025300584733486176\n",
      "Epoch 78, Batch 100, Loss: 0.03147289901971817\n",
      "Epoch 79, Batch 0, Loss: 0.027348482981324196\n",
      "Epoch 79, Batch 10, Loss: 0.020457180216908455\n",
      "Epoch 79, Batch 20, Loss: 0.025423884391784668\n",
      "Epoch 79, Batch 30, Loss: 0.03349869325757027\n",
      "Epoch 79, Batch 40, Loss: 0.025225888937711716\n",
      "Epoch 79, Batch 50, Loss: 0.028377268463373184\n",
      "Epoch 79, Batch 60, Loss: 0.02666306123137474\n",
      "Epoch 79, Batch 70, Loss: 0.02372807264328003\n",
      "Epoch 79, Batch 80, Loss: 0.01985877938568592\n",
      "Epoch 79, Batch 90, Loss: 0.028926819562911987\n",
      "Epoch 79, Batch 100, Loss: 0.021459899842739105\n",
      "Epoch 80, Batch 0, Loss: 0.015687376260757446\n",
      "Epoch 80, Batch 10, Loss: 0.02447056584060192\n",
      "Epoch 80, Batch 20, Loss: 0.019807251170277596\n",
      "Epoch 80, Batch 30, Loss: 0.020111719146370888\n",
      "Epoch 80, Batch 40, Loss: 0.02327142097055912\n",
      "Epoch 80, Batch 50, Loss: 0.023809615522623062\n",
      "Epoch 80, Batch 60, Loss: 0.01936822198331356\n",
      "Epoch 80, Batch 70, Loss: 0.025995176285505295\n",
      "Epoch 80, Batch 80, Loss: 0.026999497786164284\n",
      "Epoch 80, Batch 90, Loss: 0.02780301310122013\n",
      "Epoch 80, Batch 100, Loss: 0.023485440760850906\n",
      "Epoch 81, Batch 0, Loss: 0.030156882479786873\n",
      "Epoch 81, Batch 10, Loss: 0.021198133006691933\n",
      "Epoch 81, Batch 20, Loss: 0.02480757236480713\n",
      "Epoch 81, Batch 30, Loss: 0.021633051335811615\n",
      "Epoch 81, Batch 40, Loss: 0.030686527490615845\n",
      "Epoch 81, Batch 50, Loss: 0.02764531783759594\n",
      "Epoch 81, Batch 60, Loss: 0.02334235981106758\n",
      "Epoch 81, Batch 70, Loss: 0.018245995044708252\n",
      "Epoch 81, Batch 80, Loss: 0.02000703476369381\n",
      "Epoch 81, Batch 90, Loss: 0.025716181844472885\n",
      "Epoch 81, Batch 100, Loss: 0.021239018067717552\n",
      "Epoch 82, Batch 0, Loss: 0.018790237605571747\n",
      "Epoch 82, Batch 10, Loss: 0.026225615292787552\n",
      "Epoch 82, Batch 20, Loss: 0.02248639427125454\n",
      "Epoch 82, Batch 30, Loss: 0.02897759899497032\n",
      "Epoch 82, Batch 40, Loss: 0.017940381541848183\n",
      "Epoch 82, Batch 50, Loss: 0.02369844913482666\n",
      "Epoch 82, Batch 60, Loss: 0.022395320236682892\n",
      "Epoch 82, Batch 70, Loss: 0.02675672434270382\n",
      "Epoch 82, Batch 80, Loss: 0.025238940492272377\n",
      "Epoch 82, Batch 90, Loss: 0.024775570258498192\n",
      "Epoch 82, Batch 100, Loss: 0.02153591997921467\n",
      "Epoch 83, Batch 0, Loss: 0.016761943697929382\n",
      "Epoch 83, Batch 10, Loss: 0.018126830458641052\n",
      "Epoch 83, Batch 20, Loss: 0.024692809209227562\n",
      "Epoch 83, Batch 30, Loss: 0.028978459537029266\n",
      "Epoch 83, Batch 40, Loss: 0.022286158055067062\n",
      "Epoch 83, Batch 50, Loss: 0.02177114598453045\n",
      "Epoch 83, Batch 60, Loss: 0.019713489338755608\n",
      "Epoch 83, Batch 70, Loss: 0.02327304519712925\n",
      "Epoch 83, Batch 80, Loss: 0.02044972963631153\n",
      "Epoch 83, Batch 90, Loss: 0.020818054676055908\n",
      "Epoch 83, Batch 100, Loss: 0.01975051872432232\n",
      "Epoch 84, Batch 0, Loss: 0.027560323476791382\n",
      "Epoch 84, Batch 10, Loss: 0.027240224182605743\n",
      "Epoch 84, Batch 20, Loss: 0.018566707149147987\n",
      "Epoch 84, Batch 30, Loss: 0.02117367647588253\n",
      "Epoch 84, Batch 40, Loss: 0.02093270607292652\n",
      "Epoch 84, Batch 50, Loss: 0.020548732951283455\n",
      "Epoch 84, Batch 60, Loss: 0.03221137076616287\n",
      "Epoch 84, Batch 70, Loss: 0.018276941031217575\n",
      "Epoch 84, Batch 80, Loss: 0.027141761034727097\n",
      "Epoch 84, Batch 90, Loss: 0.02513670176267624\n",
      "Epoch 84, Batch 100, Loss: 0.02423873171210289\n",
      "Epoch 85, Batch 0, Loss: 0.019507260993123055\n",
      "Epoch 85, Batch 10, Loss: 0.027737431228160858\n",
      "Epoch 85, Batch 20, Loss: 0.021045401692390442\n",
      "Epoch 85, Batch 30, Loss: 0.025179388001561165\n",
      "Epoch 85, Batch 40, Loss: 0.02240385115146637\n",
      "Epoch 85, Batch 50, Loss: 0.02289735898375511\n",
      "Epoch 85, Batch 60, Loss: 0.02524697035551071\n",
      "Epoch 85, Batch 70, Loss: 0.020213402807712555\n",
      "Epoch 85, Batch 80, Loss: 0.01839003525674343\n",
      "Epoch 85, Batch 90, Loss: 0.01915341429412365\n",
      "Epoch 85, Batch 100, Loss: 0.020212918519973755\n",
      "Epoch 86, Batch 0, Loss: 0.02124134823679924\n",
      "Epoch 86, Batch 10, Loss: 0.02095475234091282\n",
      "Epoch 86, Batch 20, Loss: 0.021129783242940903\n",
      "Epoch 86, Batch 30, Loss: 0.021828215569257736\n",
      "Epoch 86, Batch 40, Loss: 0.020999662578105927\n",
      "Epoch 86, Batch 50, Loss: 0.021619699895381927\n",
      "Epoch 86, Batch 60, Loss: 0.022970613092184067\n",
      "Epoch 86, Batch 70, Loss: 0.019294336438179016\n",
      "Epoch 86, Batch 80, Loss: 0.017350081354379654\n",
      "Epoch 86, Batch 90, Loss: 0.02921387180685997\n",
      "Epoch 86, Batch 100, Loss: 0.03153780475258827\n",
      "Epoch 87, Batch 0, Loss: 0.02217002771794796\n",
      "Epoch 87, Batch 10, Loss: 0.03089512139558792\n",
      "Epoch 87, Batch 20, Loss: 0.026493284851312637\n",
      "Epoch 87, Batch 30, Loss: 0.019173074513673782\n",
      "Epoch 87, Batch 40, Loss: 0.02246161364018917\n",
      "Epoch 87, Batch 50, Loss: 0.019838083535432816\n",
      "Epoch 87, Batch 60, Loss: 0.027147460728883743\n",
      "Epoch 87, Batch 70, Loss: 0.02609417773783207\n",
      "Epoch 87, Batch 80, Loss: 0.021381724625825882\n",
      "Epoch 87, Batch 90, Loss: 0.027346180751919746\n",
      "Epoch 87, Batch 100, Loss: 0.02086152322590351\n",
      "Epoch 88, Batch 0, Loss: 0.02247535064816475\n",
      "Epoch 88, Batch 10, Loss: 0.02451612800359726\n",
      "Epoch 88, Batch 20, Loss: 0.018033020198345184\n",
      "Epoch 88, Batch 30, Loss: 0.02250375971198082\n",
      "Epoch 88, Batch 40, Loss: 0.02188362367451191\n",
      "Epoch 88, Batch 50, Loss: 0.0185118205845356\n",
      "Epoch 88, Batch 60, Loss: 0.02392367646098137\n",
      "Epoch 88, Batch 70, Loss: 0.02532307244837284\n",
      "Epoch 88, Batch 80, Loss: 0.03082473948597908\n",
      "Epoch 88, Batch 90, Loss: 0.019918818026781082\n",
      "Epoch 88, Batch 100, Loss: 0.01910305581986904\n",
      "Epoch 89, Batch 0, Loss: 0.020134642720222473\n",
      "Epoch 89, Batch 10, Loss: 0.019426733255386353\n",
      "Epoch 89, Batch 20, Loss: 0.021733678877353668\n",
      "Epoch 89, Batch 30, Loss: 0.021918078884482384\n",
      "Epoch 89, Batch 40, Loss: 0.023821484297513962\n",
      "Epoch 89, Batch 50, Loss: 0.0210857093334198\n",
      "Epoch 89, Batch 60, Loss: 0.024160288274288177\n",
      "Epoch 89, Batch 70, Loss: 0.031131304800510406\n",
      "Epoch 89, Batch 80, Loss: 0.024326888844370842\n",
      "Epoch 89, Batch 90, Loss: 0.021983599290251732\n",
      "Epoch 89, Batch 100, Loss: 0.033280111849308014\n",
      "Epoch 90, Batch 0, Loss: 0.016482727602124214\n",
      "Epoch 90, Batch 10, Loss: 0.027962110936641693\n",
      "Epoch 90, Batch 20, Loss: 0.01434575766324997\n",
      "Epoch 90, Batch 30, Loss: 0.02668483927845955\n",
      "Epoch 90, Batch 40, Loss: 0.01965208351612091\n",
      "Epoch 90, Batch 50, Loss: 0.028711684048175812\n",
      "Epoch 90, Batch 60, Loss: 0.022094454616308212\n",
      "Epoch 90, Batch 70, Loss: 0.018045267090201378\n",
      "Epoch 90, Batch 80, Loss: 0.017082568258047104\n",
      "Epoch 90, Batch 90, Loss: 0.02738466113805771\n",
      "Epoch 90, Batch 100, Loss: 0.015418685041368008\n",
      "Epoch 91, Batch 0, Loss: 0.02048744633793831\n",
      "Epoch 91, Batch 10, Loss: 0.01956179365515709\n",
      "Epoch 91, Batch 20, Loss: 0.023972418159246445\n",
      "Epoch 91, Batch 30, Loss: 0.016943102702498436\n",
      "Epoch 91, Batch 40, Loss: 0.023802531883120537\n",
      "Epoch 91, Batch 50, Loss: 0.027815718203783035\n",
      "Epoch 91, Batch 60, Loss: 0.034013669937849045\n",
      "Epoch 91, Batch 70, Loss: 0.01785208098590374\n",
      "Epoch 91, Batch 80, Loss: 0.02618827298283577\n",
      "Epoch 91, Batch 90, Loss: 0.022489603608846664\n",
      "Epoch 91, Batch 100, Loss: 0.027720103040337563\n",
      "Epoch 92, Batch 0, Loss: 0.019059451296925545\n",
      "Epoch 92, Batch 10, Loss: 0.029873332008719444\n",
      "Epoch 92, Batch 20, Loss: 0.02448522299528122\n",
      "Epoch 92, Batch 30, Loss: 0.025161560624837875\n",
      "Epoch 92, Batch 40, Loss: 0.0188746340572834\n",
      "Epoch 92, Batch 50, Loss: 0.022180331870913506\n",
      "Epoch 92, Batch 60, Loss: 0.021684149280190468\n",
      "Epoch 92, Batch 70, Loss: 0.025604842230677605\n",
      "Epoch 92, Batch 80, Loss: 0.02660810761153698\n",
      "Epoch 92, Batch 90, Loss: 0.018340446054935455\n",
      "Epoch 92, Batch 100, Loss: 0.024377651512622833\n",
      "Epoch 93, Batch 0, Loss: 0.017043080180883408\n",
      "Epoch 93, Batch 10, Loss: 0.019939683377742767\n",
      "Epoch 93, Batch 20, Loss: 0.020573122426867485\n",
      "Epoch 93, Batch 30, Loss: 0.017895542085170746\n",
      "Epoch 93, Batch 40, Loss: 0.02103516273200512\n",
      "Epoch 93, Batch 50, Loss: 0.021884813904762268\n",
      "Epoch 93, Batch 60, Loss: 0.02709396928548813\n",
      "Epoch 93, Batch 70, Loss: 0.022422777488827705\n",
      "Epoch 93, Batch 80, Loss: 0.02774365432560444\n",
      "Epoch 93, Batch 90, Loss: 0.024707766249775887\n",
      "Epoch 93, Batch 100, Loss: 0.023311443626880646\n",
      "Epoch 94, Batch 0, Loss: 0.031617168337106705\n",
      "Epoch 94, Batch 10, Loss: 0.03012334741652012\n",
      "Epoch 94, Batch 20, Loss: 0.02879774197936058\n",
      "Epoch 94, Batch 30, Loss: 0.023779232054948807\n",
      "Epoch 94, Batch 40, Loss: 0.029269564896821976\n",
      "Epoch 94, Batch 50, Loss: 0.027258945629000664\n",
      "Epoch 94, Batch 60, Loss: 0.020186910405755043\n",
      "Epoch 94, Batch 70, Loss: 0.02239769883453846\n",
      "Epoch 94, Batch 80, Loss: 0.0284938532859087\n",
      "Epoch 94, Batch 90, Loss: 0.02439306303858757\n",
      "Epoch 94, Batch 100, Loss: 0.01719837822020054\n",
      "Epoch 95, Batch 0, Loss: 0.024711964651942253\n",
      "Epoch 95, Batch 10, Loss: 0.025142060592770576\n",
      "Epoch 95, Batch 20, Loss: 0.022181561216711998\n",
      "Epoch 95, Batch 30, Loss: 0.023964429274201393\n",
      "Epoch 95, Batch 40, Loss: 0.025627942755818367\n",
      "Epoch 95, Batch 50, Loss: 0.02221040613949299\n",
      "Epoch 95, Batch 60, Loss: 0.028880173340439796\n",
      "Epoch 95, Batch 70, Loss: 0.024249352514743805\n",
      "Epoch 95, Batch 80, Loss: 0.028981192037463188\n",
      "Epoch 95, Batch 90, Loss: 0.021100535988807678\n",
      "Epoch 95, Batch 100, Loss: 0.021672986447811127\n",
      "Epoch 96, Batch 0, Loss: 0.028959309682250023\n",
      "Epoch 96, Batch 10, Loss: 0.0226012971252203\n",
      "Epoch 96, Batch 20, Loss: 0.027356863021850586\n",
      "Epoch 96, Batch 30, Loss: 0.024591412395238876\n",
      "Epoch 96, Batch 40, Loss: 0.03291827812790871\n",
      "Epoch 96, Batch 50, Loss: 0.02114882878959179\n",
      "Epoch 96, Batch 60, Loss: 0.022323494777083397\n",
      "Epoch 96, Batch 70, Loss: 0.028730910271406174\n",
      "Epoch 96, Batch 80, Loss: 0.022154882550239563\n",
      "Epoch 96, Batch 90, Loss: 0.01795554719865322\n",
      "Epoch 96, Batch 100, Loss: 0.025822361931204796\n",
      "Epoch 97, Batch 0, Loss: 0.018827512860298157\n",
      "Epoch 97, Batch 10, Loss: 0.025820672512054443\n",
      "Epoch 97, Batch 20, Loss: 0.01931675523519516\n",
      "Epoch 97, Batch 30, Loss: 0.0304609052836895\n",
      "Epoch 97, Batch 40, Loss: 0.020351087674498558\n",
      "Epoch 97, Batch 50, Loss: 0.024277467280626297\n",
      "Epoch 97, Batch 60, Loss: 0.023304680362343788\n",
      "Epoch 97, Batch 70, Loss: 0.023054031655192375\n",
      "Epoch 97, Batch 80, Loss: 0.028668329119682312\n",
      "Epoch 97, Batch 90, Loss: 0.03176463022828102\n",
      "Epoch 97, Batch 100, Loss: 0.03197580203413963\n",
      "Epoch 98, Batch 0, Loss: 0.024052485823631287\n",
      "Epoch 98, Batch 10, Loss: 0.02670319378376007\n",
      "Epoch 98, Batch 20, Loss: 0.03305624797940254\n",
      "Epoch 98, Batch 30, Loss: 0.027811778709292412\n",
      "Epoch 98, Batch 40, Loss: 0.021438827738165855\n",
      "Epoch 98, Batch 50, Loss: 0.024524299427866936\n",
      "Epoch 98, Batch 60, Loss: 0.02831047773361206\n",
      "Epoch 98, Batch 70, Loss: 0.028350824490189552\n",
      "Epoch 98, Batch 80, Loss: 0.032132670283317566\n",
      "Epoch 98, Batch 90, Loss: 0.027908658608794212\n",
      "Epoch 98, Batch 100, Loss: 0.021889397874474525\n",
      "Epoch 99, Batch 0, Loss: 0.02506692335009575\n",
      "Epoch 99, Batch 10, Loss: 0.020785663276910782\n",
      "Epoch 99, Batch 20, Loss: 0.026764892041683197\n",
      "Epoch 99, Batch 30, Loss: 0.020622801035642624\n",
      "Epoch 99, Batch 40, Loss: 0.01721719279885292\n",
      "Epoch 99, Batch 50, Loss: 0.02302858605980873\n",
      "Epoch 99, Batch 60, Loss: 0.024243809282779694\n",
      "Epoch 99, Batch 70, Loss: 0.031267065554857254\n",
      "Epoch 99, Batch 80, Loss: 0.02277807705104351\n",
      "Epoch 99, Batch 90, Loss: 0.019134018570184708\n",
      "Epoch 99, Batch 100, Loss: 0.026330310851335526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_BILSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (bilstm): BILSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True, bidirectional=True)\n",
       "    (fc1): Linear(in_features=128, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.041022; Test RMSE 38.856445\n",
      "\n",
      "Train  MAE: 0.023681; Test  MAE 27.215050\n",
      "\n",
      "Train  R^2: 0.998321; Test  R^2 0.965925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
