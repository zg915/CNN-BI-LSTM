{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_preprocessing import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6618, 10, 8]) torch.Size([6618, 1]) torch.Size([490, 10, 8]) (490, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, y_scaler, train_loader, test_loader = get_data(\n",
    "            timestep = 10,\n",
    "            batch_size = 64,\n",
    "            y_name = 'Closing price',\n",
    "            train_size = 6627,\n",
    "            path = \"Data/SSE000001.csv\")\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    print (\"GPU device not found.\")\n",
    "\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer parameters from the table\n",
    "        self.conv1d = nn.Conv1d(in_channels= 8, out_channels= 32, kernel_size= 1, padding=\"same\")\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Pooling\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size= 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layer\n",
    "        x = self.conv1d(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64, lstm_dropout = 0.2, fc1_output_size = 16):\n",
    "        super(BILSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True,\n",
    "                             bidirectional = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size * 2, self.fc1_output_size)\n",
    "        self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        h_fc2 = self.fc2(h_fc1)\n",
    "        output = h_fc2[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64,  fc1_output_size = 16):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        # self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p = self.lstm_dropout)\n",
    "        # self.fc1 = nn.Linear(self.lstm_hidden_size, self.fc1_output_size)\n",
    "        # self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        tan_h_lstm = self.tanh(h_lstm)\n",
    "        # h_dropout = self.dropout(h_lstm)\n",
    "\n",
    "        # h_fc1 = self.fc1(h_lstm)\n",
    "        # h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        # h_fc2 = self.fc2(h_fc1)\n",
    "        # output = h_fc2[:, -1, :]\n",
    "\n",
    "        h_fc1 = self.fc1(tan_h_lstm)\n",
    "        output = h_fc1[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.lstm = LSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x) #[64, 32, 10]\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1) #[64, 10, 32]\n",
    "        x = self.lstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_LSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.7491185665130615\n",
      "Epoch 0, Batch 10, Loss: 0.7189062833786011\n",
      "Epoch 0, Batch 20, Loss: 0.6353232860565186\n",
      "Epoch 0, Batch 30, Loss: 0.6690065860748291\n",
      "Epoch 0, Batch 40, Loss: 0.6986151337623596\n",
      "Epoch 0, Batch 50, Loss: 0.6975069046020508\n",
      "Epoch 0, Batch 60, Loss: 0.731777548789978\n",
      "Epoch 0, Batch 70, Loss: 0.6237348318099976\n",
      "Epoch 0, Batch 80, Loss: 0.6206910610198975\n",
      "Epoch 0, Batch 90, Loss: 0.621310293674469\n",
      "Epoch 0, Batch 100, Loss: 0.535598874092102\n",
      "Epoch 1, Batch 0, Loss: 0.5707441568374634\n",
      "Epoch 1, Batch 10, Loss: 0.45867636799812317\n",
      "Epoch 1, Batch 20, Loss: 0.36037755012512207\n",
      "Epoch 1, Batch 30, Loss: 0.20988866686820984\n",
      "Epoch 1, Batch 40, Loss: 0.12761858105659485\n",
      "Epoch 1, Batch 50, Loss: 0.14389510452747345\n",
      "Epoch 1, Batch 60, Loss: 0.19735851883888245\n",
      "Epoch 1, Batch 70, Loss: 0.12763115763664246\n",
      "Epoch 1, Batch 80, Loss: 0.11140377074480057\n",
      "Epoch 1, Batch 90, Loss: 0.16423824429512024\n",
      "Epoch 1, Batch 100, Loss: 0.06517411023378372\n",
      "Epoch 2, Batch 0, Loss: 0.11661655455827713\n",
      "Epoch 2, Batch 10, Loss: 0.203014075756073\n",
      "Epoch 2, Batch 20, Loss: 0.12283065170049667\n",
      "Epoch 2, Batch 30, Loss: 0.08804082125425339\n",
      "Epoch 2, Batch 40, Loss: 0.07413022965192795\n",
      "Epoch 2, Batch 50, Loss: 0.11858700215816498\n",
      "Epoch 2, Batch 60, Loss: 0.12451457977294922\n",
      "Epoch 2, Batch 70, Loss: 0.09576191753149033\n",
      "Epoch 2, Batch 80, Loss: 0.056124310940504074\n",
      "Epoch 2, Batch 90, Loss: 0.0823059231042862\n",
      "Epoch 2, Batch 100, Loss: 0.0652439296245575\n",
      "Epoch 3, Batch 0, Loss: 0.07162386178970337\n",
      "Epoch 3, Batch 10, Loss: 0.0663694366812706\n",
      "Epoch 3, Batch 20, Loss: 0.047367919236421585\n",
      "Epoch 3, Batch 30, Loss: 0.1178482174873352\n",
      "Epoch 3, Batch 40, Loss: 0.12141828238964081\n",
      "Epoch 3, Batch 50, Loss: 0.07011788338422775\n",
      "Epoch 3, Batch 60, Loss: 0.08252579718828201\n",
      "Epoch 3, Batch 70, Loss: 0.12316065281629562\n",
      "Epoch 3, Batch 80, Loss: 0.08110266923904419\n",
      "Epoch 3, Batch 90, Loss: 0.07812411338090897\n",
      "Epoch 3, Batch 100, Loss: 0.06110691279172897\n",
      "Epoch 4, Batch 0, Loss: 0.07196086645126343\n",
      "Epoch 4, Batch 10, Loss: 0.04349030181765556\n",
      "Epoch 4, Batch 20, Loss: 0.10416749864816666\n",
      "Epoch 4, Batch 30, Loss: 0.054422467947006226\n",
      "Epoch 4, Batch 40, Loss: 0.1206948310136795\n",
      "Epoch 4, Batch 50, Loss: 0.08441530168056488\n",
      "Epoch 4, Batch 60, Loss: 0.04916823282837868\n",
      "Epoch 4, Batch 70, Loss: 0.06811882555484772\n",
      "Epoch 4, Batch 80, Loss: 0.13093014061450958\n",
      "Epoch 4, Batch 90, Loss: 0.05004943534731865\n",
      "Epoch 4, Batch 100, Loss: 0.06561282277107239\n",
      "Epoch 5, Batch 0, Loss: 0.10038424283266068\n",
      "Epoch 5, Batch 10, Loss: 0.09036725014448166\n",
      "Epoch 5, Batch 20, Loss: 0.06952425092458725\n",
      "Epoch 5, Batch 30, Loss: 0.07481235265731812\n",
      "Epoch 5, Batch 40, Loss: 0.06293556839227676\n",
      "Epoch 5, Batch 50, Loss: 0.076524518430233\n",
      "Epoch 5, Batch 60, Loss: 0.0769612044095993\n",
      "Epoch 5, Batch 70, Loss: 0.036339715123176575\n",
      "Epoch 5, Batch 80, Loss: 0.09022515267133713\n",
      "Epoch 5, Batch 90, Loss: 0.06600672751665115\n",
      "Epoch 5, Batch 100, Loss: 0.0629049688577652\n",
      "Epoch 6, Batch 0, Loss: 0.05317821353673935\n",
      "Epoch 6, Batch 10, Loss: 0.05419613793492317\n",
      "Epoch 6, Batch 20, Loss: 0.07357239723205566\n",
      "Epoch 6, Batch 30, Loss: 0.04223686829209328\n",
      "Epoch 6, Batch 40, Loss: 0.029041307047009468\n",
      "Epoch 6, Batch 50, Loss: 0.048683833330869675\n",
      "Epoch 6, Batch 60, Loss: 0.06264697015285492\n",
      "Epoch 6, Batch 70, Loss: 0.028667090460658073\n",
      "Epoch 6, Batch 80, Loss: 0.043768227100372314\n",
      "Epoch 6, Batch 90, Loss: 0.03236302733421326\n",
      "Epoch 6, Batch 100, Loss: 0.05045730248093605\n",
      "Epoch 7, Batch 0, Loss: 0.06269654631614685\n",
      "Epoch 7, Batch 10, Loss: 0.05221547186374664\n",
      "Epoch 7, Batch 20, Loss: 0.047688957303762436\n",
      "Epoch 7, Batch 30, Loss: 0.04812539368867874\n",
      "Epoch 7, Batch 40, Loss: 0.0339190736413002\n",
      "Epoch 7, Batch 50, Loss: 0.033990368247032166\n",
      "Epoch 7, Batch 60, Loss: 0.024210967123508453\n",
      "Epoch 7, Batch 70, Loss: 0.03113367035984993\n",
      "Epoch 7, Batch 80, Loss: 0.030381955206394196\n",
      "Epoch 7, Batch 90, Loss: 0.03469409421086311\n",
      "Epoch 7, Batch 100, Loss: 0.02743876725435257\n",
      "Epoch 8, Batch 0, Loss: 0.029518738389015198\n",
      "Epoch 8, Batch 10, Loss: 0.051914751529693604\n",
      "Epoch 8, Batch 20, Loss: 0.02490215189754963\n",
      "Epoch 8, Batch 30, Loss: 0.04682856798171997\n",
      "Epoch 8, Batch 40, Loss: 0.054890070110559464\n",
      "Epoch 8, Batch 50, Loss: 0.02790670469403267\n",
      "Epoch 8, Batch 60, Loss: 0.03311902657151222\n",
      "Epoch 8, Batch 70, Loss: 0.042502906173467636\n",
      "Epoch 8, Batch 80, Loss: 0.023286692798137665\n",
      "Epoch 8, Batch 90, Loss: 0.05421644449234009\n",
      "Epoch 8, Batch 100, Loss: 0.02618931047618389\n",
      "Epoch 9, Batch 0, Loss: 0.0391654446721077\n",
      "Epoch 9, Batch 10, Loss: 0.0257624089717865\n",
      "Epoch 9, Batch 20, Loss: 0.041635941714048386\n",
      "Epoch 9, Batch 30, Loss: 0.039117053151130676\n",
      "Epoch 9, Batch 40, Loss: 0.029253996908664703\n",
      "Epoch 9, Batch 50, Loss: 0.02850349061191082\n",
      "Epoch 9, Batch 60, Loss: 0.030205829069018364\n",
      "Epoch 9, Batch 70, Loss: 0.03738902881741524\n",
      "Epoch 9, Batch 80, Loss: 0.02523089572787285\n",
      "Epoch 9, Batch 90, Loss: 0.028890380635857582\n",
      "Epoch 9, Batch 100, Loss: 0.031001005321741104\n",
      "Epoch 10, Batch 0, Loss: 0.03453906252980232\n",
      "Epoch 10, Batch 10, Loss: 0.03132370859384537\n",
      "Epoch 10, Batch 20, Loss: 0.038120292127132416\n",
      "Epoch 10, Batch 30, Loss: 0.02467295154929161\n",
      "Epoch 10, Batch 40, Loss: 0.024594606831669807\n",
      "Epoch 10, Batch 50, Loss: 0.04885510355234146\n",
      "Epoch 10, Batch 60, Loss: 0.03342181816697121\n",
      "Epoch 10, Batch 70, Loss: 0.025783643126487732\n",
      "Epoch 10, Batch 80, Loss: 0.03619951382279396\n",
      "Epoch 10, Batch 90, Loss: 0.031584709882736206\n",
      "Epoch 10, Batch 100, Loss: 0.0353643037378788\n",
      "Epoch 11, Batch 0, Loss: 0.024220068007707596\n",
      "Epoch 11, Batch 10, Loss: 0.025566764175891876\n",
      "Epoch 11, Batch 20, Loss: 0.04034946858882904\n",
      "Epoch 11, Batch 30, Loss: 0.047182027250528336\n",
      "Epoch 11, Batch 40, Loss: 0.03308989480137825\n",
      "Epoch 11, Batch 50, Loss: 0.024867240339517593\n",
      "Epoch 11, Batch 60, Loss: 0.03075704723596573\n",
      "Epoch 11, Batch 70, Loss: 0.026965821161866188\n",
      "Epoch 11, Batch 80, Loss: 0.038293201476335526\n",
      "Epoch 11, Batch 90, Loss: 0.03457828611135483\n",
      "Epoch 11, Batch 100, Loss: 0.025830212980508804\n",
      "Epoch 12, Batch 0, Loss: 0.03497973829507828\n",
      "Epoch 12, Batch 10, Loss: 0.038361430168151855\n",
      "Epoch 12, Batch 20, Loss: 0.037171367555856705\n",
      "Epoch 12, Batch 30, Loss: 0.025622887536883354\n",
      "Epoch 12, Batch 40, Loss: 0.02650386095046997\n",
      "Epoch 12, Batch 50, Loss: 0.028293177485466003\n",
      "Epoch 12, Batch 60, Loss: 0.035078391432762146\n",
      "Epoch 12, Batch 70, Loss: 0.02657645009458065\n",
      "Epoch 12, Batch 80, Loss: 0.02500119060277939\n",
      "Epoch 12, Batch 90, Loss: 0.03182877227663994\n",
      "Epoch 12, Batch 100, Loss: 0.033235687762498856\n",
      "Epoch 13, Batch 0, Loss: 0.02630552090704441\n",
      "Epoch 13, Batch 10, Loss: 0.03716645389795303\n",
      "Epoch 13, Batch 20, Loss: 0.026385284960269928\n",
      "Epoch 13, Batch 30, Loss: 0.03586104139685631\n",
      "Epoch 13, Batch 40, Loss: 0.0274353064596653\n",
      "Epoch 13, Batch 50, Loss: 0.032167959958314896\n",
      "Epoch 13, Batch 60, Loss: 0.022768482565879822\n",
      "Epoch 13, Batch 70, Loss: 0.02551315538585186\n",
      "Epoch 13, Batch 80, Loss: 0.024910371750593185\n",
      "Epoch 13, Batch 90, Loss: 0.03359135985374451\n",
      "Epoch 13, Batch 100, Loss: 0.029317623004317284\n",
      "Epoch 14, Batch 0, Loss: 0.03479815647006035\n",
      "Epoch 14, Batch 10, Loss: 0.03877188265323639\n",
      "Epoch 14, Batch 20, Loss: 0.019020602107048035\n",
      "Epoch 14, Batch 30, Loss: 0.020940516144037247\n",
      "Epoch 14, Batch 40, Loss: 0.033216312527656555\n",
      "Epoch 14, Batch 50, Loss: 0.02438676916062832\n",
      "Epoch 14, Batch 60, Loss: 0.03826817870140076\n",
      "Epoch 14, Batch 70, Loss: 0.02696075290441513\n",
      "Epoch 14, Batch 80, Loss: 0.02219841629266739\n",
      "Epoch 14, Batch 90, Loss: 0.030079763382673264\n",
      "Epoch 14, Batch 100, Loss: 0.027522116899490356\n",
      "Epoch 15, Batch 0, Loss: 0.024317214265465736\n",
      "Epoch 15, Batch 10, Loss: 0.02554578147828579\n",
      "Epoch 15, Batch 20, Loss: 0.03072849102318287\n",
      "Epoch 15, Batch 30, Loss: 0.03277188166975975\n",
      "Epoch 15, Batch 40, Loss: 0.02696426585316658\n",
      "Epoch 15, Batch 50, Loss: 0.037645839154720306\n",
      "Epoch 15, Batch 60, Loss: 0.03601309657096863\n",
      "Epoch 15, Batch 70, Loss: 0.02531566470861435\n",
      "Epoch 15, Batch 80, Loss: 0.03012070804834366\n",
      "Epoch 15, Batch 90, Loss: 0.026239221915602684\n",
      "Epoch 15, Batch 100, Loss: 0.021219192072749138\n",
      "Epoch 16, Batch 0, Loss: 0.02510678768157959\n",
      "Epoch 16, Batch 10, Loss: 0.027993345633149147\n",
      "Epoch 16, Batch 20, Loss: 0.034482523798942566\n",
      "Epoch 16, Batch 30, Loss: 0.027957839891314507\n",
      "Epoch 16, Batch 40, Loss: 0.027293652296066284\n",
      "Epoch 16, Batch 50, Loss: 0.026578059419989586\n",
      "Epoch 16, Batch 60, Loss: 0.034741222858428955\n",
      "Epoch 16, Batch 70, Loss: 0.025437038391828537\n",
      "Epoch 16, Batch 80, Loss: 0.02593277208507061\n",
      "Epoch 16, Batch 90, Loss: 0.02349162846803665\n",
      "Epoch 16, Batch 100, Loss: 0.03664859011769295\n",
      "Epoch 17, Batch 0, Loss: 0.022182878106832504\n",
      "Epoch 17, Batch 10, Loss: 0.02896156534552574\n",
      "Epoch 17, Batch 20, Loss: 0.024568231776356697\n",
      "Epoch 17, Batch 30, Loss: 0.022927366197109222\n",
      "Epoch 17, Batch 40, Loss: 0.02805379219353199\n",
      "Epoch 17, Batch 50, Loss: 0.017168084159493446\n",
      "Epoch 17, Batch 60, Loss: 0.025104690343141556\n",
      "Epoch 17, Batch 70, Loss: 0.03753945976495743\n",
      "Epoch 17, Batch 80, Loss: 0.01840181089937687\n",
      "Epoch 17, Batch 90, Loss: 0.02493908628821373\n",
      "Epoch 17, Batch 100, Loss: 0.017714429646730423\n",
      "Epoch 18, Batch 0, Loss: 0.02915346994996071\n",
      "Epoch 18, Batch 10, Loss: 0.022723769769072533\n",
      "Epoch 18, Batch 20, Loss: 0.02598954364657402\n",
      "Epoch 18, Batch 30, Loss: 0.027384601533412933\n",
      "Epoch 18, Batch 40, Loss: 0.03509524464607239\n",
      "Epoch 18, Batch 50, Loss: 0.022770581766963005\n",
      "Epoch 18, Batch 60, Loss: 0.02462206967175007\n",
      "Epoch 18, Batch 70, Loss: 0.023885762318968773\n",
      "Epoch 18, Batch 80, Loss: 0.022485628724098206\n",
      "Epoch 18, Batch 90, Loss: 0.021447250619530678\n",
      "Epoch 18, Batch 100, Loss: 0.0296788327395916\n",
      "Epoch 19, Batch 0, Loss: 0.02691216766834259\n",
      "Epoch 19, Batch 10, Loss: 0.018241263926029205\n",
      "Epoch 19, Batch 20, Loss: 0.031390637159347534\n",
      "Epoch 19, Batch 30, Loss: 0.023815805092453957\n",
      "Epoch 19, Batch 40, Loss: 0.019067049026489258\n",
      "Epoch 19, Batch 50, Loss: 0.021435197442770004\n",
      "Epoch 19, Batch 60, Loss: 0.03129687160253525\n",
      "Epoch 19, Batch 70, Loss: 0.02518528141081333\n",
      "Epoch 19, Batch 80, Loss: 0.02216866798698902\n",
      "Epoch 19, Batch 90, Loss: 0.028991080820560455\n",
      "Epoch 19, Batch 100, Loss: 0.02092265896499157\n",
      "Epoch 20, Batch 0, Loss: 0.02440745197236538\n",
      "Epoch 20, Batch 10, Loss: 0.01937108486890793\n",
      "Epoch 20, Batch 20, Loss: 0.03124208189547062\n",
      "Epoch 20, Batch 30, Loss: 0.02734193205833435\n",
      "Epoch 20, Batch 40, Loss: 0.024319211021065712\n",
      "Epoch 20, Batch 50, Loss: 0.027132300660014153\n",
      "Epoch 20, Batch 60, Loss: 0.024177078157663345\n",
      "Epoch 20, Batch 70, Loss: 0.02288999781012535\n",
      "Epoch 20, Batch 80, Loss: 0.02417139522731304\n",
      "Epoch 20, Batch 90, Loss: 0.019861450418829918\n",
      "Epoch 20, Batch 100, Loss: 0.02332800067961216\n",
      "Epoch 21, Batch 0, Loss: 0.025872521102428436\n",
      "Epoch 21, Batch 10, Loss: 0.025691619142889977\n",
      "Epoch 21, Batch 20, Loss: 0.023741696029901505\n",
      "Epoch 21, Batch 30, Loss: 0.027030840516090393\n",
      "Epoch 21, Batch 40, Loss: 0.026857223361730576\n",
      "Epoch 21, Batch 50, Loss: 0.02759862318634987\n",
      "Epoch 21, Batch 60, Loss: 0.03619062155485153\n",
      "Epoch 21, Batch 70, Loss: 0.026876375079154968\n",
      "Epoch 21, Batch 80, Loss: 0.027701862156391144\n",
      "Epoch 21, Batch 90, Loss: 0.03005390614271164\n",
      "Epoch 21, Batch 100, Loss: 0.029371678829193115\n",
      "Epoch 22, Batch 0, Loss: 0.02184658870100975\n",
      "Epoch 22, Batch 10, Loss: 0.02242802456021309\n",
      "Epoch 22, Batch 20, Loss: 0.021027445793151855\n",
      "Epoch 22, Batch 30, Loss: 0.03025120683014393\n",
      "Epoch 22, Batch 40, Loss: 0.02932611294090748\n",
      "Epoch 22, Batch 50, Loss: 0.02863881178200245\n",
      "Epoch 22, Batch 60, Loss: 0.02069755271077156\n",
      "Epoch 22, Batch 70, Loss: 0.024641292169690132\n",
      "Epoch 22, Batch 80, Loss: 0.03145093470811844\n",
      "Epoch 22, Batch 90, Loss: 0.03581863269209862\n",
      "Epoch 22, Batch 100, Loss: 0.025061555206775665\n",
      "Epoch 23, Batch 0, Loss: 0.03198624402284622\n",
      "Epoch 23, Batch 10, Loss: 0.02636333368718624\n",
      "Epoch 23, Batch 20, Loss: 0.02081567794084549\n",
      "Epoch 23, Batch 30, Loss: 0.02098935842514038\n",
      "Epoch 23, Batch 40, Loss: 0.025760959833860397\n",
      "Epoch 23, Batch 50, Loss: 0.017251932993531227\n",
      "Epoch 23, Batch 60, Loss: 0.024114230647683144\n",
      "Epoch 23, Batch 70, Loss: 0.02801797352731228\n",
      "Epoch 23, Batch 80, Loss: 0.031080052256584167\n",
      "Epoch 23, Batch 90, Loss: 0.029591718688607216\n",
      "Epoch 23, Batch 100, Loss: 0.04081888124346733\n",
      "Epoch 24, Batch 0, Loss: 0.01971345953643322\n",
      "Epoch 24, Batch 10, Loss: 0.01815895363688469\n",
      "Epoch 24, Batch 20, Loss: 0.030689673498272896\n",
      "Epoch 24, Batch 30, Loss: 0.02731318399310112\n",
      "Epoch 24, Batch 40, Loss: 0.026719041168689728\n",
      "Epoch 24, Batch 50, Loss: 0.023443805053830147\n",
      "Epoch 24, Batch 60, Loss: 0.02508825808763504\n",
      "Epoch 24, Batch 70, Loss: 0.026093589141964912\n",
      "Epoch 24, Batch 80, Loss: 0.03397146612405777\n",
      "Epoch 24, Batch 90, Loss: 0.03425157815217972\n",
      "Epoch 24, Batch 100, Loss: 0.020255371928215027\n",
      "Epoch 25, Batch 0, Loss: 0.022623445838689804\n",
      "Epoch 25, Batch 10, Loss: 0.027448464184999466\n",
      "Epoch 25, Batch 20, Loss: 0.023370448499917984\n",
      "Epoch 25, Batch 30, Loss: 0.022605769336223602\n",
      "Epoch 25, Batch 40, Loss: 0.01959439367055893\n",
      "Epoch 25, Batch 50, Loss: 0.020792748779058456\n",
      "Epoch 25, Batch 60, Loss: 0.022313838824629784\n",
      "Epoch 25, Batch 70, Loss: 0.02583257667720318\n",
      "Epoch 25, Batch 80, Loss: 0.020313872024416924\n",
      "Epoch 25, Batch 90, Loss: 0.029901262372732162\n",
      "Epoch 25, Batch 100, Loss: 0.02230149134993553\n",
      "Epoch 26, Batch 0, Loss: 0.01761879399418831\n",
      "Epoch 26, Batch 10, Loss: 0.038687147200107574\n",
      "Epoch 26, Batch 20, Loss: 0.02237512171268463\n",
      "Epoch 26, Batch 30, Loss: 0.022887732833623886\n",
      "Epoch 26, Batch 40, Loss: 0.022804275155067444\n",
      "Epoch 26, Batch 50, Loss: 0.03067413531243801\n",
      "Epoch 26, Batch 60, Loss: 0.03108367696404457\n",
      "Epoch 26, Batch 70, Loss: 0.021135950461030006\n",
      "Epoch 26, Batch 80, Loss: 0.024288073182106018\n",
      "Epoch 26, Batch 90, Loss: 0.03482167422771454\n",
      "Epoch 26, Batch 100, Loss: 0.023829402402043343\n",
      "Epoch 27, Batch 0, Loss: 0.028327802196145058\n",
      "Epoch 27, Batch 10, Loss: 0.032168909907341\n",
      "Epoch 27, Batch 20, Loss: 0.028083419427275658\n",
      "Epoch 27, Batch 30, Loss: 0.0247005857527256\n",
      "Epoch 27, Batch 40, Loss: 0.022609325125813484\n",
      "Epoch 27, Batch 50, Loss: 0.023346034809947014\n",
      "Epoch 27, Batch 60, Loss: 0.019699856638908386\n",
      "Epoch 27, Batch 70, Loss: 0.020599626004695892\n",
      "Epoch 27, Batch 80, Loss: 0.020322896540164948\n",
      "Epoch 27, Batch 90, Loss: 0.02372794970870018\n",
      "Epoch 27, Batch 100, Loss: 0.02439054474234581\n",
      "Epoch 28, Batch 0, Loss: 0.020072035491466522\n",
      "Epoch 28, Batch 10, Loss: 0.03653542324900627\n",
      "Epoch 28, Batch 20, Loss: 0.021127326413989067\n",
      "Epoch 28, Batch 30, Loss: 0.02429923042654991\n",
      "Epoch 28, Batch 40, Loss: 0.01965533010661602\n",
      "Epoch 28, Batch 50, Loss: 0.03261956572532654\n",
      "Epoch 28, Batch 60, Loss: 0.02886166423559189\n",
      "Epoch 28, Batch 70, Loss: 0.02146085910499096\n",
      "Epoch 28, Batch 80, Loss: 0.027274735271930695\n",
      "Epoch 28, Batch 90, Loss: 0.023435955867171288\n",
      "Epoch 28, Batch 100, Loss: 0.02592279762029648\n",
      "Epoch 29, Batch 0, Loss: 0.0241861455142498\n",
      "Epoch 29, Batch 10, Loss: 0.03024931810796261\n",
      "Epoch 29, Batch 20, Loss: 0.02995838038623333\n",
      "Epoch 29, Batch 30, Loss: 0.02454155497252941\n",
      "Epoch 29, Batch 40, Loss: 0.020464081317186356\n",
      "Epoch 29, Batch 50, Loss: 0.02168084681034088\n",
      "Epoch 29, Batch 60, Loss: 0.023223180323839188\n",
      "Epoch 29, Batch 70, Loss: 0.022863337770104408\n",
      "Epoch 29, Batch 80, Loss: 0.01946962997317314\n",
      "Epoch 29, Batch 90, Loss: 0.02603956311941147\n",
      "Epoch 29, Batch 100, Loss: 0.02223491296172142\n",
      "Epoch 30, Batch 0, Loss: 0.03207205981016159\n",
      "Epoch 30, Batch 10, Loss: 0.019895782694220543\n",
      "Epoch 30, Batch 20, Loss: 0.015512646175920963\n",
      "Epoch 30, Batch 30, Loss: 0.022914472967386246\n",
      "Epoch 30, Batch 40, Loss: 0.027123376727104187\n",
      "Epoch 30, Batch 50, Loss: 0.02770371176302433\n",
      "Epoch 30, Batch 60, Loss: 0.031558744609355927\n",
      "Epoch 30, Batch 70, Loss: 0.02134224772453308\n",
      "Epoch 30, Batch 80, Loss: 0.019958101212978363\n",
      "Epoch 30, Batch 90, Loss: 0.023519592359662056\n",
      "Epoch 30, Batch 100, Loss: 0.031643033027648926\n",
      "Epoch 31, Batch 0, Loss: 0.022254573181271553\n",
      "Epoch 31, Batch 10, Loss: 0.01645488105714321\n",
      "Epoch 31, Batch 20, Loss: 0.033948421478271484\n",
      "Epoch 31, Batch 30, Loss: 0.022117840126156807\n",
      "Epoch 31, Batch 40, Loss: 0.031012725085020065\n",
      "Epoch 31, Batch 50, Loss: 0.02385687083005905\n",
      "Epoch 31, Batch 60, Loss: 0.03142542019486427\n",
      "Epoch 31, Batch 70, Loss: 0.02722635492682457\n",
      "Epoch 31, Batch 80, Loss: 0.029423847794532776\n",
      "Epoch 31, Batch 90, Loss: 0.027380608022212982\n",
      "Epoch 31, Batch 100, Loss: 0.01948566548526287\n",
      "Epoch 32, Batch 0, Loss: 0.025947289541363716\n",
      "Epoch 32, Batch 10, Loss: 0.028836097568273544\n",
      "Epoch 32, Batch 20, Loss: 0.02077142894268036\n",
      "Epoch 32, Batch 30, Loss: 0.024160616099834442\n",
      "Epoch 32, Batch 40, Loss: 0.0340551994740963\n",
      "Epoch 32, Batch 50, Loss: 0.02404872700572014\n",
      "Epoch 32, Batch 60, Loss: 0.022052302956581116\n",
      "Epoch 32, Batch 70, Loss: 0.02402731217443943\n",
      "Epoch 32, Batch 80, Loss: 0.02783597819507122\n",
      "Epoch 32, Batch 90, Loss: 0.02373618632555008\n",
      "Epoch 32, Batch 100, Loss: 0.020157352089881897\n",
      "Epoch 33, Batch 0, Loss: 0.017098836600780487\n",
      "Epoch 33, Batch 10, Loss: 0.028935283422470093\n",
      "Epoch 33, Batch 20, Loss: 0.030102413147687912\n",
      "Epoch 33, Batch 30, Loss: 0.02337849698960781\n",
      "Epoch 33, Batch 40, Loss: 0.035101573914289474\n",
      "Epoch 33, Batch 50, Loss: 0.024203451350331306\n",
      "Epoch 33, Batch 60, Loss: 0.022572804242372513\n",
      "Epoch 33, Batch 70, Loss: 0.021838825196027756\n",
      "Epoch 33, Batch 80, Loss: 0.020506907254457474\n",
      "Epoch 33, Batch 90, Loss: 0.021828720346093178\n",
      "Epoch 33, Batch 100, Loss: 0.022924821823835373\n",
      "Epoch 34, Batch 0, Loss: 0.027660079300403595\n",
      "Epoch 34, Batch 10, Loss: 0.01960396207869053\n",
      "Epoch 34, Batch 20, Loss: 0.024885283783078194\n",
      "Epoch 34, Batch 30, Loss: 0.027884792536497116\n",
      "Epoch 34, Batch 40, Loss: 0.03541151434183121\n",
      "Epoch 34, Batch 50, Loss: 0.02326715737581253\n",
      "Epoch 34, Batch 60, Loss: 0.01874695159494877\n",
      "Epoch 34, Batch 70, Loss: 0.022479120641946793\n",
      "Epoch 34, Batch 80, Loss: 0.024064235389232635\n",
      "Epoch 34, Batch 90, Loss: 0.01927194371819496\n",
      "Epoch 34, Batch 100, Loss: 0.023838618770241737\n",
      "Epoch 35, Batch 0, Loss: 0.024057280272245407\n",
      "Epoch 35, Batch 10, Loss: 0.02410479448735714\n",
      "Epoch 35, Batch 20, Loss: 0.023894675076007843\n",
      "Epoch 35, Batch 30, Loss: 0.02596440538764\n",
      "Epoch 35, Batch 40, Loss: 0.024868138134479523\n",
      "Epoch 35, Batch 50, Loss: 0.02977629192173481\n",
      "Epoch 35, Batch 60, Loss: 0.026028070598840714\n",
      "Epoch 35, Batch 70, Loss: 0.033845894038677216\n",
      "Epoch 35, Batch 80, Loss: 0.026636909693479538\n",
      "Epoch 35, Batch 90, Loss: 0.027326250448822975\n",
      "Epoch 35, Batch 100, Loss: 0.029016928747296333\n",
      "Epoch 36, Batch 0, Loss: 0.021216897293925285\n",
      "Epoch 36, Batch 10, Loss: 0.021756049245595932\n",
      "Epoch 36, Batch 20, Loss: 0.028913257643580437\n",
      "Epoch 36, Batch 30, Loss: 0.017829613760113716\n",
      "Epoch 36, Batch 40, Loss: 0.030297361314296722\n",
      "Epoch 36, Batch 50, Loss: 0.02648274227976799\n",
      "Epoch 36, Batch 60, Loss: 0.025219064205884933\n",
      "Epoch 36, Batch 70, Loss: 0.022842807695269585\n",
      "Epoch 36, Batch 80, Loss: 0.032957419753074646\n",
      "Epoch 36, Batch 90, Loss: 0.02225772850215435\n",
      "Epoch 36, Batch 100, Loss: 0.030163409188389778\n",
      "Epoch 37, Batch 0, Loss: 0.022881394252181053\n",
      "Epoch 37, Batch 10, Loss: 0.02196609228849411\n",
      "Epoch 37, Batch 20, Loss: 0.02337345853447914\n",
      "Epoch 37, Batch 30, Loss: 0.024822033941745758\n",
      "Epoch 37, Batch 40, Loss: 0.021407002583146095\n",
      "Epoch 37, Batch 50, Loss: 0.014936234802007675\n",
      "Epoch 37, Batch 60, Loss: 0.020113013684749603\n",
      "Epoch 37, Batch 70, Loss: 0.02192467823624611\n",
      "Epoch 37, Batch 80, Loss: 0.020846089348196983\n",
      "Epoch 37, Batch 90, Loss: 0.029315076768398285\n",
      "Epoch 37, Batch 100, Loss: 0.019279390573501587\n",
      "Epoch 38, Batch 0, Loss: 0.026702340692281723\n",
      "Epoch 38, Batch 10, Loss: 0.02234066277742386\n",
      "Epoch 38, Batch 20, Loss: 0.02129414863884449\n",
      "Epoch 38, Batch 30, Loss: 0.021994883194565773\n",
      "Epoch 38, Batch 40, Loss: 0.028030354529619217\n",
      "Epoch 38, Batch 50, Loss: 0.02334573306143284\n",
      "Epoch 38, Batch 60, Loss: 0.022107401862740517\n",
      "Epoch 38, Batch 70, Loss: 0.032967232167720795\n",
      "Epoch 38, Batch 80, Loss: 0.027732476592063904\n",
      "Epoch 38, Batch 90, Loss: 0.015603296458721161\n",
      "Epoch 38, Batch 100, Loss: 0.020515691488981247\n",
      "Epoch 39, Batch 0, Loss: 0.021236596629023552\n",
      "Epoch 39, Batch 10, Loss: 0.023247351869940758\n",
      "Epoch 39, Batch 20, Loss: 0.02282470092177391\n",
      "Epoch 39, Batch 30, Loss: 0.022023504599928856\n",
      "Epoch 39, Batch 40, Loss: 0.01886037178337574\n",
      "Epoch 39, Batch 50, Loss: 0.022335242480039597\n",
      "Epoch 39, Batch 60, Loss: 0.02695561572909355\n",
      "Epoch 39, Batch 70, Loss: 0.032783813774585724\n",
      "Epoch 39, Batch 80, Loss: 0.020870236679911613\n",
      "Epoch 39, Batch 90, Loss: 0.027228472754359245\n",
      "Epoch 39, Batch 100, Loss: 0.0240604467689991\n",
      "Epoch 40, Batch 0, Loss: 0.027064992114901543\n",
      "Epoch 40, Batch 10, Loss: 0.022472236305475235\n",
      "Epoch 40, Batch 20, Loss: 0.02550116926431656\n",
      "Epoch 40, Batch 30, Loss: 0.01902606710791588\n",
      "Epoch 40, Batch 40, Loss: 0.023906337097287178\n",
      "Epoch 40, Batch 50, Loss: 0.029511429369449615\n",
      "Epoch 40, Batch 60, Loss: 0.027466105297207832\n",
      "Epoch 40, Batch 70, Loss: 0.030587535351514816\n",
      "Epoch 40, Batch 80, Loss: 0.02414536103606224\n",
      "Epoch 40, Batch 90, Loss: 0.027757063508033752\n",
      "Epoch 40, Batch 100, Loss: 0.02489234134554863\n",
      "Epoch 41, Batch 0, Loss: 0.02285628207027912\n",
      "Epoch 41, Batch 10, Loss: 0.02523568645119667\n",
      "Epoch 41, Batch 20, Loss: 0.029658475890755653\n",
      "Epoch 41, Batch 30, Loss: 0.03086598962545395\n",
      "Epoch 41, Batch 40, Loss: 0.027936717495322227\n",
      "Epoch 41, Batch 50, Loss: 0.02700183540582657\n",
      "Epoch 41, Batch 60, Loss: 0.026949254795908928\n",
      "Epoch 41, Batch 70, Loss: 0.023922866210341454\n",
      "Epoch 41, Batch 80, Loss: 0.025165051221847534\n",
      "Epoch 41, Batch 90, Loss: 0.031143831089138985\n",
      "Epoch 41, Batch 100, Loss: 0.019040094688534737\n",
      "Epoch 42, Batch 0, Loss: 0.01690134033560753\n",
      "Epoch 42, Batch 10, Loss: 0.023944642394781113\n",
      "Epoch 42, Batch 20, Loss: 0.026114877313375473\n",
      "Epoch 42, Batch 30, Loss: 0.020042799413204193\n",
      "Epoch 42, Batch 40, Loss: 0.022363843396306038\n",
      "Epoch 42, Batch 50, Loss: 0.02897469326853752\n",
      "Epoch 42, Batch 60, Loss: 0.03197754546999931\n",
      "Epoch 42, Batch 70, Loss: 0.019348226487636566\n",
      "Epoch 42, Batch 80, Loss: 0.026768771931529045\n",
      "Epoch 42, Batch 90, Loss: 0.019615964964032173\n",
      "Epoch 42, Batch 100, Loss: 0.031199844554066658\n",
      "Epoch 43, Batch 0, Loss: 0.016685668379068375\n",
      "Epoch 43, Batch 10, Loss: 0.01761028729379177\n",
      "Epoch 43, Batch 20, Loss: 0.025376062840223312\n",
      "Epoch 43, Batch 30, Loss: 0.02296634018421173\n",
      "Epoch 43, Batch 40, Loss: 0.020920969545841217\n",
      "Epoch 43, Batch 50, Loss: 0.022864030674099922\n",
      "Epoch 43, Batch 60, Loss: 0.028160322457551956\n",
      "Epoch 43, Batch 70, Loss: 0.023136520758271217\n",
      "Epoch 43, Batch 80, Loss: 0.022111114114522934\n",
      "Epoch 43, Batch 90, Loss: 0.02143329568207264\n",
      "Epoch 43, Batch 100, Loss: 0.018593911081552505\n",
      "Epoch 44, Batch 0, Loss: 0.03571980446577072\n",
      "Epoch 44, Batch 10, Loss: 0.021176576614379883\n",
      "Epoch 44, Batch 20, Loss: 0.01913050189614296\n",
      "Epoch 44, Batch 30, Loss: 0.02890186756849289\n",
      "Epoch 44, Batch 40, Loss: 0.022918086498975754\n",
      "Epoch 44, Batch 50, Loss: 0.02794516831636429\n",
      "Epoch 44, Batch 60, Loss: 0.026440994814038277\n",
      "Epoch 44, Batch 70, Loss: 0.02217303402721882\n",
      "Epoch 44, Batch 80, Loss: 0.025266099721193314\n",
      "Epoch 44, Batch 90, Loss: 0.021007908508181572\n",
      "Epoch 44, Batch 100, Loss: 0.021209031343460083\n",
      "Epoch 45, Batch 0, Loss: 0.02840353175997734\n",
      "Epoch 45, Batch 10, Loss: 0.025340527296066284\n",
      "Epoch 45, Batch 20, Loss: 0.02669188752770424\n",
      "Epoch 45, Batch 30, Loss: 0.02984851412475109\n",
      "Epoch 45, Batch 40, Loss: 0.026803690940141678\n",
      "Epoch 45, Batch 50, Loss: 0.02955452725291252\n",
      "Epoch 45, Batch 60, Loss: 0.020535282790660858\n",
      "Epoch 45, Batch 70, Loss: 0.03161393478512764\n",
      "Epoch 45, Batch 80, Loss: 0.025425197556614876\n",
      "Epoch 45, Batch 90, Loss: 0.022113654762506485\n",
      "Epoch 45, Batch 100, Loss: 0.02750682458281517\n",
      "Epoch 46, Batch 0, Loss: 0.02460591122508049\n",
      "Epoch 46, Batch 10, Loss: 0.028658663854002953\n",
      "Epoch 46, Batch 20, Loss: 0.019827090203762054\n",
      "Epoch 46, Batch 30, Loss: 0.02210797183215618\n",
      "Epoch 46, Batch 40, Loss: 0.02210291661322117\n",
      "Epoch 46, Batch 50, Loss: 0.021732831373810768\n",
      "Epoch 46, Batch 60, Loss: 0.020494766533374786\n",
      "Epoch 46, Batch 70, Loss: 0.021514682099223137\n",
      "Epoch 46, Batch 80, Loss: 0.021133266389369965\n",
      "Epoch 46, Batch 90, Loss: 0.019890442490577698\n",
      "Epoch 46, Batch 100, Loss: 0.03021075576543808\n",
      "Epoch 47, Batch 0, Loss: 0.0231853686273098\n",
      "Epoch 47, Batch 10, Loss: 0.031401604413986206\n",
      "Epoch 47, Batch 20, Loss: 0.02910025231540203\n",
      "Epoch 47, Batch 30, Loss: 0.02394425868988037\n",
      "Epoch 47, Batch 40, Loss: 0.02618066780269146\n",
      "Epoch 47, Batch 50, Loss: 0.021909218281507492\n",
      "Epoch 47, Batch 60, Loss: 0.02133513055741787\n",
      "Epoch 47, Batch 70, Loss: 0.03431018069386482\n",
      "Epoch 47, Batch 80, Loss: 0.023646652698516846\n",
      "Epoch 47, Batch 90, Loss: 0.023610834032297134\n",
      "Epoch 47, Batch 100, Loss: 0.01933848299086094\n",
      "Epoch 48, Batch 0, Loss: 0.021447530016303062\n",
      "Epoch 48, Batch 10, Loss: 0.026673272252082825\n",
      "Epoch 48, Batch 20, Loss: 0.028111224994063377\n",
      "Epoch 48, Batch 30, Loss: 0.021130681037902832\n",
      "Epoch 48, Batch 40, Loss: 0.017718108370900154\n",
      "Epoch 48, Batch 50, Loss: 0.022379813715815544\n",
      "Epoch 48, Batch 60, Loss: 0.0256771519780159\n",
      "Epoch 48, Batch 70, Loss: 0.026440635323524475\n",
      "Epoch 48, Batch 80, Loss: 0.023605523630976677\n",
      "Epoch 48, Batch 90, Loss: 0.03143968805670738\n",
      "Epoch 48, Batch 100, Loss: 0.03037405014038086\n",
      "Epoch 49, Batch 0, Loss: 0.023674041032791138\n",
      "Epoch 49, Batch 10, Loss: 0.026922207325696945\n",
      "Epoch 49, Batch 20, Loss: 0.02920488826930523\n",
      "Epoch 49, Batch 30, Loss: 0.021645594388246536\n",
      "Epoch 49, Batch 40, Loss: 0.022157886996865273\n",
      "Epoch 49, Batch 50, Loss: 0.0328785739839077\n",
      "Epoch 49, Batch 60, Loss: 0.02370111644268036\n",
      "Epoch 49, Batch 70, Loss: 0.033713143318891525\n",
      "Epoch 49, Batch 80, Loss: 0.023307355120778084\n",
      "Epoch 49, Batch 90, Loss: 0.02372605726122856\n",
      "Epoch 49, Batch 100, Loss: 0.025600163266062737\n",
      "Epoch 50, Batch 0, Loss: 0.018000390380620956\n",
      "Epoch 50, Batch 10, Loss: 0.02270537056028843\n",
      "Epoch 50, Batch 20, Loss: 0.029733959585428238\n",
      "Epoch 50, Batch 30, Loss: 0.0299395564943552\n",
      "Epoch 50, Batch 40, Loss: 0.023511862382292747\n",
      "Epoch 50, Batch 50, Loss: 0.021098539233207703\n",
      "Epoch 50, Batch 60, Loss: 0.018195705488324165\n",
      "Epoch 50, Batch 70, Loss: 0.02999579906463623\n",
      "Epoch 50, Batch 80, Loss: 0.027154603973031044\n",
      "Epoch 50, Batch 90, Loss: 0.02350747585296631\n",
      "Epoch 50, Batch 100, Loss: 0.022739142179489136\n",
      "Epoch 51, Batch 0, Loss: 0.028920896351337433\n",
      "Epoch 51, Batch 10, Loss: 0.02485022135078907\n",
      "Epoch 51, Batch 20, Loss: 0.019810235127806664\n",
      "Epoch 51, Batch 30, Loss: 0.026201527565717697\n",
      "Epoch 51, Batch 40, Loss: 0.018431531265378\n",
      "Epoch 51, Batch 50, Loss: 0.02567412704229355\n",
      "Epoch 51, Batch 60, Loss: 0.022663716226816177\n",
      "Epoch 51, Batch 70, Loss: 0.022635959088802338\n",
      "Epoch 51, Batch 80, Loss: 0.017706401646137238\n",
      "Epoch 51, Batch 90, Loss: 0.027736250311136246\n",
      "Epoch 51, Batch 100, Loss: 0.01738208718597889\n",
      "Epoch 52, Batch 0, Loss: 0.02027214877307415\n",
      "Epoch 52, Batch 10, Loss: 0.026468656957149506\n",
      "Epoch 52, Batch 20, Loss: 0.023470299318432808\n",
      "Epoch 52, Batch 30, Loss: 0.018469279631972313\n",
      "Epoch 52, Batch 40, Loss: 0.02634832076728344\n",
      "Epoch 52, Batch 50, Loss: 0.028962304815649986\n",
      "Epoch 52, Batch 60, Loss: 0.02370157279074192\n",
      "Epoch 52, Batch 70, Loss: 0.026917587965726852\n",
      "Epoch 52, Batch 80, Loss: 0.025675280019640923\n",
      "Epoch 52, Batch 90, Loss: 0.021995311602950096\n",
      "Epoch 52, Batch 100, Loss: 0.033551670610904694\n",
      "Epoch 53, Batch 0, Loss: 0.027413304895162582\n",
      "Epoch 53, Batch 10, Loss: 0.016349459066987038\n",
      "Epoch 53, Batch 20, Loss: 0.022782359272241592\n",
      "Epoch 53, Batch 30, Loss: 0.02337866835296154\n",
      "Epoch 53, Batch 40, Loss: 0.021243710070848465\n",
      "Epoch 53, Batch 50, Loss: 0.023013489320874214\n",
      "Epoch 53, Batch 60, Loss: 0.020525313913822174\n",
      "Epoch 53, Batch 70, Loss: 0.020842887461185455\n",
      "Epoch 53, Batch 80, Loss: 0.02583657205104828\n",
      "Epoch 53, Batch 90, Loss: 0.02410932257771492\n",
      "Epoch 53, Batch 100, Loss: 0.021538158878684044\n",
      "Epoch 54, Batch 0, Loss: 0.023537809029221535\n",
      "Epoch 54, Batch 10, Loss: 0.02546972967684269\n",
      "Epoch 54, Batch 20, Loss: 0.023547638207674026\n",
      "Epoch 54, Batch 30, Loss: 0.02218102104961872\n",
      "Epoch 54, Batch 40, Loss: 0.02729819528758526\n",
      "Epoch 54, Batch 50, Loss: 0.027770940214395523\n",
      "Epoch 54, Batch 60, Loss: 0.01684066653251648\n",
      "Epoch 54, Batch 70, Loss: 0.020278962329030037\n",
      "Epoch 54, Batch 80, Loss: 0.02351359836757183\n",
      "Epoch 54, Batch 90, Loss: 0.02346840314567089\n",
      "Epoch 54, Batch 100, Loss: 0.01990179531276226\n",
      "Epoch 55, Batch 0, Loss: 0.020423781126737595\n",
      "Epoch 55, Batch 10, Loss: 0.023463159799575806\n",
      "Epoch 55, Batch 20, Loss: 0.019430365413427353\n",
      "Epoch 55, Batch 30, Loss: 0.0207957960665226\n",
      "Epoch 55, Batch 40, Loss: 0.02805536612868309\n",
      "Epoch 55, Batch 50, Loss: 0.024532705545425415\n",
      "Epoch 55, Batch 60, Loss: 0.02165418118238449\n",
      "Epoch 55, Batch 70, Loss: 0.019603306427598\n",
      "Epoch 55, Batch 80, Loss: 0.03243265300989151\n",
      "Epoch 55, Batch 90, Loss: 0.03853800147771835\n",
      "Epoch 55, Batch 100, Loss: 0.019246257841587067\n",
      "Epoch 56, Batch 0, Loss: 0.021261336281895638\n",
      "Epoch 56, Batch 10, Loss: 0.02603727951645851\n",
      "Epoch 56, Batch 20, Loss: 0.02423679269850254\n",
      "Epoch 56, Batch 30, Loss: 0.026210607960820198\n",
      "Epoch 56, Batch 40, Loss: 0.016333511099219322\n",
      "Epoch 56, Batch 50, Loss: 0.03255828097462654\n",
      "Epoch 56, Batch 60, Loss: 0.02599840611219406\n",
      "Epoch 56, Batch 70, Loss: 0.02526717446744442\n",
      "Epoch 56, Batch 80, Loss: 0.02810540422797203\n",
      "Epoch 56, Batch 90, Loss: 0.03502732142806053\n",
      "Epoch 56, Batch 100, Loss: 0.02350291609764099\n",
      "Epoch 57, Batch 0, Loss: 0.022753018885850906\n",
      "Epoch 57, Batch 10, Loss: 0.016419002786278725\n",
      "Epoch 57, Batch 20, Loss: 0.017839573323726654\n",
      "Epoch 57, Batch 30, Loss: 0.022464638575911522\n",
      "Epoch 57, Batch 40, Loss: 0.024248681962490082\n",
      "Epoch 57, Batch 50, Loss: 0.025738056749105453\n",
      "Epoch 57, Batch 60, Loss: 0.04090316593647003\n",
      "Epoch 57, Batch 70, Loss: 0.020571118220686913\n",
      "Epoch 57, Batch 80, Loss: 0.021258803084492683\n",
      "Epoch 57, Batch 90, Loss: 0.023507285863161087\n",
      "Epoch 57, Batch 100, Loss: 0.027668550610542297\n",
      "Epoch 58, Batch 0, Loss: 0.03049371764063835\n",
      "Epoch 58, Batch 10, Loss: 0.027872305363416672\n",
      "Epoch 58, Batch 20, Loss: 0.021700354292988777\n",
      "Epoch 58, Batch 30, Loss: 0.04267127811908722\n",
      "Epoch 58, Batch 40, Loss: 0.019501926377415657\n",
      "Epoch 58, Batch 50, Loss: 0.016223490238189697\n",
      "Epoch 58, Batch 60, Loss: 0.026191458106040955\n",
      "Epoch 58, Batch 70, Loss: 0.026854917407035828\n",
      "Epoch 58, Batch 80, Loss: 0.031725749373435974\n",
      "Epoch 58, Batch 90, Loss: 0.0314924456179142\n",
      "Epoch 58, Batch 100, Loss: 0.021054666489362717\n",
      "Epoch 59, Batch 0, Loss: 0.019922995939850807\n",
      "Epoch 59, Batch 10, Loss: 0.017603382468223572\n",
      "Epoch 59, Batch 20, Loss: 0.016903705894947052\n",
      "Epoch 59, Batch 30, Loss: 0.025484390556812286\n",
      "Epoch 59, Batch 40, Loss: 0.02360542304813862\n",
      "Epoch 59, Batch 50, Loss: 0.020965125411748886\n",
      "Epoch 59, Batch 60, Loss: 0.020458294078707695\n",
      "Epoch 59, Batch 70, Loss: 0.0314154289662838\n",
      "Epoch 59, Batch 80, Loss: 0.022254714742302895\n",
      "Epoch 59, Batch 90, Loss: 0.02098942920565605\n",
      "Epoch 59, Batch 100, Loss: 0.023616943508386612\n",
      "Epoch 60, Batch 0, Loss: 0.01795848086476326\n",
      "Epoch 60, Batch 10, Loss: 0.015854237601161003\n",
      "Epoch 60, Batch 20, Loss: 0.020600512623786926\n",
      "Epoch 60, Batch 30, Loss: 0.01883835159242153\n",
      "Epoch 60, Batch 40, Loss: 0.02852792479097843\n",
      "Epoch 60, Batch 50, Loss: 0.02481290139257908\n",
      "Epoch 60, Batch 60, Loss: 0.020657218992710114\n",
      "Epoch 60, Batch 70, Loss: 0.021091220900416374\n",
      "Epoch 60, Batch 80, Loss: 0.029100295156240463\n",
      "Epoch 60, Batch 90, Loss: 0.022351853549480438\n",
      "Epoch 60, Batch 100, Loss: 0.022124528884887695\n",
      "Epoch 61, Batch 0, Loss: 0.03183690458536148\n",
      "Epoch 61, Batch 10, Loss: 0.037471525371074677\n",
      "Epoch 61, Batch 20, Loss: 0.028756514191627502\n",
      "Epoch 61, Batch 30, Loss: 0.01858040690422058\n",
      "Epoch 61, Batch 40, Loss: 0.023478558287024498\n",
      "Epoch 61, Batch 50, Loss: 0.025041308254003525\n",
      "Epoch 61, Batch 60, Loss: 0.023372510448098183\n",
      "Epoch 61, Batch 70, Loss: 0.02443326637148857\n",
      "Epoch 61, Batch 80, Loss: 0.023124806582927704\n",
      "Epoch 61, Batch 90, Loss: 0.02556288056075573\n",
      "Epoch 61, Batch 100, Loss: 0.024852804839611053\n",
      "Epoch 62, Batch 0, Loss: 0.015891382470726967\n",
      "Epoch 62, Batch 10, Loss: 0.02291763387620449\n",
      "Epoch 62, Batch 20, Loss: 0.026023678481578827\n",
      "Epoch 62, Batch 30, Loss: 0.020842071622610092\n",
      "Epoch 62, Batch 40, Loss: 0.024685930460691452\n",
      "Epoch 62, Batch 50, Loss: 0.030202068388462067\n",
      "Epoch 62, Batch 60, Loss: 0.01991717517375946\n",
      "Epoch 62, Batch 70, Loss: 0.02676139399409294\n",
      "Epoch 62, Batch 80, Loss: 0.025438666343688965\n",
      "Epoch 62, Batch 90, Loss: 0.02002706006169319\n",
      "Epoch 62, Batch 100, Loss: 0.02128487452864647\n",
      "Epoch 63, Batch 0, Loss: 0.025307470932602882\n",
      "Epoch 63, Batch 10, Loss: 0.02532539889216423\n",
      "Epoch 63, Batch 20, Loss: 0.025334492325782776\n",
      "Epoch 63, Batch 30, Loss: 0.025948217138648033\n",
      "Epoch 63, Batch 40, Loss: 0.028935173526406288\n",
      "Epoch 63, Batch 50, Loss: 0.019982462748885155\n",
      "Epoch 63, Batch 60, Loss: 0.017284579575061798\n",
      "Epoch 63, Batch 70, Loss: 0.0209648460149765\n",
      "Epoch 63, Batch 80, Loss: 0.018897313624620438\n",
      "Epoch 63, Batch 90, Loss: 0.021683469414711\n",
      "Epoch 63, Batch 100, Loss: 0.02344703860580921\n",
      "Epoch 64, Batch 0, Loss: 0.02080930769443512\n",
      "Epoch 64, Batch 10, Loss: 0.02520802430808544\n",
      "Epoch 64, Batch 20, Loss: 0.02643599733710289\n",
      "Epoch 64, Batch 30, Loss: 0.020253339782357216\n",
      "Epoch 64, Batch 40, Loss: 0.032787442207336426\n",
      "Epoch 64, Batch 50, Loss: 0.026842691004276276\n",
      "Epoch 64, Batch 60, Loss: 0.016215186566114426\n",
      "Epoch 64, Batch 70, Loss: 0.017854411154985428\n",
      "Epoch 64, Batch 80, Loss: 0.02840062603354454\n",
      "Epoch 64, Batch 90, Loss: 0.01890813559293747\n",
      "Epoch 64, Batch 100, Loss: 0.018714340403676033\n",
      "Epoch 65, Batch 0, Loss: 0.022677090018987656\n",
      "Epoch 65, Batch 10, Loss: 0.018379898741841316\n",
      "Epoch 65, Batch 20, Loss: 0.03230627253651619\n",
      "Epoch 65, Batch 30, Loss: 0.020225536078214645\n",
      "Epoch 65, Batch 40, Loss: 0.017701812088489532\n",
      "Epoch 65, Batch 50, Loss: 0.020836157724261284\n",
      "Epoch 65, Batch 60, Loss: 0.022280871868133545\n",
      "Epoch 65, Batch 70, Loss: 0.019687335938215256\n",
      "Epoch 65, Batch 80, Loss: 0.029721073806285858\n",
      "Epoch 65, Batch 90, Loss: 0.020586317405104637\n",
      "Epoch 65, Batch 100, Loss: 0.017460448667407036\n",
      "Epoch 66, Batch 0, Loss: 0.020394325256347656\n",
      "Epoch 66, Batch 10, Loss: 0.024358931928873062\n",
      "Epoch 66, Batch 20, Loss: 0.022136474028229713\n",
      "Epoch 66, Batch 30, Loss: 0.02980443276464939\n",
      "Epoch 66, Batch 40, Loss: 0.02662649378180504\n",
      "Epoch 66, Batch 50, Loss: 0.01983540877699852\n",
      "Epoch 66, Batch 60, Loss: 0.021748840808868408\n",
      "Epoch 66, Batch 70, Loss: 0.022216636687517166\n",
      "Epoch 66, Batch 80, Loss: 0.021574443206191063\n",
      "Epoch 66, Batch 90, Loss: 0.027281295508146286\n",
      "Epoch 66, Batch 100, Loss: 0.018513906747102737\n",
      "Epoch 67, Batch 0, Loss: 0.019400227814912796\n",
      "Epoch 67, Batch 10, Loss: 0.021072475239634514\n",
      "Epoch 67, Batch 20, Loss: 0.02277846820652485\n",
      "Epoch 67, Batch 30, Loss: 0.021135911345481873\n",
      "Epoch 67, Batch 40, Loss: 0.029449893161654472\n",
      "Epoch 67, Batch 50, Loss: 0.020480813458561897\n",
      "Epoch 67, Batch 60, Loss: 0.026210201904177666\n",
      "Epoch 67, Batch 70, Loss: 0.024601347744464874\n",
      "Epoch 67, Batch 80, Loss: 0.021002240478992462\n",
      "Epoch 67, Batch 90, Loss: 0.022646095603704453\n",
      "Epoch 67, Batch 100, Loss: 0.021374661475419998\n",
      "Epoch 68, Batch 0, Loss: 0.018172405660152435\n",
      "Epoch 68, Batch 10, Loss: 0.01984255015850067\n",
      "Epoch 68, Batch 20, Loss: 0.02631305903196335\n",
      "Epoch 68, Batch 30, Loss: 0.02206926792860031\n",
      "Epoch 68, Batch 40, Loss: 0.021541280671954155\n",
      "Epoch 68, Batch 50, Loss: 0.020204078406095505\n",
      "Epoch 68, Batch 60, Loss: 0.019476423040032387\n",
      "Epoch 68, Batch 70, Loss: 0.02960675023496151\n",
      "Epoch 68, Batch 80, Loss: 0.024047963321208954\n",
      "Epoch 68, Batch 90, Loss: 0.02503795176744461\n",
      "Epoch 68, Batch 100, Loss: 0.03276098147034645\n",
      "Epoch 69, Batch 0, Loss: 0.021208753809332848\n",
      "Epoch 69, Batch 10, Loss: 0.021352455019950867\n",
      "Epoch 69, Batch 20, Loss: 0.02337072417140007\n",
      "Epoch 69, Batch 30, Loss: 0.026460837572813034\n",
      "Epoch 69, Batch 40, Loss: 0.022662745788693428\n",
      "Epoch 69, Batch 50, Loss: 0.02340730093419552\n",
      "Epoch 69, Batch 60, Loss: 0.018585728481411934\n",
      "Epoch 69, Batch 70, Loss: 0.023717856034636497\n",
      "Epoch 69, Batch 80, Loss: 0.020240675657987595\n",
      "Epoch 69, Batch 90, Loss: 0.01854727976024151\n",
      "Epoch 69, Batch 100, Loss: 0.023572610691189766\n",
      "Epoch 70, Batch 0, Loss: 0.028621502220630646\n",
      "Epoch 70, Batch 10, Loss: 0.01873999834060669\n",
      "Epoch 70, Batch 20, Loss: 0.021375399082899094\n",
      "Epoch 70, Batch 30, Loss: 0.019567066803574562\n",
      "Epoch 70, Batch 40, Loss: 0.018749825656414032\n",
      "Epoch 70, Batch 50, Loss: 0.032672908157110214\n",
      "Epoch 70, Batch 60, Loss: 0.01569240912795067\n",
      "Epoch 70, Batch 70, Loss: 0.021560553461313248\n",
      "Epoch 70, Batch 80, Loss: 0.022301917895674706\n",
      "Epoch 70, Batch 90, Loss: 0.022446680814027786\n",
      "Epoch 70, Batch 100, Loss: 0.019536854699254036\n",
      "Epoch 71, Batch 0, Loss: 0.020459014922380447\n",
      "Epoch 71, Batch 10, Loss: 0.028719265013933182\n",
      "Epoch 71, Batch 20, Loss: 0.022319458425045013\n",
      "Epoch 71, Batch 30, Loss: 0.027482856065034866\n",
      "Epoch 71, Batch 40, Loss: 0.02978067472577095\n",
      "Epoch 71, Batch 50, Loss: 0.025899702683091164\n",
      "Epoch 71, Batch 60, Loss: 0.029171504080295563\n",
      "Epoch 71, Batch 70, Loss: 0.023033564910292625\n",
      "Epoch 71, Batch 80, Loss: 0.0228557251393795\n",
      "Epoch 71, Batch 90, Loss: 0.017943285405635834\n",
      "Epoch 71, Batch 100, Loss: 0.02245902828872204\n",
      "Epoch 72, Batch 0, Loss: 0.01936115138232708\n",
      "Epoch 72, Batch 10, Loss: 0.027300134301185608\n",
      "Epoch 72, Batch 20, Loss: 0.02512665092945099\n",
      "Epoch 72, Batch 30, Loss: 0.029584048315882683\n",
      "Epoch 72, Batch 40, Loss: 0.02293522097170353\n",
      "Epoch 72, Batch 50, Loss: 0.028415178880095482\n",
      "Epoch 72, Batch 60, Loss: 0.03217250481247902\n",
      "Epoch 72, Batch 70, Loss: 0.020441874861717224\n",
      "Epoch 72, Batch 80, Loss: 0.02345435693860054\n",
      "Epoch 72, Batch 90, Loss: 0.02359975501894951\n",
      "Epoch 72, Batch 100, Loss: 0.03267382085323334\n",
      "Epoch 73, Batch 0, Loss: 0.029618993401527405\n",
      "Epoch 73, Batch 10, Loss: 0.019735537469387054\n",
      "Epoch 73, Batch 20, Loss: 0.020471984520554543\n",
      "Epoch 73, Batch 30, Loss: 0.023559853434562683\n",
      "Epoch 73, Batch 40, Loss: 0.036248378455638885\n",
      "Epoch 73, Batch 50, Loss: 0.022293588146567345\n",
      "Epoch 73, Batch 60, Loss: 0.02530847117304802\n",
      "Epoch 73, Batch 70, Loss: 0.024932516738772392\n",
      "Epoch 73, Batch 80, Loss: 0.03417922183871269\n",
      "Epoch 73, Batch 90, Loss: 0.022213032469153404\n",
      "Epoch 73, Batch 100, Loss: 0.01787675730884075\n",
      "Epoch 74, Batch 0, Loss: 0.020970715209841728\n",
      "Epoch 74, Batch 10, Loss: 0.026753142476081848\n",
      "Epoch 74, Batch 20, Loss: 0.02364741824567318\n",
      "Epoch 74, Batch 30, Loss: 0.0288890078663826\n",
      "Epoch 74, Batch 40, Loss: 0.021682029590010643\n",
      "Epoch 74, Batch 50, Loss: 0.02080293744802475\n",
      "Epoch 74, Batch 60, Loss: 0.02934851497411728\n",
      "Epoch 74, Batch 70, Loss: 0.022918954491615295\n",
      "Epoch 74, Batch 80, Loss: 0.026439905166625977\n",
      "Epoch 74, Batch 90, Loss: 0.019949592649936676\n",
      "Epoch 74, Batch 100, Loss: 0.02249804139137268\n",
      "Epoch 75, Batch 0, Loss: 0.025565342977643013\n",
      "Epoch 75, Batch 10, Loss: 0.024864554405212402\n",
      "Epoch 75, Batch 20, Loss: 0.028267692774534225\n",
      "Epoch 75, Batch 30, Loss: 0.025713590905070305\n",
      "Epoch 75, Batch 40, Loss: 0.020733578130602837\n",
      "Epoch 75, Batch 50, Loss: 0.018862120807170868\n",
      "Epoch 75, Batch 60, Loss: 0.015493353828787804\n",
      "Epoch 75, Batch 70, Loss: 0.020283441990613937\n",
      "Epoch 75, Batch 80, Loss: 0.029615070670843124\n",
      "Epoch 75, Batch 90, Loss: 0.030271748080849648\n",
      "Epoch 75, Batch 100, Loss: 0.02502317540347576\n",
      "Epoch 76, Batch 0, Loss: 0.02103683166205883\n",
      "Epoch 76, Batch 10, Loss: 0.01702950894832611\n",
      "Epoch 76, Batch 20, Loss: 0.02767261676490307\n",
      "Epoch 76, Batch 30, Loss: 0.025314534083008766\n",
      "Epoch 76, Batch 40, Loss: 0.019317861646413803\n",
      "Epoch 76, Batch 50, Loss: 0.02123287320137024\n",
      "Epoch 76, Batch 60, Loss: 0.021930117160081863\n",
      "Epoch 76, Batch 70, Loss: 0.023097451776266098\n",
      "Epoch 76, Batch 80, Loss: 0.027033643797039986\n",
      "Epoch 76, Batch 90, Loss: 0.022117288783192635\n",
      "Epoch 76, Batch 100, Loss: 0.025578495115041733\n",
      "Epoch 77, Batch 0, Loss: 0.029556531459093094\n",
      "Epoch 77, Batch 10, Loss: 0.023351550102233887\n",
      "Epoch 77, Batch 20, Loss: 0.028067758306860924\n",
      "Epoch 77, Batch 30, Loss: 0.02321300096809864\n",
      "Epoch 77, Batch 40, Loss: 0.02009626477956772\n",
      "Epoch 77, Batch 50, Loss: 0.017946327105164528\n",
      "Epoch 77, Batch 60, Loss: 0.021900517866015434\n",
      "Epoch 77, Batch 70, Loss: 0.038883596658706665\n",
      "Epoch 77, Batch 80, Loss: 0.022822320461273193\n",
      "Epoch 77, Batch 90, Loss: 0.026392336934804916\n",
      "Epoch 77, Batch 100, Loss: 0.02006051316857338\n",
      "Epoch 78, Batch 0, Loss: 0.025566481053829193\n",
      "Epoch 78, Batch 10, Loss: 0.025589901953935623\n",
      "Epoch 78, Batch 20, Loss: 0.017076699063181877\n",
      "Epoch 78, Batch 30, Loss: 0.026785455644130707\n",
      "Epoch 78, Batch 40, Loss: 0.02200794219970703\n",
      "Epoch 78, Batch 50, Loss: 0.024695098400115967\n",
      "Epoch 78, Batch 60, Loss: 0.028025835752487183\n",
      "Epoch 78, Batch 70, Loss: 0.017391204833984375\n",
      "Epoch 78, Batch 80, Loss: 0.02895316854119301\n",
      "Epoch 78, Batch 90, Loss: 0.031959641724824905\n",
      "Epoch 78, Batch 100, Loss: 0.018368501216173172\n",
      "Epoch 79, Batch 0, Loss: 0.027852678671479225\n",
      "Epoch 79, Batch 10, Loss: 0.021706320345401764\n",
      "Epoch 79, Batch 20, Loss: 0.016800321638584137\n",
      "Epoch 79, Batch 30, Loss: 0.029589826241135597\n",
      "Epoch 79, Batch 40, Loss: 0.031618840992450714\n",
      "Epoch 79, Batch 50, Loss: 0.01638285629451275\n",
      "Epoch 79, Batch 60, Loss: 0.019484858959913254\n",
      "Epoch 79, Batch 70, Loss: 0.023358620703220367\n",
      "Epoch 79, Batch 80, Loss: 0.02461938001215458\n",
      "Epoch 79, Batch 90, Loss: 0.026962555944919586\n",
      "Epoch 79, Batch 100, Loss: 0.0330047532916069\n",
      "Epoch 80, Batch 0, Loss: 0.026566144078969955\n",
      "Epoch 80, Batch 10, Loss: 0.024745263159275055\n",
      "Epoch 80, Batch 20, Loss: 0.022362295538187027\n",
      "Epoch 80, Batch 30, Loss: 0.019488990306854248\n",
      "Epoch 80, Batch 40, Loss: 0.023268822580575943\n",
      "Epoch 80, Batch 50, Loss: 0.01991993933916092\n",
      "Epoch 80, Batch 60, Loss: 0.022730085998773575\n",
      "Epoch 80, Batch 70, Loss: 0.01850811019539833\n",
      "Epoch 80, Batch 80, Loss: 0.02294359914958477\n",
      "Epoch 80, Batch 90, Loss: 0.021562140434980392\n",
      "Epoch 80, Batch 100, Loss: 0.02273375727236271\n",
      "Epoch 81, Batch 0, Loss: 0.02283044531941414\n",
      "Epoch 81, Batch 10, Loss: 0.02212538756430149\n",
      "Epoch 81, Batch 20, Loss: 0.02316652424633503\n",
      "Epoch 81, Batch 30, Loss: 0.02756374515593052\n",
      "Epoch 81, Batch 40, Loss: 0.017954032868146896\n",
      "Epoch 81, Batch 50, Loss: 0.027034064754843712\n",
      "Epoch 81, Batch 60, Loss: 0.024204997345805168\n",
      "Epoch 81, Batch 70, Loss: 0.026422204449772835\n",
      "Epoch 81, Batch 80, Loss: 0.026340872049331665\n",
      "Epoch 81, Batch 90, Loss: 0.02617425099015236\n",
      "Epoch 81, Batch 100, Loss: 0.027932478114962578\n",
      "Epoch 82, Batch 0, Loss: 0.025993840768933296\n",
      "Epoch 82, Batch 10, Loss: 0.03565085679292679\n",
      "Epoch 82, Batch 20, Loss: 0.023397376760840416\n",
      "Epoch 82, Batch 30, Loss: 0.027181459590792656\n",
      "Epoch 82, Batch 40, Loss: 0.019824542105197906\n",
      "Epoch 82, Batch 50, Loss: 0.027117440477013588\n",
      "Epoch 82, Batch 60, Loss: 0.019199447706341743\n",
      "Epoch 82, Batch 70, Loss: 0.0196352731436491\n",
      "Epoch 82, Batch 80, Loss: 0.020986609160900116\n",
      "Epoch 82, Batch 90, Loss: 0.02923191338777542\n",
      "Epoch 82, Batch 100, Loss: 0.018864061683416367\n",
      "Epoch 83, Batch 0, Loss: 0.030625183135271072\n",
      "Epoch 83, Batch 10, Loss: 0.018797798082232475\n",
      "Epoch 83, Batch 20, Loss: 0.02801644429564476\n",
      "Epoch 83, Batch 30, Loss: 0.02644187957048416\n",
      "Epoch 83, Batch 40, Loss: 0.02413000911474228\n",
      "Epoch 83, Batch 50, Loss: 0.028352797031402588\n",
      "Epoch 83, Batch 60, Loss: 0.02249850705265999\n",
      "Epoch 83, Batch 70, Loss: 0.025252047926187515\n",
      "Epoch 83, Batch 80, Loss: 0.018789511173963547\n",
      "Epoch 83, Batch 90, Loss: 0.022542618215084076\n",
      "Epoch 83, Batch 100, Loss: 0.020407240837812424\n",
      "Epoch 84, Batch 0, Loss: 0.021912438794970512\n",
      "Epoch 84, Batch 10, Loss: 0.021087830886244774\n",
      "Epoch 84, Batch 20, Loss: 0.023711420595645905\n",
      "Epoch 84, Batch 30, Loss: 0.019176391884684563\n",
      "Epoch 84, Batch 40, Loss: 0.02363898605108261\n",
      "Epoch 84, Batch 50, Loss: 0.030177265405654907\n",
      "Epoch 84, Batch 60, Loss: 0.017582381144165993\n",
      "Epoch 84, Batch 70, Loss: 0.023681536316871643\n",
      "Epoch 84, Batch 80, Loss: 0.023139219731092453\n",
      "Epoch 84, Batch 90, Loss: 0.023287368938326836\n",
      "Epoch 84, Batch 100, Loss: 0.02294241078197956\n",
      "Epoch 85, Batch 0, Loss: 0.01808260940015316\n",
      "Epoch 85, Batch 10, Loss: 0.018893437460064888\n",
      "Epoch 85, Batch 20, Loss: 0.020215235650539398\n",
      "Epoch 85, Batch 30, Loss: 0.023829862475395203\n",
      "Epoch 85, Batch 40, Loss: 0.030722947791218758\n",
      "Epoch 85, Batch 50, Loss: 0.02575768530368805\n",
      "Epoch 85, Batch 60, Loss: 0.023476406931877136\n",
      "Epoch 85, Batch 70, Loss: 0.023880554363131523\n",
      "Epoch 85, Batch 80, Loss: 0.029731348156929016\n",
      "Epoch 85, Batch 90, Loss: 0.017585396766662598\n",
      "Epoch 85, Batch 100, Loss: 0.022944465279579163\n",
      "Epoch 86, Batch 0, Loss: 0.022576652467250824\n",
      "Epoch 86, Batch 10, Loss: 0.016821762546896935\n",
      "Epoch 86, Batch 20, Loss: 0.021159956231713295\n",
      "Epoch 86, Batch 30, Loss: 0.024494929239153862\n",
      "Epoch 86, Batch 40, Loss: 0.017453238368034363\n",
      "Epoch 86, Batch 50, Loss: 0.02585681900382042\n",
      "Epoch 86, Batch 60, Loss: 0.025156350806355476\n",
      "Epoch 86, Batch 70, Loss: 0.024154987186193466\n",
      "Epoch 86, Batch 80, Loss: 0.03029654175043106\n",
      "Epoch 86, Batch 90, Loss: 0.020029932260513306\n",
      "Epoch 86, Batch 100, Loss: 0.022874489426612854\n",
      "Epoch 87, Batch 0, Loss: 0.034879520535469055\n",
      "Epoch 87, Batch 10, Loss: 0.019665604457259178\n",
      "Epoch 87, Batch 20, Loss: 0.017592869699001312\n",
      "Epoch 87, Batch 30, Loss: 0.023823050782084465\n",
      "Epoch 87, Batch 40, Loss: 0.024316132068634033\n",
      "Epoch 87, Batch 50, Loss: 0.022094931453466415\n",
      "Epoch 87, Batch 60, Loss: 0.021405816078186035\n",
      "Epoch 87, Batch 70, Loss: 0.020942533388733864\n",
      "Epoch 87, Batch 80, Loss: 0.025729086250066757\n",
      "Epoch 87, Batch 90, Loss: 0.021266838535666466\n",
      "Epoch 87, Batch 100, Loss: 0.024785423651337624\n",
      "Epoch 88, Batch 0, Loss: 0.022711699828505516\n",
      "Epoch 88, Batch 10, Loss: 0.021005799993872643\n",
      "Epoch 88, Batch 20, Loss: 0.020564628764986992\n",
      "Epoch 88, Batch 30, Loss: 0.029109971597790718\n",
      "Epoch 88, Batch 40, Loss: 0.025900395587086678\n",
      "Epoch 88, Batch 50, Loss: 0.02231329120695591\n",
      "Epoch 88, Batch 60, Loss: 0.018919331952929497\n",
      "Epoch 88, Batch 70, Loss: 0.017753416672348976\n",
      "Epoch 88, Batch 80, Loss: 0.022020533680915833\n",
      "Epoch 88, Batch 90, Loss: 0.020003851503133774\n",
      "Epoch 88, Batch 100, Loss: 0.019073696807026863\n",
      "Epoch 89, Batch 0, Loss: 0.02154330350458622\n",
      "Epoch 89, Batch 10, Loss: 0.026421042159199715\n",
      "Epoch 89, Batch 20, Loss: 0.024187803268432617\n",
      "Epoch 89, Batch 30, Loss: 0.02361818216741085\n",
      "Epoch 89, Batch 40, Loss: 0.026424720883369446\n",
      "Epoch 89, Batch 50, Loss: 0.02273235283792019\n",
      "Epoch 89, Batch 60, Loss: 0.019884593784809113\n",
      "Epoch 89, Batch 70, Loss: 0.022025998681783676\n",
      "Epoch 89, Batch 80, Loss: 0.022136377170681953\n",
      "Epoch 89, Batch 90, Loss: 0.01954108290374279\n",
      "Epoch 89, Batch 100, Loss: 0.017835967242717743\n",
      "Epoch 90, Batch 0, Loss: 0.02442050166428089\n",
      "Epoch 90, Batch 10, Loss: 0.021540911868214607\n",
      "Epoch 90, Batch 20, Loss: 0.020202776417136192\n",
      "Epoch 90, Batch 30, Loss: 0.030222905799746513\n",
      "Epoch 90, Batch 40, Loss: 0.024510376155376434\n",
      "Epoch 90, Batch 50, Loss: 0.022627435624599457\n",
      "Epoch 90, Batch 60, Loss: 0.0303837601095438\n",
      "Epoch 90, Batch 70, Loss: 0.024436404928565025\n",
      "Epoch 90, Batch 80, Loss: 0.022818729281425476\n",
      "Epoch 90, Batch 90, Loss: 0.024605078622698784\n",
      "Epoch 90, Batch 100, Loss: 0.019702069461345673\n",
      "Epoch 91, Batch 0, Loss: 0.02013956755399704\n",
      "Epoch 91, Batch 10, Loss: 0.025177158415317535\n",
      "Epoch 91, Batch 20, Loss: 0.02698579803109169\n",
      "Epoch 91, Batch 30, Loss: 0.026568925008177757\n",
      "Epoch 91, Batch 40, Loss: 0.016790444031357765\n",
      "Epoch 91, Batch 50, Loss: 0.033129073679447174\n",
      "Epoch 91, Batch 60, Loss: 0.02296333573758602\n",
      "Epoch 91, Batch 70, Loss: 0.026129920035600662\n",
      "Epoch 91, Batch 80, Loss: 0.034769631922245026\n",
      "Epoch 91, Batch 90, Loss: 0.027744611725211143\n",
      "Epoch 91, Batch 100, Loss: 0.020510882139205933\n",
      "Epoch 92, Batch 0, Loss: 0.021188046783208847\n",
      "Epoch 92, Batch 10, Loss: 0.02408994361758232\n",
      "Epoch 92, Batch 20, Loss: 0.022463195025920868\n",
      "Epoch 92, Batch 30, Loss: 0.023827292025089264\n",
      "Epoch 92, Batch 40, Loss: 0.02154427021741867\n",
      "Epoch 92, Batch 50, Loss: 0.020521551370620728\n",
      "Epoch 92, Batch 60, Loss: 0.026623379439115524\n",
      "Epoch 92, Batch 70, Loss: 0.028211822733283043\n",
      "Epoch 92, Batch 80, Loss: 0.033110879361629486\n",
      "Epoch 92, Batch 90, Loss: 0.021585147827863693\n",
      "Epoch 92, Batch 100, Loss: 0.02696082927286625\n",
      "Epoch 93, Batch 0, Loss: 0.023029325529932976\n",
      "Epoch 93, Batch 10, Loss: 0.021232647821307182\n",
      "Epoch 93, Batch 20, Loss: 0.024217840284109116\n",
      "Epoch 93, Batch 30, Loss: 0.019829099997878075\n",
      "Epoch 93, Batch 40, Loss: 0.024401450529694557\n",
      "Epoch 93, Batch 50, Loss: 0.024412499740719795\n",
      "Epoch 93, Batch 60, Loss: 0.024353066459298134\n",
      "Epoch 93, Batch 70, Loss: 0.020269764587283134\n",
      "Epoch 93, Batch 80, Loss: 0.025027649477124214\n",
      "Epoch 93, Batch 90, Loss: 0.014282265678048134\n",
      "Epoch 93, Batch 100, Loss: 0.018317919224500656\n",
      "Epoch 94, Batch 0, Loss: 0.026308976113796234\n",
      "Epoch 94, Batch 10, Loss: 0.014507339335978031\n",
      "Epoch 94, Batch 20, Loss: 0.01858101785182953\n",
      "Epoch 94, Batch 30, Loss: 0.021406065672636032\n",
      "Epoch 94, Batch 40, Loss: 0.02079886943101883\n",
      "Epoch 94, Batch 50, Loss: 0.02402576059103012\n",
      "Epoch 94, Batch 60, Loss: 0.021697329357266426\n",
      "Epoch 94, Batch 70, Loss: 0.025659676641225815\n",
      "Epoch 94, Batch 80, Loss: 0.025679592043161392\n",
      "Epoch 94, Batch 90, Loss: 0.01736048422753811\n",
      "Epoch 94, Batch 100, Loss: 0.02077675238251686\n",
      "Epoch 95, Batch 0, Loss: 0.027883708477020264\n",
      "Epoch 95, Batch 10, Loss: 0.03810287266969681\n",
      "Epoch 95, Batch 20, Loss: 0.021917816251516342\n",
      "Epoch 95, Batch 30, Loss: 0.01950845681130886\n",
      "Epoch 95, Batch 40, Loss: 0.027137717232108116\n",
      "Epoch 95, Batch 50, Loss: 0.025335349142551422\n",
      "Epoch 95, Batch 60, Loss: 0.021976061165332794\n",
      "Epoch 95, Batch 70, Loss: 0.01913198083639145\n",
      "Epoch 95, Batch 80, Loss: 0.020036468282341957\n",
      "Epoch 95, Batch 90, Loss: 0.02106257900595665\n",
      "Epoch 95, Batch 100, Loss: 0.023785846307873726\n",
      "Epoch 96, Batch 0, Loss: 0.025575648993253708\n",
      "Epoch 96, Batch 10, Loss: 0.029197335243225098\n",
      "Epoch 96, Batch 20, Loss: 0.021984562277793884\n",
      "Epoch 96, Batch 30, Loss: 0.024105284363031387\n",
      "Epoch 96, Batch 40, Loss: 0.035114835947752\n",
      "Epoch 96, Batch 50, Loss: 0.023436959832906723\n",
      "Epoch 96, Batch 60, Loss: 0.02897072583436966\n",
      "Epoch 96, Batch 70, Loss: 0.021662432700395584\n",
      "Epoch 96, Batch 80, Loss: 0.019431082531809807\n",
      "Epoch 96, Batch 90, Loss: 0.022284023463726044\n",
      "Epoch 96, Batch 100, Loss: 0.02581377699971199\n",
      "Epoch 97, Batch 0, Loss: 0.017460038885474205\n",
      "Epoch 97, Batch 10, Loss: 0.02618097886443138\n",
      "Epoch 97, Batch 20, Loss: 0.029966112226247787\n",
      "Epoch 97, Batch 30, Loss: 0.02142202854156494\n",
      "Epoch 97, Batch 40, Loss: 0.028129778802394867\n",
      "Epoch 97, Batch 50, Loss: 0.02696034498512745\n",
      "Epoch 97, Batch 60, Loss: 0.03152617812156677\n",
      "Epoch 97, Batch 70, Loss: 0.025358637794852257\n",
      "Epoch 97, Batch 80, Loss: 0.019729752093553543\n",
      "Epoch 97, Batch 90, Loss: 0.022975489497184753\n",
      "Epoch 97, Batch 100, Loss: 0.02479509264230728\n",
      "Epoch 98, Batch 0, Loss: 0.01657121255993843\n",
      "Epoch 98, Batch 10, Loss: 0.025047961622476578\n",
      "Epoch 98, Batch 20, Loss: 0.026147594675421715\n",
      "Epoch 98, Batch 30, Loss: 0.027399418875575066\n",
      "Epoch 98, Batch 40, Loss: 0.02287915349006653\n",
      "Epoch 98, Batch 50, Loss: 0.021319642663002014\n",
      "Epoch 98, Batch 60, Loss: 0.021489322185516357\n",
      "Epoch 98, Batch 70, Loss: 0.03006128780543804\n",
      "Epoch 98, Batch 80, Loss: 0.03124113567173481\n",
      "Epoch 98, Batch 90, Loss: 0.025557588785886765\n",
      "Epoch 98, Batch 100, Loss: 0.026859598234295845\n",
      "Epoch 99, Batch 0, Loss: 0.01894897222518921\n",
      "Epoch 99, Batch 10, Loss: 0.02630896493792534\n",
      "Epoch 99, Batch 20, Loss: 0.03433636203408241\n",
      "Epoch 99, Batch 30, Loss: 0.017580151557922363\n",
      "Epoch 99, Batch 40, Loss: 0.02517980895936489\n",
      "Epoch 99, Batch 50, Loss: 0.022061433643102646\n",
      "Epoch 99, Batch 60, Loss: 0.02670038491487503\n",
      "Epoch 99, Batch 70, Loss: 0.020937543362379074\n",
      "Epoch 99, Batch 80, Loss: 0.027034467086195946\n",
      "Epoch 99, Batch 90, Loss: 0.0320831835269928\n",
      "Epoch 99, Batch 100, Loss: 0.0206682737916708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_LSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True)\n",
       "    (fc1): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.039533; Test RMSE 41.517510\n",
      "\n",
      "Train  MAE: 0.023022; Test  MAE 28.970661\n",
      "\n",
      "Train  R^2: 0.998437; Test  R^2 0.962160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_num_layers = 1\n",
    "lstm_hidden_size = 64\n",
    "path = \"models/CNN_LSTM_{0}Epoch_{1}Lr_{2}Layer_{3}Size.pt\".format(num_epoch, learning_rate, lstm_num_layers, lstm_hidden_size)\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BILSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BILSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.bilstm = BILSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x)\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Reshape x to fit BiLSTM input requirements\n",
    "        # x = x.contiguous().view(batch_size, seq_len, -1)\n",
    "        x = self.bilstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_BILSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.7793314456939697\n",
      "Epoch 0, Batch 10, Loss: 0.7929213643074036\n",
      "Epoch 0, Batch 20, Loss: 0.8718426823616028\n",
      "Epoch 0, Batch 30, Loss: 0.8045697808265686\n",
      "Epoch 0, Batch 40, Loss: 0.7431604862213135\n",
      "Epoch 0, Batch 50, Loss: 0.6652891039848328\n",
      "Epoch 0, Batch 60, Loss: 0.6235947608947754\n",
      "Epoch 0, Batch 70, Loss: 0.6872910261154175\n",
      "Epoch 0, Batch 80, Loss: 0.714838981628418\n",
      "Epoch 0, Batch 90, Loss: 0.6941305994987488\n",
      "Epoch 0, Batch 100, Loss: 0.4842958450317383\n",
      "Epoch 1, Batch 0, Loss: 0.6399528980255127\n",
      "Epoch 1, Batch 10, Loss: 0.44477778673171997\n",
      "Epoch 1, Batch 20, Loss: 0.5268522500991821\n",
      "Epoch 1, Batch 30, Loss: 0.5145187377929688\n",
      "Epoch 1, Batch 40, Loss: 0.45475220680236816\n",
      "Epoch 1, Batch 50, Loss: 0.2933398187160492\n",
      "Epoch 1, Batch 60, Loss: 0.23742803931236267\n",
      "Epoch 1, Batch 70, Loss: 0.20982234179973602\n",
      "Epoch 1, Batch 80, Loss: 0.29928478598594666\n",
      "Epoch 1, Batch 90, Loss: 0.19146794080734253\n",
      "Epoch 1, Batch 100, Loss: 0.18411894142627716\n",
      "Epoch 2, Batch 0, Loss: 0.16804085671901703\n",
      "Epoch 2, Batch 10, Loss: 0.1351923942565918\n",
      "Epoch 2, Batch 20, Loss: 0.11645331233739853\n",
      "Epoch 2, Batch 30, Loss: 0.15278899669647217\n",
      "Epoch 2, Batch 40, Loss: 0.18028023838996887\n",
      "Epoch 2, Batch 50, Loss: 0.07243607193231583\n",
      "Epoch 2, Batch 60, Loss: 0.14095069468021393\n",
      "Epoch 2, Batch 70, Loss: 0.05766554921865463\n",
      "Epoch 2, Batch 80, Loss: 0.06708915531635284\n",
      "Epoch 2, Batch 90, Loss: 0.11089282482862473\n",
      "Epoch 2, Batch 100, Loss: 0.08897711336612701\n",
      "Epoch 3, Batch 0, Loss: 0.04540739953517914\n",
      "Epoch 3, Batch 10, Loss: 0.06441890448331833\n",
      "Epoch 3, Batch 20, Loss: 0.07569897919893265\n",
      "Epoch 3, Batch 30, Loss: 0.08311060070991516\n",
      "Epoch 3, Batch 40, Loss: 0.07526826858520508\n",
      "Epoch 3, Batch 50, Loss: 0.08013992756605148\n",
      "Epoch 3, Batch 60, Loss: 0.08212719112634659\n",
      "Epoch 3, Batch 70, Loss: 0.04646690934896469\n",
      "Epoch 3, Batch 80, Loss: 0.04519565403461456\n",
      "Epoch 3, Batch 90, Loss: 0.0359247587621212\n",
      "Epoch 3, Batch 100, Loss: 0.04232970252633095\n",
      "Epoch 4, Batch 0, Loss: 0.05046295374631882\n",
      "Epoch 4, Batch 10, Loss: 0.03373434767127037\n",
      "Epoch 4, Batch 20, Loss: 0.06960280984640121\n",
      "Epoch 4, Batch 30, Loss: 0.041997894644737244\n",
      "Epoch 4, Batch 40, Loss: 0.05933060124516487\n",
      "Epoch 4, Batch 50, Loss: 0.05854172259569168\n",
      "Epoch 4, Batch 60, Loss: 0.06973224878311157\n",
      "Epoch 4, Batch 70, Loss: 0.0366474911570549\n",
      "Epoch 4, Batch 80, Loss: 0.05351525917649269\n",
      "Epoch 4, Batch 90, Loss: 0.05122668668627739\n",
      "Epoch 4, Batch 100, Loss: 0.02799363061785698\n",
      "Epoch 5, Batch 0, Loss: 0.04317568987607956\n",
      "Epoch 5, Batch 10, Loss: 0.048338379710912704\n",
      "Epoch 5, Batch 20, Loss: 0.04876010864973068\n",
      "Epoch 5, Batch 30, Loss: 0.04297206178307533\n",
      "Epoch 5, Batch 40, Loss: 0.03829526528716087\n",
      "Epoch 5, Batch 50, Loss: 0.04355379194021225\n",
      "Epoch 5, Batch 60, Loss: 0.04552600532770157\n",
      "Epoch 5, Batch 70, Loss: 0.04626120626926422\n",
      "Epoch 5, Batch 80, Loss: 0.045366186648607254\n",
      "Epoch 5, Batch 90, Loss: 0.037184394896030426\n",
      "Epoch 5, Batch 100, Loss: 0.04167916625738144\n",
      "Epoch 6, Batch 0, Loss: 0.05956101417541504\n",
      "Epoch 6, Batch 10, Loss: 0.028701361268758774\n",
      "Epoch 6, Batch 20, Loss: 0.04081428796052933\n",
      "Epoch 6, Batch 30, Loss: 0.04533906653523445\n",
      "Epoch 6, Batch 40, Loss: 0.05610502511262894\n",
      "Epoch 6, Batch 50, Loss: 0.04273989796638489\n",
      "Epoch 6, Batch 60, Loss: 0.04078671336174011\n",
      "Epoch 6, Batch 70, Loss: 0.040204040706157684\n",
      "Epoch 6, Batch 80, Loss: 0.03313493728637695\n",
      "Epoch 6, Batch 90, Loss: 0.05787098407745361\n",
      "Epoch 6, Batch 100, Loss: 0.04783064126968384\n",
      "Epoch 7, Batch 0, Loss: 0.032707393169403076\n",
      "Epoch 7, Batch 10, Loss: 0.04428477957844734\n",
      "Epoch 7, Batch 20, Loss: 0.03766833245754242\n",
      "Epoch 7, Batch 30, Loss: 0.06038527563214302\n",
      "Epoch 7, Batch 40, Loss: 0.04820942133665085\n",
      "Epoch 7, Batch 50, Loss: 0.038279276341199875\n",
      "Epoch 7, Batch 60, Loss: 0.03518955036997795\n",
      "Epoch 7, Batch 70, Loss: 0.03893786296248436\n",
      "Epoch 7, Batch 80, Loss: 0.028474336490035057\n",
      "Epoch 7, Batch 90, Loss: 0.030497796833515167\n",
      "Epoch 7, Batch 100, Loss: 0.033180322498083115\n",
      "Epoch 8, Batch 0, Loss: 0.034444667398929596\n",
      "Epoch 8, Batch 10, Loss: 0.04622597247362137\n",
      "Epoch 8, Batch 20, Loss: 0.03635333478450775\n",
      "Epoch 8, Batch 30, Loss: 0.03247320279479027\n",
      "Epoch 8, Batch 40, Loss: 0.03787340223789215\n",
      "Epoch 8, Batch 50, Loss: 0.03446429595351219\n",
      "Epoch 8, Batch 60, Loss: 0.03016636148095131\n",
      "Epoch 8, Batch 70, Loss: 0.03132955729961395\n",
      "Epoch 8, Batch 80, Loss: 0.03777293860912323\n",
      "Epoch 8, Batch 90, Loss: 0.03498733043670654\n",
      "Epoch 8, Batch 100, Loss: 0.02886909805238247\n",
      "Epoch 9, Batch 0, Loss: 0.027335407212376595\n",
      "Epoch 9, Batch 10, Loss: 0.03880196437239647\n",
      "Epoch 9, Batch 20, Loss: 0.020771214738488197\n",
      "Epoch 9, Batch 30, Loss: 0.03281416371464729\n",
      "Epoch 9, Batch 40, Loss: 0.026236997917294502\n",
      "Epoch 9, Batch 50, Loss: 0.02525242231786251\n",
      "Epoch 9, Batch 60, Loss: 0.048238370567560196\n",
      "Epoch 9, Batch 70, Loss: 0.026040606200695038\n",
      "Epoch 9, Batch 80, Loss: 0.03184385970234871\n",
      "Epoch 9, Batch 90, Loss: 0.04118786007165909\n",
      "Epoch 9, Batch 100, Loss: 0.03841815143823624\n",
      "Epoch 10, Batch 0, Loss: 0.01858111470937729\n",
      "Epoch 10, Batch 10, Loss: 0.02811211720108986\n",
      "Epoch 10, Batch 20, Loss: 0.038462452590465546\n",
      "Epoch 10, Batch 30, Loss: 0.027169153094291687\n",
      "Epoch 10, Batch 40, Loss: 0.02375160902738571\n",
      "Epoch 10, Batch 50, Loss: 0.02635837532579899\n",
      "Epoch 10, Batch 60, Loss: 0.027714218944311142\n",
      "Epoch 10, Batch 70, Loss: 0.03502907231450081\n",
      "Epoch 10, Batch 80, Loss: 0.028396259993314743\n",
      "Epoch 10, Batch 90, Loss: 0.02782137505710125\n",
      "Epoch 10, Batch 100, Loss: 0.027667902410030365\n",
      "Epoch 11, Batch 0, Loss: 0.03588550537824631\n",
      "Epoch 11, Batch 10, Loss: 0.03743733838200569\n",
      "Epoch 11, Batch 20, Loss: 0.022733982652425766\n",
      "Epoch 11, Batch 30, Loss: 0.030192047357559204\n",
      "Epoch 11, Batch 40, Loss: 0.02568155899643898\n",
      "Epoch 11, Batch 50, Loss: 0.04334138333797455\n",
      "Epoch 11, Batch 60, Loss: 0.03822430223226547\n",
      "Epoch 11, Batch 70, Loss: 0.04021690785884857\n",
      "Epoch 11, Batch 80, Loss: 0.026151327416300774\n",
      "Epoch 11, Batch 90, Loss: 0.0349443294107914\n",
      "Epoch 11, Batch 100, Loss: 0.027295149862766266\n",
      "Epoch 12, Batch 0, Loss: 0.022404147312045097\n",
      "Epoch 12, Batch 10, Loss: 0.04230103641748428\n",
      "Epoch 12, Batch 20, Loss: 0.026902424171566963\n",
      "Epoch 12, Batch 30, Loss: 0.02789914794266224\n",
      "Epoch 12, Batch 40, Loss: 0.028104640543460846\n",
      "Epoch 12, Batch 50, Loss: 0.029362443834543228\n",
      "Epoch 12, Batch 60, Loss: 0.02747257798910141\n",
      "Epoch 12, Batch 70, Loss: 0.035997506231069565\n",
      "Epoch 12, Batch 80, Loss: 0.03393526002764702\n",
      "Epoch 12, Batch 90, Loss: 0.02950574830174446\n",
      "Epoch 12, Batch 100, Loss: 0.017088811844587326\n",
      "Epoch 13, Batch 0, Loss: 0.025403983891010284\n",
      "Epoch 13, Batch 10, Loss: 0.024050554260611534\n",
      "Epoch 13, Batch 20, Loss: 0.025398610159754753\n",
      "Epoch 13, Batch 30, Loss: 0.03392805531620979\n",
      "Epoch 13, Batch 40, Loss: 0.03036859817802906\n",
      "Epoch 13, Batch 50, Loss: 0.022494807839393616\n",
      "Epoch 13, Batch 60, Loss: 0.03687898814678192\n",
      "Epoch 13, Batch 70, Loss: 0.03062237612903118\n",
      "Epoch 13, Batch 80, Loss: 0.02839525230228901\n",
      "Epoch 13, Batch 90, Loss: 0.030013862997293472\n",
      "Epoch 13, Batch 100, Loss: 0.02539750747382641\n",
      "Epoch 14, Batch 0, Loss: 0.03694227337837219\n",
      "Epoch 14, Batch 10, Loss: 0.025906486436724663\n",
      "Epoch 14, Batch 20, Loss: 0.032380640506744385\n",
      "Epoch 14, Batch 30, Loss: 0.03134055435657501\n",
      "Epoch 14, Batch 40, Loss: 0.03018222376704216\n",
      "Epoch 14, Batch 50, Loss: 0.031170789152383804\n",
      "Epoch 14, Batch 60, Loss: 0.0269736647605896\n",
      "Epoch 14, Batch 70, Loss: 0.02907838299870491\n",
      "Epoch 14, Batch 80, Loss: 0.03372430056333542\n",
      "Epoch 14, Batch 90, Loss: 0.02549055404961109\n",
      "Epoch 14, Batch 100, Loss: 0.027901992201805115\n",
      "Epoch 15, Batch 0, Loss: 0.03068818897008896\n",
      "Epoch 15, Batch 10, Loss: 0.020218513906002045\n",
      "Epoch 15, Batch 20, Loss: 0.023539002984762192\n",
      "Epoch 15, Batch 30, Loss: 0.021932832896709442\n",
      "Epoch 15, Batch 40, Loss: 0.02680838480591774\n",
      "Epoch 15, Batch 50, Loss: 0.02649552933871746\n",
      "Epoch 15, Batch 60, Loss: 0.03034083917737007\n",
      "Epoch 15, Batch 70, Loss: 0.03305075690150261\n",
      "Epoch 15, Batch 80, Loss: 0.02677908167243004\n",
      "Epoch 15, Batch 90, Loss: 0.025558143854141235\n",
      "Epoch 15, Batch 100, Loss: 0.038428887724876404\n",
      "Epoch 16, Batch 0, Loss: 0.02274475246667862\n",
      "Epoch 16, Batch 10, Loss: 0.0257689468562603\n",
      "Epoch 16, Batch 20, Loss: 0.025529853999614716\n",
      "Epoch 16, Batch 30, Loss: 0.024780845269560814\n",
      "Epoch 16, Batch 40, Loss: 0.027279650792479515\n",
      "Epoch 16, Batch 50, Loss: 0.033785343170166016\n",
      "Epoch 16, Batch 60, Loss: 0.03237520530819893\n",
      "Epoch 16, Batch 70, Loss: 0.030844779685139656\n",
      "Epoch 16, Batch 80, Loss: 0.03015706315636635\n",
      "Epoch 16, Batch 90, Loss: 0.018760457634925842\n",
      "Epoch 16, Batch 100, Loss: 0.027307504788041115\n",
      "Epoch 17, Batch 0, Loss: 0.024384401738643646\n",
      "Epoch 17, Batch 10, Loss: 0.02791300043463707\n",
      "Epoch 17, Batch 20, Loss: 0.036925964057445526\n",
      "Epoch 17, Batch 30, Loss: 0.02961081638932228\n",
      "Epoch 17, Batch 40, Loss: 0.018667669966816902\n",
      "Epoch 17, Batch 50, Loss: 0.026040272787213326\n",
      "Epoch 17, Batch 60, Loss: 0.03518025949597359\n",
      "Epoch 17, Batch 70, Loss: 0.019165175035595894\n",
      "Epoch 17, Batch 80, Loss: 0.033659256994724274\n",
      "Epoch 17, Batch 90, Loss: 0.027423353865742683\n",
      "Epoch 17, Batch 100, Loss: 0.023434631526470184\n",
      "Epoch 18, Batch 0, Loss: 0.027794407680630684\n",
      "Epoch 18, Batch 10, Loss: 0.031272877007722855\n",
      "Epoch 18, Batch 20, Loss: 0.03910481184720993\n",
      "Epoch 18, Batch 30, Loss: 0.02895287796854973\n",
      "Epoch 18, Batch 40, Loss: 0.02584647573530674\n",
      "Epoch 18, Batch 50, Loss: 0.02973497286438942\n",
      "Epoch 18, Batch 60, Loss: 0.017842404544353485\n",
      "Epoch 18, Batch 70, Loss: 0.024315010756254196\n",
      "Epoch 18, Batch 80, Loss: 0.025584019720554352\n",
      "Epoch 18, Batch 90, Loss: 0.02262643165886402\n",
      "Epoch 18, Batch 100, Loss: 0.022084256634116173\n",
      "Epoch 19, Batch 0, Loss: 0.027168704196810722\n",
      "Epoch 19, Batch 10, Loss: 0.0222935751080513\n",
      "Epoch 19, Batch 20, Loss: 0.022329481318593025\n",
      "Epoch 19, Batch 30, Loss: 0.02434557117521763\n",
      "Epoch 19, Batch 40, Loss: 0.020855236798524857\n",
      "Epoch 19, Batch 50, Loss: 0.025691967457532883\n",
      "Epoch 19, Batch 60, Loss: 0.03619583323597908\n",
      "Epoch 19, Batch 70, Loss: 0.025654707103967667\n",
      "Epoch 19, Batch 80, Loss: 0.026343543082475662\n",
      "Epoch 19, Batch 90, Loss: 0.025255519896745682\n",
      "Epoch 19, Batch 100, Loss: 0.028257163241505623\n",
      "Epoch 20, Batch 0, Loss: 0.026460370048880577\n",
      "Epoch 20, Batch 10, Loss: 0.02292863093316555\n",
      "Epoch 20, Batch 20, Loss: 0.023875676095485687\n",
      "Epoch 20, Batch 30, Loss: 0.03214571252465248\n",
      "Epoch 20, Batch 40, Loss: 0.029447510838508606\n",
      "Epoch 20, Batch 50, Loss: 0.03165893256664276\n",
      "Epoch 20, Batch 60, Loss: 0.01564868539571762\n",
      "Epoch 20, Batch 70, Loss: 0.024583183228969574\n",
      "Epoch 20, Batch 80, Loss: 0.0338679775595665\n",
      "Epoch 20, Batch 90, Loss: 0.02289874665439129\n",
      "Epoch 20, Batch 100, Loss: 0.021210139617323875\n",
      "Epoch 21, Batch 0, Loss: 0.027135629206895828\n",
      "Epoch 21, Batch 10, Loss: 0.032063886523246765\n",
      "Epoch 21, Batch 20, Loss: 0.04400419816374779\n",
      "Epoch 21, Batch 30, Loss: 0.02867431752383709\n",
      "Epoch 21, Batch 40, Loss: 0.02828562632203102\n",
      "Epoch 21, Batch 50, Loss: 0.03534037247300148\n",
      "Epoch 21, Batch 60, Loss: 0.02574482560157776\n",
      "Epoch 21, Batch 70, Loss: 0.024790624156594276\n",
      "Epoch 21, Batch 80, Loss: 0.02471325732767582\n",
      "Epoch 21, Batch 90, Loss: 0.031078657135367393\n",
      "Epoch 21, Batch 100, Loss: 0.02348214015364647\n",
      "Epoch 22, Batch 0, Loss: 0.020717501640319824\n",
      "Epoch 22, Batch 10, Loss: 0.02888529561460018\n",
      "Epoch 22, Batch 20, Loss: 0.0320686437189579\n",
      "Epoch 22, Batch 30, Loss: 0.03331378102302551\n",
      "Epoch 22, Batch 40, Loss: 0.024801436811685562\n",
      "Epoch 22, Batch 50, Loss: 0.018703926354646683\n",
      "Epoch 22, Batch 60, Loss: 0.030467933043837547\n",
      "Epoch 22, Batch 70, Loss: 0.02333778701722622\n",
      "Epoch 22, Batch 80, Loss: 0.02692246064543724\n",
      "Epoch 22, Batch 90, Loss: 0.027102654799818993\n",
      "Epoch 22, Batch 100, Loss: 0.03455398604273796\n",
      "Epoch 23, Batch 0, Loss: 0.025418557226657867\n",
      "Epoch 23, Batch 10, Loss: 0.024076776579022408\n",
      "Epoch 23, Batch 20, Loss: 0.029685411602258682\n",
      "Epoch 23, Batch 30, Loss: 0.02211594767868519\n",
      "Epoch 23, Batch 40, Loss: 0.023363614454865456\n",
      "Epoch 23, Batch 50, Loss: 0.023174764588475227\n",
      "Epoch 23, Batch 60, Loss: 0.027528144419193268\n",
      "Epoch 23, Batch 70, Loss: 0.028253110125660896\n",
      "Epoch 23, Batch 80, Loss: 0.02658950909972191\n",
      "Epoch 23, Batch 90, Loss: 0.0284119863063097\n",
      "Epoch 23, Batch 100, Loss: 0.02485477738082409\n",
      "Epoch 24, Batch 0, Loss: 0.023972099646925926\n",
      "Epoch 24, Batch 10, Loss: 0.03131931275129318\n",
      "Epoch 24, Batch 20, Loss: 0.029952272772789\n",
      "Epoch 24, Batch 30, Loss: 0.03182453662157059\n",
      "Epoch 24, Batch 40, Loss: 0.03162304311990738\n",
      "Epoch 24, Batch 50, Loss: 0.021596405655145645\n",
      "Epoch 24, Batch 60, Loss: 0.016873102635145187\n",
      "Epoch 24, Batch 70, Loss: 0.019409675151109695\n",
      "Epoch 24, Batch 80, Loss: 0.028387073427438736\n",
      "Epoch 24, Batch 90, Loss: 0.019045762717723846\n",
      "Epoch 24, Batch 100, Loss: 0.029318340122699738\n",
      "Epoch 25, Batch 0, Loss: 0.031156081706285477\n",
      "Epoch 25, Batch 10, Loss: 0.021309515461325645\n",
      "Epoch 25, Batch 20, Loss: 0.027368538081645966\n",
      "Epoch 25, Batch 30, Loss: 0.03015006147325039\n",
      "Epoch 25, Batch 40, Loss: 0.028940865769982338\n",
      "Epoch 25, Batch 50, Loss: 0.025043144822120667\n",
      "Epoch 25, Batch 60, Loss: 0.030976423993706703\n",
      "Epoch 25, Batch 70, Loss: 0.0233010184019804\n",
      "Epoch 25, Batch 80, Loss: 0.023071181029081345\n",
      "Epoch 25, Batch 90, Loss: 0.026525361463427544\n",
      "Epoch 25, Batch 100, Loss: 0.031744495034217834\n",
      "Epoch 26, Batch 0, Loss: 0.023758525028824806\n",
      "Epoch 26, Batch 10, Loss: 0.024417582899332047\n",
      "Epoch 26, Batch 20, Loss: 0.025259580463171005\n",
      "Epoch 26, Batch 30, Loss: 0.03037552535533905\n",
      "Epoch 26, Batch 40, Loss: 0.027528854086995125\n",
      "Epoch 26, Batch 50, Loss: 0.027112377807497978\n",
      "Epoch 26, Batch 60, Loss: 0.023677345365285873\n",
      "Epoch 26, Batch 70, Loss: 0.03136279806494713\n",
      "Epoch 26, Batch 80, Loss: 0.027536144480109215\n",
      "Epoch 26, Batch 90, Loss: 0.026886077597737312\n",
      "Epoch 26, Batch 100, Loss: 0.026371784508228302\n",
      "Epoch 27, Batch 0, Loss: 0.02622993104159832\n",
      "Epoch 27, Batch 10, Loss: 0.029445838183164597\n",
      "Epoch 27, Batch 20, Loss: 0.026361793279647827\n",
      "Epoch 27, Batch 30, Loss: 0.01987636275589466\n",
      "Epoch 27, Batch 40, Loss: 0.031174011528491974\n",
      "Epoch 27, Batch 50, Loss: 0.0308469720184803\n",
      "Epoch 27, Batch 60, Loss: 0.028680788353085518\n",
      "Epoch 27, Batch 70, Loss: 0.026962555944919586\n",
      "Epoch 27, Batch 80, Loss: 0.02432732656598091\n",
      "Epoch 27, Batch 90, Loss: 0.028302587568759918\n",
      "Epoch 27, Batch 100, Loss: 0.022100988775491714\n",
      "Epoch 28, Batch 0, Loss: 0.027648795396089554\n",
      "Epoch 28, Batch 10, Loss: 0.026562122628092766\n",
      "Epoch 28, Batch 20, Loss: 0.03308022394776344\n",
      "Epoch 28, Batch 30, Loss: 0.022717557847499847\n",
      "Epoch 28, Batch 40, Loss: 0.027588607743382454\n",
      "Epoch 28, Batch 50, Loss: 0.025131497532129288\n",
      "Epoch 28, Batch 60, Loss: 0.027693066745996475\n",
      "Epoch 28, Batch 70, Loss: 0.024722103029489517\n",
      "Epoch 28, Batch 80, Loss: 0.036226555705070496\n",
      "Epoch 28, Batch 90, Loss: 0.028298677876591682\n",
      "Epoch 28, Batch 100, Loss: 0.030627617612481117\n",
      "Epoch 29, Batch 0, Loss: 0.023378251120448112\n",
      "Epoch 29, Batch 10, Loss: 0.027682313695549965\n",
      "Epoch 29, Batch 20, Loss: 0.02437846176326275\n",
      "Epoch 29, Batch 30, Loss: 0.025199294090270996\n",
      "Epoch 29, Batch 40, Loss: 0.026155898347496986\n",
      "Epoch 29, Batch 50, Loss: 0.029327817261219025\n",
      "Epoch 29, Batch 60, Loss: 0.018886413425207138\n",
      "Epoch 29, Batch 70, Loss: 0.022397546097636223\n",
      "Epoch 29, Batch 80, Loss: 0.02728075161576271\n",
      "Epoch 29, Batch 90, Loss: 0.02919662743806839\n",
      "Epoch 29, Batch 100, Loss: 0.0308469720184803\n",
      "Epoch 30, Batch 0, Loss: 0.02092076651751995\n",
      "Epoch 30, Batch 10, Loss: 0.028010593727231026\n",
      "Epoch 30, Batch 20, Loss: 0.021698636934161186\n",
      "Epoch 30, Batch 30, Loss: 0.025753425434231758\n",
      "Epoch 30, Batch 40, Loss: 0.03125083073973656\n",
      "Epoch 30, Batch 50, Loss: 0.026353944092988968\n",
      "Epoch 30, Batch 60, Loss: 0.031687330454587936\n",
      "Epoch 30, Batch 70, Loss: 0.030355162918567657\n",
      "Epoch 30, Batch 80, Loss: 0.020393600687384605\n",
      "Epoch 30, Batch 90, Loss: 0.024875178933143616\n",
      "Epoch 30, Batch 100, Loss: 0.020658545196056366\n",
      "Epoch 31, Batch 0, Loss: 0.026038145646452904\n",
      "Epoch 31, Batch 10, Loss: 0.023588914424180984\n",
      "Epoch 31, Batch 20, Loss: 0.03564639762043953\n",
      "Epoch 31, Batch 30, Loss: 0.025313865393400192\n",
      "Epoch 31, Batch 40, Loss: 0.021242747083306313\n",
      "Epoch 31, Batch 50, Loss: 0.03160089999437332\n",
      "Epoch 31, Batch 60, Loss: 0.02772083505988121\n",
      "Epoch 31, Batch 70, Loss: 0.02883889712393284\n",
      "Epoch 31, Batch 80, Loss: 0.035829849541187286\n",
      "Epoch 31, Batch 90, Loss: 0.03150084614753723\n",
      "Epoch 31, Batch 100, Loss: 0.029442911967635155\n",
      "Epoch 32, Batch 0, Loss: 0.03903381526470184\n",
      "Epoch 32, Batch 10, Loss: 0.02669491432607174\n",
      "Epoch 32, Batch 20, Loss: 0.02523954212665558\n",
      "Epoch 32, Batch 30, Loss: 0.024627549573779106\n",
      "Epoch 32, Batch 40, Loss: 0.023038726300001144\n",
      "Epoch 32, Batch 50, Loss: 0.030914517119526863\n",
      "Epoch 32, Batch 60, Loss: 0.027038250118494034\n",
      "Epoch 32, Batch 70, Loss: 0.02756962552666664\n",
      "Epoch 32, Batch 80, Loss: 0.022520771250128746\n",
      "Epoch 32, Batch 90, Loss: 0.025199156254529953\n",
      "Epoch 32, Batch 100, Loss: 0.024038713425397873\n",
      "Epoch 33, Batch 0, Loss: 0.033601582050323486\n",
      "Epoch 33, Batch 10, Loss: 0.02384699508547783\n",
      "Epoch 33, Batch 20, Loss: 0.029186587780714035\n",
      "Epoch 33, Batch 30, Loss: 0.023160891607403755\n",
      "Epoch 33, Batch 40, Loss: 0.022787941619753838\n",
      "Epoch 33, Batch 50, Loss: 0.021765567362308502\n",
      "Epoch 33, Batch 60, Loss: 0.029574129730463028\n",
      "Epoch 33, Batch 70, Loss: 0.03092038258910179\n",
      "Epoch 33, Batch 80, Loss: 0.030005766078829765\n",
      "Epoch 33, Batch 90, Loss: 0.02381892316043377\n",
      "Epoch 33, Batch 100, Loss: 0.025418182834982872\n",
      "Epoch 34, Batch 0, Loss: 0.03071017377078533\n",
      "Epoch 34, Batch 10, Loss: 0.02098829485476017\n",
      "Epoch 34, Batch 20, Loss: 0.020746003836393356\n",
      "Epoch 34, Batch 30, Loss: 0.022970860823988914\n",
      "Epoch 34, Batch 40, Loss: 0.022826828062534332\n",
      "Epoch 34, Batch 50, Loss: 0.01699213683605194\n",
      "Epoch 34, Batch 60, Loss: 0.02320200577378273\n",
      "Epoch 34, Batch 70, Loss: 0.026181736961007118\n",
      "Epoch 34, Batch 80, Loss: 0.02884596586227417\n",
      "Epoch 34, Batch 90, Loss: 0.020710572600364685\n",
      "Epoch 34, Batch 100, Loss: 0.029983779415488243\n",
      "Epoch 35, Batch 0, Loss: 0.02561240643262863\n",
      "Epoch 35, Batch 10, Loss: 0.019421163946390152\n",
      "Epoch 35, Batch 20, Loss: 0.02276490069925785\n",
      "Epoch 35, Batch 30, Loss: 0.024799969047307968\n",
      "Epoch 35, Batch 40, Loss: 0.019084881991147995\n",
      "Epoch 35, Batch 50, Loss: 0.02443194016814232\n",
      "Epoch 35, Batch 60, Loss: 0.02538307011127472\n",
      "Epoch 35, Batch 70, Loss: 0.029007302597165108\n",
      "Epoch 35, Batch 80, Loss: 0.026808351278305054\n",
      "Epoch 35, Batch 90, Loss: 0.022464491426944733\n",
      "Epoch 35, Batch 100, Loss: 0.027327945455908775\n",
      "Epoch 36, Batch 0, Loss: 0.01857778988778591\n",
      "Epoch 36, Batch 10, Loss: 0.02012653462588787\n",
      "Epoch 36, Batch 20, Loss: 0.026447083801031113\n",
      "Epoch 36, Batch 30, Loss: 0.028866242617368698\n",
      "Epoch 36, Batch 40, Loss: 0.02601572312414646\n",
      "Epoch 36, Batch 50, Loss: 0.029268428683280945\n",
      "Epoch 36, Batch 60, Loss: 0.02452862076461315\n",
      "Epoch 36, Batch 70, Loss: 0.02789265476167202\n",
      "Epoch 36, Batch 80, Loss: 0.025830170139670372\n",
      "Epoch 36, Batch 90, Loss: 0.029955586418509483\n",
      "Epoch 36, Batch 100, Loss: 0.020525719970464706\n",
      "Epoch 37, Batch 0, Loss: 0.022862819954752922\n",
      "Epoch 37, Batch 10, Loss: 0.021682986989617348\n",
      "Epoch 37, Batch 20, Loss: 0.02224617637693882\n",
      "Epoch 37, Batch 30, Loss: 0.02377302013337612\n",
      "Epoch 37, Batch 40, Loss: 0.026261163875460625\n",
      "Epoch 37, Batch 50, Loss: 0.02950877696275711\n",
      "Epoch 37, Batch 60, Loss: 0.02384677529335022\n",
      "Epoch 37, Batch 70, Loss: 0.031615741550922394\n",
      "Epoch 37, Batch 80, Loss: 0.025211039930582047\n",
      "Epoch 37, Batch 90, Loss: 0.025740083307027817\n",
      "Epoch 37, Batch 100, Loss: 0.026479756459593773\n",
      "Epoch 38, Batch 0, Loss: 0.01715472713112831\n",
      "Epoch 38, Batch 10, Loss: 0.03146718069911003\n",
      "Epoch 38, Batch 20, Loss: 0.029858339577913284\n",
      "Epoch 38, Batch 30, Loss: 0.02655630186200142\n",
      "Epoch 38, Batch 40, Loss: 0.020134510472416878\n",
      "Epoch 38, Batch 50, Loss: 0.02347463183104992\n",
      "Epoch 38, Batch 60, Loss: 0.022829031571745872\n",
      "Epoch 38, Batch 70, Loss: 0.02927231416106224\n",
      "Epoch 38, Batch 80, Loss: 0.018932154402136803\n",
      "Epoch 38, Batch 90, Loss: 0.030635327100753784\n",
      "Epoch 38, Batch 100, Loss: 0.023030119016766548\n",
      "Epoch 39, Batch 0, Loss: 0.03018602728843689\n",
      "Epoch 39, Batch 10, Loss: 0.026371916756033897\n",
      "Epoch 39, Batch 20, Loss: 0.02440425008535385\n",
      "Epoch 39, Batch 30, Loss: 0.024362564086914062\n",
      "Epoch 39, Batch 40, Loss: 0.019890105351805687\n",
      "Epoch 39, Batch 50, Loss: 0.023202700540423393\n",
      "Epoch 39, Batch 60, Loss: 0.017658637836575508\n",
      "Epoch 39, Batch 70, Loss: 0.0341840460896492\n",
      "Epoch 39, Batch 80, Loss: 0.025751594454050064\n",
      "Epoch 39, Batch 90, Loss: 0.02525615692138672\n",
      "Epoch 39, Batch 100, Loss: 0.029672961682081223\n",
      "Epoch 40, Batch 0, Loss: 0.02009756676852703\n",
      "Epoch 40, Batch 10, Loss: 0.01971328631043434\n",
      "Epoch 40, Batch 20, Loss: 0.02892739325761795\n",
      "Epoch 40, Batch 30, Loss: 0.023116841912269592\n",
      "Epoch 40, Batch 40, Loss: 0.03412993624806404\n",
      "Epoch 40, Batch 50, Loss: 0.021779395639896393\n",
      "Epoch 40, Batch 60, Loss: 0.029118141159415245\n",
      "Epoch 40, Batch 70, Loss: 0.024156760424375534\n",
      "Epoch 40, Batch 80, Loss: 0.018790850415825844\n",
      "Epoch 40, Batch 90, Loss: 0.02111331932246685\n",
      "Epoch 40, Batch 100, Loss: 0.021660374477505684\n",
      "Epoch 41, Batch 0, Loss: 0.01966160535812378\n",
      "Epoch 41, Batch 10, Loss: 0.027151811867952347\n",
      "Epoch 41, Batch 20, Loss: 0.029978808015584946\n",
      "Epoch 41, Batch 30, Loss: 0.02645116299390793\n",
      "Epoch 41, Batch 40, Loss: 0.022212058305740356\n",
      "Epoch 41, Batch 50, Loss: 0.026222866028547287\n",
      "Epoch 41, Batch 60, Loss: 0.026790505275130272\n",
      "Epoch 41, Batch 70, Loss: 0.024082237854599953\n",
      "Epoch 41, Batch 80, Loss: 0.03102058544754982\n",
      "Epoch 41, Batch 90, Loss: 0.02545086294412613\n",
      "Epoch 41, Batch 100, Loss: 0.021303247660398483\n",
      "Epoch 42, Batch 0, Loss: 0.02049834653735161\n",
      "Epoch 42, Batch 10, Loss: 0.026602989062666893\n",
      "Epoch 42, Batch 20, Loss: 0.026666875928640366\n",
      "Epoch 42, Batch 30, Loss: 0.02820969931781292\n",
      "Epoch 42, Batch 40, Loss: 0.030236078426241875\n",
      "Epoch 42, Batch 50, Loss: 0.026005297899246216\n",
      "Epoch 42, Batch 60, Loss: 0.0264614075422287\n",
      "Epoch 42, Batch 70, Loss: 0.025232696905732155\n",
      "Epoch 42, Batch 80, Loss: 0.027963880449533463\n",
      "Epoch 42, Batch 90, Loss: 0.032508984208106995\n",
      "Epoch 42, Batch 100, Loss: 0.02578289993107319\n",
      "Epoch 43, Batch 0, Loss: 0.02903282642364502\n",
      "Epoch 43, Batch 10, Loss: 0.021328896284103394\n",
      "Epoch 43, Batch 20, Loss: 0.026245389133691788\n",
      "Epoch 43, Batch 30, Loss: 0.02659701555967331\n",
      "Epoch 43, Batch 40, Loss: 0.029742054641246796\n",
      "Epoch 43, Batch 50, Loss: 0.03142716735601425\n",
      "Epoch 43, Batch 60, Loss: 0.024723347276449203\n",
      "Epoch 43, Batch 70, Loss: 0.01936573162674904\n",
      "Epoch 43, Batch 80, Loss: 0.022733349353075027\n",
      "Epoch 43, Batch 90, Loss: 0.022639956325292587\n",
      "Epoch 43, Batch 100, Loss: 0.029014984145760536\n",
      "Epoch 44, Batch 0, Loss: 0.025447893887758255\n",
      "Epoch 44, Batch 10, Loss: 0.021075960248708725\n",
      "Epoch 44, Batch 20, Loss: 0.02247558906674385\n",
      "Epoch 44, Batch 30, Loss: 0.026964280754327774\n",
      "Epoch 44, Batch 40, Loss: 0.01781616359949112\n",
      "Epoch 44, Batch 50, Loss: 0.03006652742624283\n",
      "Epoch 44, Batch 60, Loss: 0.02517564408481121\n",
      "Epoch 44, Batch 70, Loss: 0.026138294488191605\n",
      "Epoch 44, Batch 80, Loss: 0.03193189203739166\n",
      "Epoch 44, Batch 90, Loss: 0.03317642584443092\n",
      "Epoch 44, Batch 100, Loss: 0.02021818235516548\n",
      "Epoch 45, Batch 0, Loss: 0.022936346009373665\n",
      "Epoch 45, Batch 10, Loss: 0.021062184125185013\n",
      "Epoch 45, Batch 20, Loss: 0.02512241154909134\n",
      "Epoch 45, Batch 30, Loss: 0.02713187225162983\n",
      "Epoch 45, Batch 40, Loss: 0.020538805052638054\n",
      "Epoch 45, Batch 50, Loss: 0.02470671944320202\n",
      "Epoch 45, Batch 60, Loss: 0.019689228385686874\n",
      "Epoch 45, Batch 70, Loss: 0.022444818168878555\n",
      "Epoch 45, Batch 80, Loss: 0.027851527556777\n",
      "Epoch 45, Batch 90, Loss: 0.02536306343972683\n",
      "Epoch 45, Batch 100, Loss: 0.023990783840417862\n",
      "Epoch 46, Batch 0, Loss: 0.030185556039214134\n",
      "Epoch 46, Batch 10, Loss: 0.02569497562944889\n",
      "Epoch 46, Batch 20, Loss: 0.025127381086349487\n",
      "Epoch 46, Batch 30, Loss: 0.028367767110466957\n",
      "Epoch 46, Batch 40, Loss: 0.021965041756629944\n",
      "Epoch 46, Batch 50, Loss: 0.029711829498410225\n",
      "Epoch 46, Batch 60, Loss: 0.037067607045173645\n",
      "Epoch 46, Batch 70, Loss: 0.026286981999874115\n",
      "Epoch 46, Batch 80, Loss: 0.020283138379454613\n",
      "Epoch 46, Batch 90, Loss: 0.023415854200720787\n",
      "Epoch 46, Batch 100, Loss: 0.028130725026130676\n",
      "Epoch 47, Batch 0, Loss: 0.026873355731368065\n",
      "Epoch 47, Batch 10, Loss: 0.02329735830426216\n",
      "Epoch 47, Batch 20, Loss: 0.02998402900993824\n",
      "Epoch 47, Batch 30, Loss: 0.024494152516126633\n",
      "Epoch 47, Batch 40, Loss: 0.027722209692001343\n",
      "Epoch 47, Batch 50, Loss: 0.027672428637742996\n",
      "Epoch 47, Batch 60, Loss: 0.027155086398124695\n",
      "Epoch 47, Batch 70, Loss: 0.02638774737715721\n",
      "Epoch 47, Batch 80, Loss: 0.03293243795633316\n",
      "Epoch 47, Batch 90, Loss: 0.01557270810008049\n",
      "Epoch 47, Batch 100, Loss: 0.031771138310432434\n",
      "Epoch 48, Batch 0, Loss: 0.029720865190029144\n",
      "Epoch 48, Batch 10, Loss: 0.02632482536137104\n",
      "Epoch 48, Batch 20, Loss: 0.029728753492236137\n",
      "Epoch 48, Batch 30, Loss: 0.024716082960367203\n",
      "Epoch 48, Batch 40, Loss: 0.021707788109779358\n",
      "Epoch 48, Batch 50, Loss: 0.02170470915734768\n",
      "Epoch 48, Batch 60, Loss: 0.0300302691757679\n",
      "Epoch 48, Batch 70, Loss: 0.020118216052651405\n",
      "Epoch 48, Batch 80, Loss: 0.018242983147501945\n",
      "Epoch 48, Batch 90, Loss: 0.03075190633535385\n",
      "Epoch 48, Batch 100, Loss: 0.032403793185949326\n",
      "Epoch 49, Batch 0, Loss: 0.03752690181136131\n",
      "Epoch 49, Batch 10, Loss: 0.01710917055606842\n",
      "Epoch 49, Batch 20, Loss: 0.02096538059413433\n",
      "Epoch 49, Batch 30, Loss: 0.021968519315123558\n",
      "Epoch 49, Batch 40, Loss: 0.023018673062324524\n",
      "Epoch 49, Batch 50, Loss: 0.026145312935113907\n",
      "Epoch 49, Batch 60, Loss: 0.02522433176636696\n",
      "Epoch 49, Batch 70, Loss: 0.029866959899663925\n",
      "Epoch 49, Batch 80, Loss: 0.026907775551080704\n",
      "Epoch 49, Batch 90, Loss: 0.017867086455225945\n",
      "Epoch 49, Batch 100, Loss: 0.02268395759165287\n",
      "Epoch 50, Batch 0, Loss: 0.028346657752990723\n",
      "Epoch 50, Batch 10, Loss: 0.023472774773836136\n",
      "Epoch 50, Batch 20, Loss: 0.02645091339945793\n",
      "Epoch 50, Batch 30, Loss: 0.02088928408920765\n",
      "Epoch 50, Batch 40, Loss: 0.02799663133919239\n",
      "Epoch 50, Batch 50, Loss: 0.022280512377619743\n",
      "Epoch 50, Batch 60, Loss: 0.02727731503546238\n",
      "Epoch 50, Batch 70, Loss: 0.02215738222002983\n",
      "Epoch 50, Batch 80, Loss: 0.022718967869877815\n",
      "Epoch 50, Batch 90, Loss: 0.02445068396627903\n",
      "Epoch 50, Batch 100, Loss: 0.02523988112807274\n",
      "Epoch 51, Batch 0, Loss: 0.022308556362986565\n",
      "Epoch 51, Batch 10, Loss: 0.025797244161367416\n",
      "Epoch 51, Batch 20, Loss: 0.02240905351936817\n",
      "Epoch 51, Batch 30, Loss: 0.021539464592933655\n",
      "Epoch 51, Batch 40, Loss: 0.02277766913175583\n",
      "Epoch 51, Batch 50, Loss: 0.028575686737895012\n",
      "Epoch 51, Batch 60, Loss: 0.026886126026511192\n",
      "Epoch 51, Batch 70, Loss: 0.0211480762809515\n",
      "Epoch 51, Batch 80, Loss: 0.028741752728819847\n",
      "Epoch 51, Batch 90, Loss: 0.030455678701400757\n",
      "Epoch 51, Batch 100, Loss: 0.02023734524846077\n",
      "Epoch 52, Batch 0, Loss: 0.03205966204404831\n",
      "Epoch 52, Batch 10, Loss: 0.021165601909160614\n",
      "Epoch 52, Batch 20, Loss: 0.02999405935406685\n",
      "Epoch 52, Batch 30, Loss: 0.01958341896533966\n",
      "Epoch 52, Batch 40, Loss: 0.032795291393995285\n",
      "Epoch 52, Batch 50, Loss: 0.0332954078912735\n",
      "Epoch 52, Batch 60, Loss: 0.026302805170416832\n",
      "Epoch 52, Batch 70, Loss: 0.022294888272881508\n",
      "Epoch 52, Batch 80, Loss: 0.020796652883291245\n",
      "Epoch 52, Batch 90, Loss: 0.023314816877245903\n",
      "Epoch 52, Batch 100, Loss: 0.02308025397360325\n",
      "Epoch 53, Batch 0, Loss: 0.031491611152887344\n",
      "Epoch 53, Batch 10, Loss: 0.026678962633013725\n",
      "Epoch 53, Batch 20, Loss: 0.030157266184687614\n",
      "Epoch 53, Batch 30, Loss: 0.028330091387033463\n",
      "Epoch 53, Batch 40, Loss: 0.02733834832906723\n",
      "Epoch 53, Batch 50, Loss: 0.026394754648208618\n",
      "Epoch 53, Batch 60, Loss: 0.01778484508395195\n",
      "Epoch 53, Batch 70, Loss: 0.019022522494196892\n",
      "Epoch 53, Batch 80, Loss: 0.026397738605737686\n",
      "Epoch 53, Batch 90, Loss: 0.026642823591828346\n",
      "Epoch 53, Batch 100, Loss: 0.024629691615700722\n",
      "Epoch 54, Batch 0, Loss: 0.027772357687354088\n",
      "Epoch 54, Batch 10, Loss: 0.023867115378379822\n",
      "Epoch 54, Batch 20, Loss: 0.028061186894774437\n",
      "Epoch 54, Batch 30, Loss: 0.02920287661254406\n",
      "Epoch 54, Batch 40, Loss: 0.028177505359053612\n",
      "Epoch 54, Batch 50, Loss: 0.022154029458761215\n",
      "Epoch 54, Batch 60, Loss: 0.030151044949889183\n",
      "Epoch 54, Batch 70, Loss: 0.021155662834644318\n",
      "Epoch 54, Batch 80, Loss: 0.020994169637560844\n",
      "Epoch 54, Batch 90, Loss: 0.020896242931485176\n",
      "Epoch 54, Batch 100, Loss: 0.024105824530124664\n",
      "Epoch 55, Batch 0, Loss: 0.023910533636808395\n",
      "Epoch 55, Batch 10, Loss: 0.022576551884412766\n",
      "Epoch 55, Batch 20, Loss: 0.026882339268922806\n",
      "Epoch 55, Batch 30, Loss: 0.02171313762664795\n",
      "Epoch 55, Batch 40, Loss: 0.02344811148941517\n",
      "Epoch 55, Batch 50, Loss: 0.0253475122153759\n",
      "Epoch 55, Batch 60, Loss: 0.03009548783302307\n",
      "Epoch 55, Batch 70, Loss: 0.018911568447947502\n",
      "Epoch 55, Batch 80, Loss: 0.020159518346190453\n",
      "Epoch 55, Batch 90, Loss: 0.020942043513059616\n",
      "Epoch 55, Batch 100, Loss: 0.026281211525201797\n",
      "Epoch 56, Batch 0, Loss: 0.019584208726882935\n",
      "Epoch 56, Batch 10, Loss: 0.026500437408685684\n",
      "Epoch 56, Batch 20, Loss: 0.022359732538461685\n",
      "Epoch 56, Batch 30, Loss: 0.029675684869289398\n",
      "Epoch 56, Batch 40, Loss: 0.024651899933815002\n",
      "Epoch 56, Batch 50, Loss: 0.026798291131854057\n",
      "Epoch 56, Batch 60, Loss: 0.03185313940048218\n",
      "Epoch 56, Batch 70, Loss: 0.02503250166773796\n",
      "Epoch 56, Batch 80, Loss: 0.025214286521077156\n",
      "Epoch 56, Batch 90, Loss: 0.02516779489815235\n",
      "Epoch 56, Batch 100, Loss: 0.019581757485866547\n",
      "Epoch 57, Batch 0, Loss: 0.01704278215765953\n",
      "Epoch 57, Batch 10, Loss: 0.027777064591646194\n",
      "Epoch 57, Batch 20, Loss: 0.022012393921613693\n",
      "Epoch 57, Batch 30, Loss: 0.021241022273898125\n",
      "Epoch 57, Batch 40, Loss: 0.02196270041167736\n",
      "Epoch 57, Batch 50, Loss: 0.02927386201918125\n",
      "Epoch 57, Batch 60, Loss: 0.024819210171699524\n",
      "Epoch 57, Batch 70, Loss: 0.02722243033349514\n",
      "Epoch 57, Batch 80, Loss: 0.026145774871110916\n",
      "Epoch 57, Batch 90, Loss: 0.020988214761018753\n",
      "Epoch 57, Batch 100, Loss: 0.029764816164970398\n",
      "Epoch 58, Batch 0, Loss: 0.02957153506577015\n",
      "Epoch 58, Batch 10, Loss: 0.028353184461593628\n",
      "Epoch 58, Batch 20, Loss: 0.03236732259392738\n",
      "Epoch 58, Batch 30, Loss: 0.02066131681203842\n",
      "Epoch 58, Batch 40, Loss: 0.021215595304965973\n",
      "Epoch 58, Batch 50, Loss: 0.02748006395995617\n",
      "Epoch 58, Batch 60, Loss: 0.02616514451801777\n",
      "Epoch 58, Batch 70, Loss: 0.02150949463248253\n",
      "Epoch 58, Batch 80, Loss: 0.026501063257455826\n",
      "Epoch 58, Batch 90, Loss: 0.023078035563230515\n",
      "Epoch 58, Batch 100, Loss: 0.03392180800437927\n",
      "Epoch 59, Batch 0, Loss: 0.02019977755844593\n",
      "Epoch 59, Batch 10, Loss: 0.024264946579933167\n",
      "Epoch 59, Batch 20, Loss: 0.023863401263952255\n",
      "Epoch 59, Batch 30, Loss: 0.023524390533566475\n",
      "Epoch 59, Batch 40, Loss: 0.019865447655320168\n",
      "Epoch 59, Batch 50, Loss: 0.020988989621400833\n",
      "Epoch 59, Batch 60, Loss: 0.024173714220523834\n",
      "Epoch 59, Batch 70, Loss: 0.025549769401550293\n",
      "Epoch 59, Batch 80, Loss: 0.025355374440550804\n",
      "Epoch 59, Batch 90, Loss: 0.03412516415119171\n",
      "Epoch 59, Batch 100, Loss: 0.0284908264875412\n",
      "Epoch 60, Batch 0, Loss: 0.027598615735769272\n",
      "Epoch 60, Batch 10, Loss: 0.022678466513752937\n",
      "Epoch 60, Batch 20, Loss: 0.02574986033141613\n",
      "Epoch 60, Batch 30, Loss: 0.022815091535449028\n",
      "Epoch 60, Batch 40, Loss: 0.02693277597427368\n",
      "Epoch 60, Batch 50, Loss: 0.026568934321403503\n",
      "Epoch 60, Batch 60, Loss: 0.031222103163599968\n",
      "Epoch 60, Batch 70, Loss: 0.022399207577109337\n",
      "Epoch 60, Batch 80, Loss: 0.021117258816957474\n",
      "Epoch 60, Batch 90, Loss: 0.027482621371746063\n",
      "Epoch 60, Batch 100, Loss: 0.021699903532862663\n",
      "Epoch 61, Batch 0, Loss: 0.019912730902433395\n",
      "Epoch 61, Batch 10, Loss: 0.01632799208164215\n",
      "Epoch 61, Batch 20, Loss: 0.02013937383890152\n",
      "Epoch 61, Batch 30, Loss: 0.02078261598944664\n",
      "Epoch 61, Batch 40, Loss: 0.03075108304619789\n",
      "Epoch 61, Batch 50, Loss: 0.027663279324769974\n",
      "Epoch 61, Batch 60, Loss: 0.017762674018740654\n",
      "Epoch 61, Batch 70, Loss: 0.030666975304484367\n",
      "Epoch 61, Batch 80, Loss: 0.01936672069132328\n",
      "Epoch 61, Batch 90, Loss: 0.024369683116674423\n",
      "Epoch 61, Batch 100, Loss: 0.030180463567376137\n",
      "Epoch 62, Batch 0, Loss: 0.01719190739095211\n",
      "Epoch 62, Batch 10, Loss: 0.02272159978747368\n",
      "Epoch 62, Batch 20, Loss: 0.03252805024385452\n",
      "Epoch 62, Batch 30, Loss: 0.016955358907580376\n",
      "Epoch 62, Batch 40, Loss: 0.029200246557593346\n",
      "Epoch 62, Batch 50, Loss: 0.024549124762415886\n",
      "Epoch 62, Batch 60, Loss: 0.03148207440972328\n",
      "Epoch 62, Batch 70, Loss: 0.02145807445049286\n",
      "Epoch 62, Batch 80, Loss: 0.025458894670009613\n",
      "Epoch 62, Batch 90, Loss: 0.024526167660951614\n",
      "Epoch 62, Batch 100, Loss: 0.017575975507497787\n",
      "Epoch 63, Batch 0, Loss: 0.021157488226890564\n",
      "Epoch 63, Batch 10, Loss: 0.02044633962213993\n",
      "Epoch 63, Batch 20, Loss: 0.023691238835453987\n",
      "Epoch 63, Batch 30, Loss: 0.02195846475660801\n",
      "Epoch 63, Batch 40, Loss: 0.017343634739518166\n",
      "Epoch 63, Batch 50, Loss: 0.031212544068694115\n",
      "Epoch 63, Batch 60, Loss: 0.025142338126897812\n",
      "Epoch 63, Batch 70, Loss: 0.026870453730225563\n",
      "Epoch 63, Batch 80, Loss: 0.02941347286105156\n",
      "Epoch 63, Batch 90, Loss: 0.026272349059581757\n",
      "Epoch 63, Batch 100, Loss: 0.02679343894124031\n",
      "Epoch 64, Batch 0, Loss: 0.02825228124856949\n",
      "Epoch 64, Batch 10, Loss: 0.02134104073047638\n",
      "Epoch 64, Batch 20, Loss: 0.030922217294573784\n",
      "Epoch 64, Batch 30, Loss: 0.027211345732212067\n",
      "Epoch 64, Batch 40, Loss: 0.02589215710759163\n",
      "Epoch 64, Batch 50, Loss: 0.03176623582839966\n",
      "Epoch 64, Batch 60, Loss: 0.019564904272556305\n",
      "Epoch 64, Batch 70, Loss: 0.02390487678349018\n",
      "Epoch 64, Batch 80, Loss: 0.026273716241121292\n",
      "Epoch 64, Batch 90, Loss: 0.028729168698191643\n",
      "Epoch 64, Batch 100, Loss: 0.02100088633596897\n",
      "Epoch 65, Batch 0, Loss: 0.020691649988293648\n",
      "Epoch 65, Batch 10, Loss: 0.02000366523861885\n",
      "Epoch 65, Batch 20, Loss: 0.027742812409996986\n",
      "Epoch 65, Batch 30, Loss: 0.023221541196107864\n",
      "Epoch 65, Batch 40, Loss: 0.025253433734178543\n",
      "Epoch 65, Batch 50, Loss: 0.021070823073387146\n",
      "Epoch 65, Batch 60, Loss: 0.02270645461976528\n",
      "Epoch 65, Batch 70, Loss: 0.024618729948997498\n",
      "Epoch 65, Batch 80, Loss: 0.018647998571395874\n",
      "Epoch 65, Batch 90, Loss: 0.020507989451289177\n",
      "Epoch 65, Batch 100, Loss: 0.027729202061891556\n",
      "Epoch 66, Batch 0, Loss: 0.02697008289396763\n",
      "Epoch 66, Batch 10, Loss: 0.025007829070091248\n",
      "Epoch 66, Batch 20, Loss: 0.023028206080198288\n",
      "Epoch 66, Batch 30, Loss: 0.020665768533945084\n",
      "Epoch 66, Batch 40, Loss: 0.025531183928251266\n",
      "Epoch 66, Batch 50, Loss: 0.028753552585840225\n",
      "Epoch 66, Batch 60, Loss: 0.028834078460931778\n",
      "Epoch 66, Batch 70, Loss: 0.02781734988093376\n",
      "Epoch 66, Batch 80, Loss: 0.0249351616948843\n",
      "Epoch 66, Batch 90, Loss: 0.015855863690376282\n",
      "Epoch 66, Batch 100, Loss: 0.024232886731624603\n",
      "Epoch 67, Batch 0, Loss: 0.022758515551686287\n",
      "Epoch 67, Batch 10, Loss: 0.027117948979139328\n",
      "Epoch 67, Batch 20, Loss: 0.027611877769231796\n",
      "Epoch 67, Batch 30, Loss: 0.026671327650547028\n",
      "Epoch 67, Batch 40, Loss: 0.030665474012494087\n",
      "Epoch 67, Batch 50, Loss: 0.022712744772434235\n",
      "Epoch 67, Batch 60, Loss: 0.022920796647667885\n",
      "Epoch 67, Batch 70, Loss: 0.025758711621165276\n",
      "Epoch 67, Batch 80, Loss: 0.023099549114704132\n",
      "Epoch 67, Batch 90, Loss: 0.021934106945991516\n",
      "Epoch 67, Batch 100, Loss: 0.022204207256436348\n",
      "Epoch 68, Batch 0, Loss: 0.020078251138329506\n",
      "Epoch 68, Batch 10, Loss: 0.025246931239962578\n",
      "Epoch 68, Batch 20, Loss: 0.02528292126953602\n",
      "Epoch 68, Batch 30, Loss: 0.020005177706480026\n",
      "Epoch 68, Batch 40, Loss: 0.02315470017492771\n",
      "Epoch 68, Batch 50, Loss: 0.024544626474380493\n",
      "Epoch 68, Batch 60, Loss: 0.026466790586709976\n",
      "Epoch 68, Batch 70, Loss: 0.03450879454612732\n",
      "Epoch 68, Batch 80, Loss: 0.03056381084024906\n",
      "Epoch 68, Batch 90, Loss: 0.018152428790926933\n",
      "Epoch 68, Batch 100, Loss: 0.02419014647603035\n",
      "Epoch 69, Batch 0, Loss: 0.02522905543446541\n",
      "Epoch 69, Batch 10, Loss: 0.022226573899388313\n",
      "Epoch 69, Batch 20, Loss: 0.018575483933091164\n",
      "Epoch 69, Batch 30, Loss: 0.019993042573332787\n",
      "Epoch 69, Batch 40, Loss: 0.01948370411992073\n",
      "Epoch 69, Batch 50, Loss: 0.024145301431417465\n",
      "Epoch 69, Batch 60, Loss: 0.021179644390940666\n",
      "Epoch 69, Batch 70, Loss: 0.03022588975727558\n",
      "Epoch 69, Batch 80, Loss: 0.02207755111157894\n",
      "Epoch 69, Batch 90, Loss: 0.02537728101015091\n",
      "Epoch 69, Batch 100, Loss: 0.029075903818011284\n",
      "Epoch 70, Batch 0, Loss: 0.02682337909936905\n",
      "Epoch 70, Batch 10, Loss: 0.026698501780629158\n",
      "Epoch 70, Batch 20, Loss: 0.028667692095041275\n",
      "Epoch 70, Batch 30, Loss: 0.02559344656765461\n",
      "Epoch 70, Batch 40, Loss: 0.027042893692851067\n",
      "Epoch 70, Batch 50, Loss: 0.027627188712358475\n",
      "Epoch 70, Batch 60, Loss: 0.019702693447470665\n",
      "Epoch 70, Batch 70, Loss: 0.02220340073108673\n",
      "Epoch 70, Batch 80, Loss: 0.017158709466457367\n",
      "Epoch 70, Batch 90, Loss: 0.032499391585588455\n",
      "Epoch 70, Batch 100, Loss: 0.023980211466550827\n",
      "Epoch 71, Batch 0, Loss: 0.0185953788459301\n",
      "Epoch 71, Batch 10, Loss: 0.022696562111377716\n",
      "Epoch 71, Batch 20, Loss: 0.029954561963677406\n",
      "Epoch 71, Batch 30, Loss: 0.025805458426475525\n",
      "Epoch 71, Batch 40, Loss: 0.018434571102261543\n",
      "Epoch 71, Batch 50, Loss: 0.022297721356153488\n",
      "Epoch 71, Batch 60, Loss: 0.029527317732572556\n",
      "Epoch 71, Batch 70, Loss: 0.017110217362642288\n",
      "Epoch 71, Batch 80, Loss: 0.02843450754880905\n",
      "Epoch 71, Batch 90, Loss: 0.024553196504712105\n",
      "Epoch 71, Batch 100, Loss: 0.024272626265883446\n",
      "Epoch 72, Batch 0, Loss: 0.017319776117801666\n",
      "Epoch 72, Batch 10, Loss: 0.035930752754211426\n",
      "Epoch 72, Batch 20, Loss: 0.02692914567887783\n",
      "Epoch 72, Batch 30, Loss: 0.022626714780926704\n",
      "Epoch 72, Batch 40, Loss: 0.02571577951312065\n",
      "Epoch 72, Batch 50, Loss: 0.03339097648859024\n",
      "Epoch 72, Batch 60, Loss: 0.029697222635149956\n",
      "Epoch 72, Batch 70, Loss: 0.021086975932121277\n",
      "Epoch 72, Batch 80, Loss: 0.031955912709236145\n",
      "Epoch 72, Batch 90, Loss: 0.024626167491078377\n",
      "Epoch 72, Batch 100, Loss: 0.027741486206650734\n",
      "Epoch 73, Batch 0, Loss: 0.02059607394039631\n",
      "Epoch 73, Batch 10, Loss: 0.027992727234959602\n",
      "Epoch 73, Batch 20, Loss: 0.021549275144934654\n",
      "Epoch 73, Batch 30, Loss: 0.022136038169264793\n",
      "Epoch 73, Batch 40, Loss: 0.025582974776625633\n",
      "Epoch 73, Batch 50, Loss: 0.024311700835824013\n",
      "Epoch 73, Batch 60, Loss: 0.02225586026906967\n",
      "Epoch 73, Batch 70, Loss: 0.030757375061511993\n",
      "Epoch 73, Batch 80, Loss: 0.02090592123568058\n",
      "Epoch 73, Batch 90, Loss: 0.020376116037368774\n",
      "Epoch 73, Batch 100, Loss: 0.029644779860973358\n",
      "Epoch 74, Batch 0, Loss: 0.023517141118645668\n",
      "Epoch 74, Batch 10, Loss: 0.025473395362496376\n",
      "Epoch 74, Batch 20, Loss: 0.030837219208478928\n",
      "Epoch 74, Batch 30, Loss: 0.0209125317633152\n",
      "Epoch 74, Batch 40, Loss: 0.025831134989857674\n",
      "Epoch 74, Batch 50, Loss: 0.03556236997246742\n",
      "Epoch 74, Batch 60, Loss: 0.027743663638830185\n",
      "Epoch 74, Batch 70, Loss: 0.02458898350596428\n",
      "Epoch 74, Batch 80, Loss: 0.023637188598513603\n",
      "Epoch 74, Batch 90, Loss: 0.022075247019529343\n",
      "Epoch 74, Batch 100, Loss: 0.02140958234667778\n",
      "Epoch 75, Batch 0, Loss: 0.021414291113615036\n",
      "Epoch 75, Batch 10, Loss: 0.023088717833161354\n",
      "Epoch 75, Batch 20, Loss: 0.025606831535696983\n",
      "Epoch 75, Batch 30, Loss: 0.0216766856610775\n",
      "Epoch 75, Batch 40, Loss: 0.020223895087838173\n",
      "Epoch 75, Batch 50, Loss: 0.025564320385456085\n",
      "Epoch 75, Batch 60, Loss: 0.028109336271882057\n",
      "Epoch 75, Batch 70, Loss: 0.024020403623580933\n",
      "Epoch 75, Batch 80, Loss: 0.019416121765971184\n",
      "Epoch 75, Batch 90, Loss: 0.022881973534822464\n",
      "Epoch 75, Batch 100, Loss: 0.03192486613988876\n",
      "Epoch 76, Batch 0, Loss: 0.021826764568686485\n",
      "Epoch 76, Batch 10, Loss: 0.02194352261722088\n",
      "Epoch 76, Batch 20, Loss: 0.02468058466911316\n",
      "Epoch 76, Batch 30, Loss: 0.02904086373746395\n",
      "Epoch 76, Batch 40, Loss: 0.020727910101413727\n",
      "Epoch 76, Batch 50, Loss: 0.02338520810008049\n",
      "Epoch 76, Batch 60, Loss: 0.02817564085125923\n",
      "Epoch 76, Batch 70, Loss: 0.021838197484612465\n",
      "Epoch 76, Batch 80, Loss: 0.022193171083927155\n",
      "Epoch 76, Batch 90, Loss: 0.01729753240942955\n",
      "Epoch 76, Batch 100, Loss: 0.02112499624490738\n",
      "Epoch 77, Batch 0, Loss: 0.02920473739504814\n",
      "Epoch 77, Batch 10, Loss: 0.032483987510204315\n",
      "Epoch 77, Batch 20, Loss: 0.023358767852187157\n",
      "Epoch 77, Batch 30, Loss: 0.022876765578985214\n",
      "Epoch 77, Batch 40, Loss: 0.02061666175723076\n",
      "Epoch 77, Batch 50, Loss: 0.025447221472859383\n",
      "Epoch 77, Batch 60, Loss: 0.02729937620460987\n",
      "Epoch 77, Batch 70, Loss: 0.022710060700774193\n",
      "Epoch 77, Batch 80, Loss: 0.033769406378269196\n",
      "Epoch 77, Batch 90, Loss: 0.021917548030614853\n",
      "Epoch 77, Batch 100, Loss: 0.026286393404006958\n",
      "Epoch 78, Batch 0, Loss: 0.016972700133919716\n",
      "Epoch 78, Batch 10, Loss: 0.02419818565249443\n",
      "Epoch 78, Batch 20, Loss: 0.02537676692008972\n",
      "Epoch 78, Batch 30, Loss: 0.027671312913298607\n",
      "Epoch 78, Batch 40, Loss: 0.024885015562176704\n",
      "Epoch 78, Batch 50, Loss: 0.028235051780939102\n",
      "Epoch 78, Batch 60, Loss: 0.031040864065289497\n",
      "Epoch 78, Batch 70, Loss: 0.025743845850229263\n",
      "Epoch 78, Batch 80, Loss: 0.02755402773618698\n",
      "Epoch 78, Batch 90, Loss: 0.027898520231246948\n",
      "Epoch 78, Batch 100, Loss: 0.02107429876923561\n",
      "Epoch 79, Batch 0, Loss: 0.018034279346466064\n",
      "Epoch 79, Batch 10, Loss: 0.027714582160115242\n",
      "Epoch 79, Batch 20, Loss: 0.02589961141347885\n",
      "Epoch 79, Batch 30, Loss: 0.02001904882490635\n",
      "Epoch 79, Batch 40, Loss: 0.01958771049976349\n",
      "Epoch 79, Batch 50, Loss: 0.02589176408946514\n",
      "Epoch 79, Batch 60, Loss: 0.02208482287824154\n",
      "Epoch 79, Batch 70, Loss: 0.02596832439303398\n",
      "Epoch 79, Batch 80, Loss: 0.02535528875887394\n",
      "Epoch 79, Batch 90, Loss: 0.02270578406751156\n",
      "Epoch 79, Batch 100, Loss: 0.024406673386693\n",
      "Epoch 80, Batch 0, Loss: 0.03023126907646656\n",
      "Epoch 80, Batch 10, Loss: 0.02335681952536106\n",
      "Epoch 80, Batch 20, Loss: 0.022000810131430626\n",
      "Epoch 80, Batch 30, Loss: 0.03895873576402664\n",
      "Epoch 80, Batch 40, Loss: 0.02536299079656601\n",
      "Epoch 80, Batch 50, Loss: 0.025576140731573105\n",
      "Epoch 80, Batch 60, Loss: 0.02108234539628029\n",
      "Epoch 80, Batch 70, Loss: 0.025076834484934807\n",
      "Epoch 80, Batch 80, Loss: 0.02728215418756008\n",
      "Epoch 80, Batch 90, Loss: 0.024081028997898102\n",
      "Epoch 80, Batch 100, Loss: 0.021735606715083122\n",
      "Epoch 81, Batch 0, Loss: 0.025659693405032158\n",
      "Epoch 81, Batch 10, Loss: 0.02388937771320343\n",
      "Epoch 81, Batch 20, Loss: 0.020701570436358452\n",
      "Epoch 81, Batch 30, Loss: 0.025785736739635468\n",
      "Epoch 81, Batch 40, Loss: 0.03770805895328522\n",
      "Epoch 81, Batch 50, Loss: 0.02129860781133175\n",
      "Epoch 81, Batch 60, Loss: 0.02129342034459114\n",
      "Epoch 81, Batch 70, Loss: 0.026369508355855942\n",
      "Epoch 81, Batch 80, Loss: 0.02428818680346012\n",
      "Epoch 81, Batch 90, Loss: 0.033551983535289764\n",
      "Epoch 81, Batch 100, Loss: 0.027541056275367737\n",
      "Epoch 82, Batch 0, Loss: 0.02155045047402382\n",
      "Epoch 82, Batch 10, Loss: 0.01949675753712654\n",
      "Epoch 82, Batch 20, Loss: 0.02084829844534397\n",
      "Epoch 82, Batch 30, Loss: 0.019462652504444122\n",
      "Epoch 82, Batch 40, Loss: 0.021200573071837425\n",
      "Epoch 82, Batch 50, Loss: 0.02805253118276596\n",
      "Epoch 82, Batch 60, Loss: 0.01882382296025753\n",
      "Epoch 82, Batch 70, Loss: 0.025413844734430313\n",
      "Epoch 82, Batch 80, Loss: 0.024534812197089195\n",
      "Epoch 82, Batch 90, Loss: 0.023776747286319733\n",
      "Epoch 82, Batch 100, Loss: 0.02704562619328499\n",
      "Epoch 83, Batch 0, Loss: 0.03580894321203232\n",
      "Epoch 83, Batch 10, Loss: 0.03143085166811943\n",
      "Epoch 83, Batch 20, Loss: 0.02378685027360916\n",
      "Epoch 83, Batch 30, Loss: 0.022431958466768265\n",
      "Epoch 83, Batch 40, Loss: 0.03000362031161785\n",
      "Epoch 83, Batch 50, Loss: 0.023976370692253113\n",
      "Epoch 83, Batch 60, Loss: 0.022648632526397705\n",
      "Epoch 83, Batch 70, Loss: 0.027227893471717834\n",
      "Epoch 83, Batch 80, Loss: 0.02159683033823967\n",
      "Epoch 83, Batch 90, Loss: 0.02513512223958969\n",
      "Epoch 83, Batch 100, Loss: 0.02896450273692608\n",
      "Epoch 84, Batch 0, Loss: 0.02886226214468479\n",
      "Epoch 84, Batch 10, Loss: 0.025216374546289444\n",
      "Epoch 84, Batch 20, Loss: 0.02125682681798935\n",
      "Epoch 84, Batch 30, Loss: 0.022266371175646782\n",
      "Epoch 84, Batch 40, Loss: 0.02930465154349804\n",
      "Epoch 84, Batch 50, Loss: 0.02499961666762829\n",
      "Epoch 84, Batch 60, Loss: 0.026089990511536598\n",
      "Epoch 84, Batch 70, Loss: 0.02549658715724945\n",
      "Epoch 84, Batch 80, Loss: 0.02838130295276642\n",
      "Epoch 84, Batch 90, Loss: 0.03070804849267006\n",
      "Epoch 84, Batch 100, Loss: 0.018009481951594353\n",
      "Epoch 85, Batch 0, Loss: 0.023341147229075432\n",
      "Epoch 85, Batch 10, Loss: 0.020817380398511887\n",
      "Epoch 85, Batch 20, Loss: 0.02410985715687275\n",
      "Epoch 85, Batch 30, Loss: 0.024788130074739456\n",
      "Epoch 85, Batch 40, Loss: 0.01879987306892872\n",
      "Epoch 85, Batch 50, Loss: 0.019423549994826317\n",
      "Epoch 85, Batch 60, Loss: 0.01859092526137829\n",
      "Epoch 85, Batch 70, Loss: 0.021197322756052017\n",
      "Epoch 85, Batch 80, Loss: 0.027150021865963936\n",
      "Epoch 85, Batch 90, Loss: 0.021039124578237534\n",
      "Epoch 85, Batch 100, Loss: 0.02670145593583584\n",
      "Epoch 86, Batch 0, Loss: 0.01937164179980755\n",
      "Epoch 86, Batch 10, Loss: 0.022963697090744972\n",
      "Epoch 86, Batch 20, Loss: 0.022782379761338234\n",
      "Epoch 86, Batch 30, Loss: 0.025745032355189323\n",
      "Epoch 86, Batch 40, Loss: 0.014295807108283043\n",
      "Epoch 86, Batch 50, Loss: 0.02260434255003929\n",
      "Epoch 86, Batch 60, Loss: 0.019243011251091957\n",
      "Epoch 86, Batch 70, Loss: 0.024020420387387276\n",
      "Epoch 86, Batch 80, Loss: 0.027546662837266922\n",
      "Epoch 86, Batch 90, Loss: 0.03045636974275112\n",
      "Epoch 86, Batch 100, Loss: 0.027823075652122498\n",
      "Epoch 87, Batch 0, Loss: 0.02475018799304962\n",
      "Epoch 87, Batch 10, Loss: 0.022453490644693375\n",
      "Epoch 87, Batch 20, Loss: 0.038644012063741684\n",
      "Epoch 87, Batch 30, Loss: 0.029748061671853065\n",
      "Epoch 87, Batch 40, Loss: 0.024506809189915657\n",
      "Epoch 87, Batch 50, Loss: 0.025003351271152496\n",
      "Epoch 87, Batch 60, Loss: 0.02404274046421051\n",
      "Epoch 87, Batch 70, Loss: 0.0241398848593235\n",
      "Epoch 87, Batch 80, Loss: 0.021780813112854958\n",
      "Epoch 87, Batch 90, Loss: 0.021748336032032967\n",
      "Epoch 87, Batch 100, Loss: 0.02174491062760353\n",
      "Epoch 88, Batch 0, Loss: 0.02487458847463131\n",
      "Epoch 88, Batch 10, Loss: 0.018950220197439194\n",
      "Epoch 88, Batch 20, Loss: 0.021359581500291824\n",
      "Epoch 88, Batch 30, Loss: 0.025937190279364586\n",
      "Epoch 88, Batch 40, Loss: 0.026851410046219826\n",
      "Epoch 88, Batch 50, Loss: 0.026405004784464836\n",
      "Epoch 88, Batch 60, Loss: 0.027984218671917915\n",
      "Epoch 88, Batch 70, Loss: 0.019032591953873634\n",
      "Epoch 88, Batch 80, Loss: 0.024782324209809303\n",
      "Epoch 88, Batch 90, Loss: 0.021650301292538643\n",
      "Epoch 88, Batch 100, Loss: 0.017785942181944847\n",
      "Epoch 89, Batch 0, Loss: 0.02264558896422386\n",
      "Epoch 89, Batch 10, Loss: 0.017216475680470467\n",
      "Epoch 89, Batch 20, Loss: 0.032217033207416534\n",
      "Epoch 89, Batch 30, Loss: 0.025023961439728737\n",
      "Epoch 89, Batch 40, Loss: 0.02715069055557251\n",
      "Epoch 89, Batch 50, Loss: 0.03324630483984947\n",
      "Epoch 89, Batch 60, Loss: 0.01967877894639969\n",
      "Epoch 89, Batch 70, Loss: 0.02580445073544979\n",
      "Epoch 89, Batch 80, Loss: 0.025631079450249672\n",
      "Epoch 89, Batch 90, Loss: 0.02321883849799633\n",
      "Epoch 89, Batch 100, Loss: 0.024191496893763542\n",
      "Epoch 90, Batch 0, Loss: 0.023273319005966187\n",
      "Epoch 90, Batch 10, Loss: 0.023010578006505966\n",
      "Epoch 90, Batch 20, Loss: 0.017123349010944366\n",
      "Epoch 90, Batch 30, Loss: 0.02750633843243122\n",
      "Epoch 90, Batch 40, Loss: 0.0233711376786232\n",
      "Epoch 90, Batch 50, Loss: 0.031324636191129684\n",
      "Epoch 90, Batch 60, Loss: 0.018227536231279373\n",
      "Epoch 90, Batch 70, Loss: 0.030797556042671204\n",
      "Epoch 90, Batch 80, Loss: 0.02033311314880848\n",
      "Epoch 90, Batch 90, Loss: 0.02343526855111122\n",
      "Epoch 90, Batch 100, Loss: 0.025111239403486252\n",
      "Epoch 91, Batch 0, Loss: 0.02125268243253231\n",
      "Epoch 91, Batch 10, Loss: 0.022661389783024788\n",
      "Epoch 91, Batch 20, Loss: 0.03320453315973282\n",
      "Epoch 91, Batch 30, Loss: 0.01875065453350544\n",
      "Epoch 91, Batch 40, Loss: 0.030587462708353996\n",
      "Epoch 91, Batch 50, Loss: 0.023633792996406555\n",
      "Epoch 91, Batch 60, Loss: 0.028009874746203423\n",
      "Epoch 91, Batch 70, Loss: 0.022535577416419983\n",
      "Epoch 91, Batch 80, Loss: 0.02213476039469242\n",
      "Epoch 91, Batch 90, Loss: 0.03161397576332092\n",
      "Epoch 91, Batch 100, Loss: 0.02008805051445961\n",
      "Epoch 92, Batch 0, Loss: 0.020820461213588715\n",
      "Epoch 92, Batch 10, Loss: 0.020658310502767563\n",
      "Epoch 92, Batch 20, Loss: 0.022664152085781097\n",
      "Epoch 92, Batch 30, Loss: 0.02034010738134384\n",
      "Epoch 92, Batch 40, Loss: 0.022480079904198647\n",
      "Epoch 92, Batch 50, Loss: 0.029040904715657234\n",
      "Epoch 92, Batch 60, Loss: 0.016147881746292114\n",
      "Epoch 92, Batch 70, Loss: 0.024822067469358444\n",
      "Epoch 92, Batch 80, Loss: 0.019937394186854362\n",
      "Epoch 92, Batch 90, Loss: 0.02162124030292034\n",
      "Epoch 92, Batch 100, Loss: 0.016936833038926125\n",
      "Epoch 93, Batch 0, Loss: 0.02453450858592987\n",
      "Epoch 93, Batch 10, Loss: 0.02490391954779625\n",
      "Epoch 93, Batch 20, Loss: 0.027758298441767693\n",
      "Epoch 93, Batch 30, Loss: 0.020526206120848656\n",
      "Epoch 93, Batch 40, Loss: 0.02462100051343441\n",
      "Epoch 93, Batch 50, Loss: 0.025767888873815536\n",
      "Epoch 93, Batch 60, Loss: 0.020214056596159935\n",
      "Epoch 93, Batch 70, Loss: 0.03591872379183769\n",
      "Epoch 93, Batch 80, Loss: 0.025820964947342873\n",
      "Epoch 93, Batch 90, Loss: 0.02568298578262329\n",
      "Epoch 93, Batch 100, Loss: 0.0204642154276371\n",
      "Epoch 94, Batch 0, Loss: 0.02189934253692627\n",
      "Epoch 94, Batch 10, Loss: 0.02256416156888008\n",
      "Epoch 94, Batch 20, Loss: 0.020410247147083282\n",
      "Epoch 94, Batch 30, Loss: 0.016881240531802177\n",
      "Epoch 94, Batch 40, Loss: 0.019278310239315033\n",
      "Epoch 94, Batch 50, Loss: 0.022784382104873657\n",
      "Epoch 94, Batch 60, Loss: 0.029324090108275414\n",
      "Epoch 94, Batch 70, Loss: 0.024094833061099052\n",
      "Epoch 94, Batch 80, Loss: 0.019736705347895622\n",
      "Epoch 94, Batch 90, Loss: 0.03079087845981121\n",
      "Epoch 94, Batch 100, Loss: 0.024683281779289246\n",
      "Epoch 95, Batch 0, Loss: 0.029856346547603607\n",
      "Epoch 95, Batch 10, Loss: 0.028957460075616837\n",
      "Epoch 95, Batch 20, Loss: 0.02542647160589695\n",
      "Epoch 95, Batch 30, Loss: 0.022620001807808876\n",
      "Epoch 95, Batch 40, Loss: 0.033778995275497437\n",
      "Epoch 95, Batch 50, Loss: 0.02085462212562561\n",
      "Epoch 95, Batch 60, Loss: 0.018643125891685486\n",
      "Epoch 95, Batch 70, Loss: 0.024356627836823463\n",
      "Epoch 95, Batch 80, Loss: 0.023536773398518562\n",
      "Epoch 95, Batch 90, Loss: 0.02446899004280567\n",
      "Epoch 95, Batch 100, Loss: 0.024213511496782303\n",
      "Epoch 96, Batch 0, Loss: 0.02015729248523712\n",
      "Epoch 96, Batch 10, Loss: 0.02676941081881523\n",
      "Epoch 96, Batch 20, Loss: 0.021142009645700455\n",
      "Epoch 96, Batch 30, Loss: 0.020928608253598213\n",
      "Epoch 96, Batch 40, Loss: 0.02230571024119854\n",
      "Epoch 96, Batch 50, Loss: 0.017720995470881462\n",
      "Epoch 96, Batch 60, Loss: 0.023434216156601906\n",
      "Epoch 96, Batch 70, Loss: 0.021153822541236877\n",
      "Epoch 96, Batch 80, Loss: 0.023836124688386917\n",
      "Epoch 96, Batch 90, Loss: 0.021226240321993828\n",
      "Epoch 96, Batch 100, Loss: 0.03160451352596283\n",
      "Epoch 97, Batch 0, Loss: 0.028449293226003647\n",
      "Epoch 97, Batch 10, Loss: 0.024319104850292206\n",
      "Epoch 97, Batch 20, Loss: 0.024248741567134857\n",
      "Epoch 97, Batch 30, Loss: 0.02336334064602852\n",
      "Epoch 97, Batch 40, Loss: 0.02024839259684086\n",
      "Epoch 97, Batch 50, Loss: 0.02219690941274166\n",
      "Epoch 97, Batch 60, Loss: 0.024106081575155258\n",
      "Epoch 97, Batch 70, Loss: 0.022507131099700928\n",
      "Epoch 97, Batch 80, Loss: 0.022452564910054207\n",
      "Epoch 97, Batch 90, Loss: 0.02234680950641632\n",
      "Epoch 97, Batch 100, Loss: 0.019282523542642593\n",
      "Epoch 98, Batch 0, Loss: 0.018983742222189903\n",
      "Epoch 98, Batch 10, Loss: 0.019761087372899055\n",
      "Epoch 98, Batch 20, Loss: 0.018348196521401405\n",
      "Epoch 98, Batch 30, Loss: 0.021129194647073746\n",
      "Epoch 98, Batch 40, Loss: 0.028213413432240486\n",
      "Epoch 98, Batch 50, Loss: 0.026105916127562523\n",
      "Epoch 98, Batch 60, Loss: 0.03672463819384575\n",
      "Epoch 98, Batch 70, Loss: 0.02050325646996498\n",
      "Epoch 98, Batch 80, Loss: 0.025297746062278748\n",
      "Epoch 98, Batch 90, Loss: 0.02910572662949562\n",
      "Epoch 98, Batch 100, Loss: 0.02730688825249672\n",
      "Epoch 99, Batch 0, Loss: 0.02613135054707527\n",
      "Epoch 99, Batch 10, Loss: 0.02348722144961357\n",
      "Epoch 99, Batch 20, Loss: 0.0215245820581913\n",
      "Epoch 99, Batch 30, Loss: 0.02160884067416191\n",
      "Epoch 99, Batch 40, Loss: 0.03136057406663895\n",
      "Epoch 99, Batch 50, Loss: 0.018301431089639664\n",
      "Epoch 99, Batch 60, Loss: 0.025447659194469452\n",
      "Epoch 99, Batch 70, Loss: 0.021206218749284744\n",
      "Epoch 99, Batch 80, Loss: 0.02536114677786827\n",
      "Epoch 99, Batch 90, Loss: 0.022030841559171677\n",
      "Epoch 99, Batch 100, Loss: 0.02454439550638199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_BILSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (bilstm): BILSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True, bidirectional=True)\n",
       "    (fc1): Linear(in_features=128, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.041022; Test RMSE 38.856445\n",
      "\n",
      "Train  MAE: 0.023681; Test  MAE 27.215050\n",
      "\n",
      "Train  R^2: 0.998321; Test  R^2 0.965925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
