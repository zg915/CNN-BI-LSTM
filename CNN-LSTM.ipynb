{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_preprocessing import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6618, 10, 8]) torch.Size([6618, 1]) torch.Size([490, 10, 8]) (490, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, y_scaler, train_loader, test_loader = get_data(\n",
    "            timestep = 10,\n",
    "            batch_size = 64,\n",
    "            y_name = 'Closing price',\n",
    "            train_size = 6627,\n",
    "            path = \"Data/SSE000001.csv\")\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    print (\"GPU device not found.\")\n",
    "\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer parameters from the table\n",
    "        self.conv1d = nn.Conv1d(in_channels= 8, out_channels= 32, kernel_size= 1, padding=\"same\")\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Pooling\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size= 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layer\n",
    "        x = self.conv1d(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64, lstm_dropout = 0.2, fc1_output_size = 16):\n",
    "        super(BILSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True,\n",
    "                             bidirectional = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size * 2, self.fc1_output_size)\n",
    "        self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        h_fc2 = self.fc2(h_fc1)\n",
    "        output = h_fc2[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64,  fc1_output_size = 16):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        # self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p = self.lstm_dropout)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size, self.fc1_output_size)\n",
    "        self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "        # h_dropout = self.dropout(h_lstm)\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        h_fc2 = self.fc2(h_fc1)\n",
    "        output = h_fc2[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.lstm = LSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x)\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Reshape x to fit BiLSTM input requirements\n",
    "        x = x.contiguous().view(batch_size, seq_len, -1)\n",
    "        x = self.lstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 10 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 20\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.8401562571525574\n",
      "Epoch 0, Batch 10, Loss: 0.7411416172981262\n",
      "Epoch 0, Batch 20, Loss: 0.747985303401947\n",
      "Epoch 0, Batch 30, Loss: 0.7415758371353149\n",
      "Epoch 0, Batch 40, Loss: 0.7992125749588013\n",
      "Epoch 0, Batch 50, Loss: 0.7775716781616211\n",
      "Epoch 0, Batch 60, Loss: 0.8420056104660034\n",
      "Epoch 0, Batch 70, Loss: 0.8659330010414124\n",
      "Epoch 0, Batch 80, Loss: 0.8858482241630554\n",
      "Epoch 0, Batch 90, Loss: 0.7847834229469299\n",
      "Epoch 0, Batch 100, Loss: 0.7281306385993958\n",
      "Epoch 1, Batch 0, Loss: 0.854185163974762\n",
      "Epoch 1, Batch 10, Loss: 0.7755770683288574\n",
      "Epoch 1, Batch 20, Loss: 0.9179984331130981\n",
      "Epoch 1, Batch 30, Loss: 0.8442075252532959\n",
      "Epoch 1, Batch 40, Loss: 0.85542231798172\n",
      "Epoch 1, Batch 50, Loss: 0.8503406643867493\n",
      "Epoch 1, Batch 60, Loss: 0.7518830895423889\n",
      "Epoch 1, Batch 70, Loss: 0.7183777093887329\n",
      "Epoch 1, Batch 80, Loss: 0.804106593132019\n",
      "Epoch 1, Batch 90, Loss: 0.8458857536315918\n",
      "Epoch 1, Batch 100, Loss: 0.781325101852417\n",
      "Epoch 2, Batch 0, Loss: 1.0457748174667358\n",
      "Epoch 2, Batch 10, Loss: 0.869259238243103\n",
      "Epoch 2, Batch 20, Loss: 0.8192794322967529\n",
      "Epoch 2, Batch 30, Loss: 0.8473027944564819\n",
      "Epoch 2, Batch 40, Loss: 0.7980263233184814\n",
      "Epoch 2, Batch 50, Loss: 0.7576687335968018\n",
      "Epoch 2, Batch 60, Loss: 0.8608660697937012\n",
      "Epoch 2, Batch 70, Loss: 0.9911518096923828\n",
      "Epoch 2, Batch 80, Loss: 0.8543025851249695\n",
      "Epoch 2, Batch 90, Loss: 0.9117322564125061\n",
      "Epoch 2, Batch 100, Loss: 0.6020008325576782\n",
      "Epoch 3, Batch 0, Loss: 0.769773542881012\n",
      "Epoch 3, Batch 10, Loss: 0.7480593919754028\n",
      "Epoch 3, Batch 20, Loss: 0.7225257754325867\n",
      "Epoch 3, Batch 30, Loss: 0.81145179271698\n",
      "Epoch 3, Batch 40, Loss: 0.7942748665809631\n",
      "Epoch 3, Batch 50, Loss: 0.7905559539794922\n",
      "Epoch 3, Batch 60, Loss: 0.968070387840271\n",
      "Epoch 3, Batch 70, Loss: 0.7288475036621094\n",
      "Epoch 3, Batch 80, Loss: 0.8376287221908569\n",
      "Epoch 3, Batch 90, Loss: 0.808489203453064\n",
      "Epoch 3, Batch 100, Loss: 0.8378841876983643\n",
      "Epoch 4, Batch 0, Loss: 0.9030769467353821\n",
      "Epoch 4, Batch 10, Loss: 0.9328804612159729\n",
      "Epoch 4, Batch 20, Loss: 0.948492169380188\n",
      "Epoch 4, Batch 30, Loss: 0.7752590775489807\n",
      "Epoch 4, Batch 40, Loss: 0.8000679016113281\n",
      "Epoch 4, Batch 50, Loss: 0.677274227142334\n",
      "Epoch 4, Batch 60, Loss: 0.8016393780708313\n",
      "Epoch 4, Batch 70, Loss: 0.764748752117157\n",
      "Epoch 4, Batch 80, Loss: 0.7249889969825745\n",
      "Epoch 4, Batch 90, Loss: 0.7810110449790955\n",
      "Epoch 4, Batch 100, Loss: 0.8816788792610168\n",
      "Epoch 5, Batch 0, Loss: 0.7613128423690796\n",
      "Epoch 5, Batch 10, Loss: 0.8099372386932373\n",
      "Epoch 5, Batch 20, Loss: 0.718019962310791\n",
      "Epoch 5, Batch 30, Loss: 0.7894366979598999\n",
      "Epoch 5, Batch 40, Loss: 0.7641664147377014\n",
      "Epoch 5, Batch 50, Loss: 0.8243207335472107\n",
      "Epoch 5, Batch 60, Loss: 0.8770002126693726\n",
      "Epoch 5, Batch 70, Loss: 0.8067359328269958\n",
      "Epoch 5, Batch 80, Loss: 0.9152265787124634\n",
      "Epoch 5, Batch 90, Loss: 0.9216607809066772\n",
      "Epoch 5, Batch 100, Loss: 0.805150032043457\n",
      "Epoch 6, Batch 0, Loss: 0.9941086769104004\n",
      "Epoch 6, Batch 10, Loss: 0.8438087105751038\n",
      "Epoch 6, Batch 20, Loss: 0.8685853481292725\n",
      "Epoch 6, Batch 30, Loss: 0.8401892185211182\n",
      "Epoch 6, Batch 40, Loss: 0.916816771030426\n",
      "Epoch 6, Batch 50, Loss: 0.8816941976547241\n",
      "Epoch 6, Batch 60, Loss: 0.7854130864143372\n",
      "Epoch 6, Batch 70, Loss: 0.7485816478729248\n",
      "Epoch 6, Batch 80, Loss: 0.7524706125259399\n",
      "Epoch 6, Batch 90, Loss: 0.8294899463653564\n",
      "Epoch 6, Batch 100, Loss: 0.8958281874656677\n",
      "Epoch 7, Batch 0, Loss: 0.876957356929779\n",
      "Epoch 7, Batch 10, Loss: 0.9059204459190369\n",
      "Epoch 7, Batch 20, Loss: 0.9744479060173035\n",
      "Epoch 7, Batch 30, Loss: 0.7800818085670471\n",
      "Epoch 7, Batch 40, Loss: 0.6764713525772095\n",
      "Epoch 7, Batch 50, Loss: 0.7934860587120056\n",
      "Epoch 7, Batch 60, Loss: 0.8944861888885498\n",
      "Epoch 7, Batch 70, Loss: 0.8295714259147644\n",
      "Epoch 7, Batch 80, Loss: 0.7808866500854492\n",
      "Epoch 7, Batch 90, Loss: 0.8191177845001221\n",
      "Epoch 7, Batch 100, Loss: 0.793675422668457\n",
      "Epoch 8, Batch 0, Loss: 0.8698748350143433\n",
      "Epoch 8, Batch 10, Loss: 0.8706360459327698\n",
      "Epoch 8, Batch 20, Loss: 0.9024118781089783\n",
      "Epoch 8, Batch 30, Loss: 0.8362287878990173\n",
      "Epoch 8, Batch 40, Loss: 0.8573178052902222\n",
      "Epoch 8, Batch 50, Loss: 0.8480747938156128\n",
      "Epoch 8, Batch 60, Loss: 0.7004089951515198\n",
      "Epoch 8, Batch 70, Loss: 0.7170460224151611\n",
      "Epoch 8, Batch 80, Loss: 0.9168809652328491\n",
      "Epoch 8, Batch 90, Loss: 0.9619169235229492\n",
      "Epoch 8, Batch 100, Loss: 0.7711816430091858\n",
      "Epoch 9, Batch 0, Loss: 0.8238973617553711\n",
      "Epoch 9, Batch 10, Loss: 0.8185713291168213\n",
      "Epoch 9, Batch 20, Loss: 0.9150182604789734\n",
      "Epoch 9, Batch 30, Loss: 0.8922180533409119\n",
      "Epoch 9, Batch 40, Loss: 0.7863760590553284\n",
      "Epoch 9, Batch 50, Loss: 0.8778566718101501\n",
      "Epoch 9, Batch 60, Loss: 0.8172871470451355\n",
      "Epoch 9, Batch 70, Loss: 0.8292039632797241\n",
      "Epoch 9, Batch 80, Loss: 1.026354432106018\n",
      "Epoch 9, Batch 90, Loss: 0.8317133784294128\n",
      "Epoch 9, Batch 100, Loss: 0.8545306324958801\n",
      "Epoch 10, Batch 0, Loss: 0.9609540700912476\n",
      "Epoch 10, Batch 10, Loss: 0.9616514444351196\n",
      "Epoch 10, Batch 20, Loss: 0.9267719984054565\n",
      "Epoch 10, Batch 30, Loss: 0.8524590134620667\n",
      "Epoch 10, Batch 40, Loss: 0.7557793855667114\n",
      "Epoch 10, Batch 50, Loss: 0.8601613640785217\n",
      "Epoch 10, Batch 60, Loss: 0.9170625805854797\n",
      "Epoch 10, Batch 70, Loss: 0.7634076476097107\n",
      "Epoch 10, Batch 80, Loss: 0.7840607166290283\n",
      "Epoch 10, Batch 90, Loss: 0.9250555038452148\n",
      "Epoch 10, Batch 100, Loss: 0.8948232531547546\n",
      "Epoch 11, Batch 0, Loss: 0.8223332166671753\n",
      "Epoch 11, Batch 10, Loss: 0.8470206260681152\n",
      "Epoch 11, Batch 20, Loss: 0.8210946321487427\n",
      "Epoch 11, Batch 30, Loss: 0.8672782182693481\n",
      "Epoch 11, Batch 40, Loss: 0.8442087769508362\n",
      "Epoch 11, Batch 50, Loss: 0.9475669264793396\n",
      "Epoch 11, Batch 60, Loss: 0.8247321248054504\n",
      "Epoch 11, Batch 70, Loss: 0.8614850640296936\n",
      "Epoch 11, Batch 80, Loss: 0.9426226019859314\n",
      "Epoch 11, Batch 90, Loss: 0.8271633982658386\n",
      "Epoch 11, Batch 100, Loss: 0.8140774965286255\n",
      "Epoch 12, Batch 0, Loss: 0.8113353252410889\n",
      "Epoch 12, Batch 10, Loss: 0.8058108687400818\n",
      "Epoch 12, Batch 20, Loss: 0.8821454644203186\n",
      "Epoch 12, Batch 30, Loss: 0.71233731508255\n",
      "Epoch 12, Batch 40, Loss: 0.8564975261688232\n",
      "Epoch 12, Batch 50, Loss: 0.8921933770179749\n",
      "Epoch 12, Batch 60, Loss: 0.872539222240448\n",
      "Epoch 12, Batch 70, Loss: 0.776278555393219\n",
      "Epoch 12, Batch 80, Loss: 0.785851776599884\n",
      "Epoch 12, Batch 90, Loss: 0.7287889719009399\n",
      "Epoch 12, Batch 100, Loss: 0.8632135391235352\n",
      "Epoch 13, Batch 0, Loss: 0.9254936575889587\n",
      "Epoch 13, Batch 10, Loss: 0.9148719906806946\n",
      "Epoch 13, Batch 20, Loss: 0.9014440774917603\n",
      "Epoch 13, Batch 30, Loss: 0.6728473901748657\n",
      "Epoch 13, Batch 40, Loss: 0.8516987562179565\n",
      "Epoch 13, Batch 50, Loss: 0.7829264998435974\n",
      "Epoch 13, Batch 60, Loss: 0.8315720558166504\n",
      "Epoch 13, Batch 70, Loss: 0.791649341583252\n",
      "Epoch 13, Batch 80, Loss: 0.676291286945343\n",
      "Epoch 13, Batch 90, Loss: 0.8056747317314148\n",
      "Epoch 13, Batch 100, Loss: 0.8963277339935303\n",
      "Epoch 14, Batch 0, Loss: 0.8647472262382507\n",
      "Epoch 14, Batch 10, Loss: 0.7622793316841125\n",
      "Epoch 14, Batch 20, Loss: 0.7384682893753052\n",
      "Epoch 14, Batch 30, Loss: 0.8605532646179199\n",
      "Epoch 14, Batch 40, Loss: 0.7312135696411133\n",
      "Epoch 14, Batch 50, Loss: 0.7953829765319824\n",
      "Epoch 14, Batch 60, Loss: 0.8745152950286865\n",
      "Epoch 14, Batch 70, Loss: 0.8870813846588135\n",
      "Epoch 14, Batch 80, Loss: 0.8534780144691467\n",
      "Epoch 14, Batch 90, Loss: 0.7612752318382263\n",
      "Epoch 14, Batch 100, Loss: 0.7237136960029602\n",
      "Epoch 15, Batch 0, Loss: 0.7997224926948547\n",
      "Epoch 15, Batch 10, Loss: 0.8218573927879333\n",
      "Epoch 15, Batch 20, Loss: 0.9802611470222473\n",
      "Epoch 15, Batch 30, Loss: 0.6878598928451538\n",
      "Epoch 15, Batch 40, Loss: 0.8004807233810425\n",
      "Epoch 15, Batch 50, Loss: 0.7849143743515015\n",
      "Epoch 15, Batch 60, Loss: 0.803576648235321\n",
      "Epoch 15, Batch 70, Loss: 0.8517487049102783\n",
      "Epoch 15, Batch 80, Loss: 0.7549999952316284\n",
      "Epoch 15, Batch 90, Loss: 0.7931568026542664\n",
      "Epoch 15, Batch 100, Loss: 0.8773308396339417\n",
      "Epoch 16, Batch 0, Loss: 0.7341030240058899\n",
      "Epoch 16, Batch 10, Loss: 0.815786600112915\n",
      "Epoch 16, Batch 20, Loss: 0.9206234812736511\n",
      "Epoch 16, Batch 30, Loss: 0.9053475856781006\n",
      "Epoch 16, Batch 40, Loss: 0.82242751121521\n",
      "Epoch 16, Batch 50, Loss: 0.9762928485870361\n",
      "Epoch 16, Batch 60, Loss: 0.8742110133171082\n",
      "Epoch 16, Batch 70, Loss: 0.7454181909561157\n",
      "Epoch 16, Batch 80, Loss: 0.9839164018630981\n",
      "Epoch 16, Batch 90, Loss: 0.7198664546012878\n",
      "Epoch 16, Batch 100, Loss: 0.8075984716415405\n",
      "Epoch 17, Batch 0, Loss: 0.7331963777542114\n",
      "Epoch 17, Batch 10, Loss: 0.7440720796585083\n",
      "Epoch 17, Batch 20, Loss: 0.8744232654571533\n",
      "Epoch 17, Batch 30, Loss: 0.6942886114120483\n",
      "Epoch 17, Batch 40, Loss: 0.8976874351501465\n",
      "Epoch 17, Batch 50, Loss: 0.8783143758773804\n",
      "Epoch 17, Batch 60, Loss: 0.9352317452430725\n",
      "Epoch 17, Batch 70, Loss: 0.7830122709274292\n",
      "Epoch 17, Batch 80, Loss: 0.8615337014198303\n",
      "Epoch 17, Batch 90, Loss: 0.7464373707771301\n",
      "Epoch 17, Batch 100, Loss: 0.8183082342147827\n",
      "Epoch 18, Batch 0, Loss: 0.8459250926971436\n",
      "Epoch 18, Batch 10, Loss: 0.8480296730995178\n",
      "Epoch 18, Batch 20, Loss: 0.7762724757194519\n",
      "Epoch 18, Batch 30, Loss: 0.7982537150382996\n",
      "Epoch 18, Batch 40, Loss: 0.8236622214317322\n",
      "Epoch 18, Batch 50, Loss: 0.7799233794212341\n",
      "Epoch 18, Batch 60, Loss: 0.9556311964988708\n",
      "Epoch 18, Batch 70, Loss: 0.7926448583602905\n",
      "Epoch 18, Batch 80, Loss: 0.7773163318634033\n",
      "Epoch 18, Batch 90, Loss: 0.9008419513702393\n",
      "Epoch 18, Batch 100, Loss: 0.9249904155731201\n",
      "Epoch 19, Batch 0, Loss: 0.7035537958145142\n",
      "Epoch 19, Batch 10, Loss: 0.8895308971405029\n",
      "Epoch 19, Batch 20, Loss: 0.9206416606903076\n",
      "Epoch 19, Batch 30, Loss: 0.8197086453437805\n",
      "Epoch 19, Batch 40, Loss: 0.7259523272514343\n",
      "Epoch 19, Batch 50, Loss: 0.8332502245903015\n",
      "Epoch 19, Batch 60, Loss: 0.7481467127799988\n",
      "Epoch 19, Batch 70, Loss: 0.7738136053085327\n",
      "Epoch 19, Batch 80, Loss: 0.9615625143051147\n",
      "Epoch 19, Batch 90, Loss: 0.9383571743965149\n",
      "Epoch 19, Batch 100, Loss: 0.6983959674835205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_BiLSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True)\n",
       "    (fc1): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_BiLSTM().to()\n",
    "\n",
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 1.011897; Test RMSE 830.823616\n",
      "\n",
      "Train  MAE: 0.834884; Test  MAE 802.333726\n",
      "\n",
      "Train  R^2: -59532.806139; Test  R^2 -18519.540509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
