{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from data_preprocessing import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6618, 10, 8]) torch.Size([6618, 1]) torch.Size([490, 10, 8]) (490, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, y_scaler, train_loader, test_loader = get_data(\n",
    "            timestep = 10,\n",
    "            batch_size = 64,\n",
    "            y_name = 'Closing price',\n",
    "            train_size = 6627,\n",
    "            path = \"Data/SSE000001.csv\")\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(device)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    print (\"GPU device not found.\")\n",
    "\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer parameters from the table\n",
    "        self.conv1d = nn.Conv1d(in_channels= 8, out_channels= 32, kernel_size= 1, padding=\"same\")\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Pooling\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size= 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layer\n",
    "        x = self.conv1d(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64, lstm_dropout = 0.2, fc1_output_size = 16):\n",
    "        super(BILSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True,\n",
    "                             bidirectional = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size * 2, self.fc1_output_size)\n",
    "        self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers * 2, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        h_fc2 = self.fc2(h_fc1)\n",
    "        output = h_fc2[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 8, lstm_num_layers = 1, lstm_hidden_size = 64,  fc1_output_size = 16):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        # self.lstm_dropout = lstm_dropout\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self.input_size, \n",
    "                             hidden_size = self.lstm_hidden_size,\n",
    "                             num_layers = self.lstm_num_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p = self.lstm_dropout)\n",
    "        # self.fc1 = nn.Linear(self.lstm_hidden_size, self.fc1_output_size)\n",
    "        # self.fc2 = nn.Linear(self.fc1_output_size, 1)\n",
    "        self.fc1 = nn.Linear(self.lstm_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.lstm_num_layers, x.size(0), self.lstm_hidden_size).to(device)\n",
    "\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        h_lstm, _ = self.lstm(x, (h0, c0))\n",
    "        # h_dropout = self.dropout(h_lstm)\n",
    "\n",
    "        # h_fc1 = self.fc1(h_lstm)\n",
    "        # h_fc1 = F.relu(h_fc1)\n",
    "\n",
    "        # h_fc2 = self.fc2(h_fc1)\n",
    "        # output = h_fc2[:, -1, :]\n",
    "\n",
    "        h_fc1 = self.fc1(h_lstm)\n",
    "        output = h_fc1[:, -1, :]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.lstm = LSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x) #[64, 32, 10]\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1) #[64, 10, 32]\n",
    "        x = self.lstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_LSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.8278360962867737\n",
      "Epoch 0, Batch 10, Loss: 0.7521199584007263\n",
      "Epoch 0, Batch 20, Loss: 0.8677293658256531\n",
      "Epoch 0, Batch 30, Loss: 0.8542541265487671\n",
      "Epoch 0, Batch 40, Loss: 0.8626081347465515\n",
      "Epoch 0, Batch 50, Loss: 0.8006448149681091\n",
      "Epoch 0, Batch 60, Loss: 0.7381938099861145\n",
      "Epoch 0, Batch 70, Loss: 0.6592618823051453\n",
      "Epoch 0, Batch 80, Loss: 0.6576324701309204\n",
      "Epoch 0, Batch 90, Loss: 0.7188338041305542\n",
      "Epoch 0, Batch 100, Loss: 0.7444726228713989\n",
      "Epoch 1, Batch 0, Loss: 0.6784178614616394\n",
      "Epoch 1, Batch 10, Loss: 0.5507567524909973\n",
      "Epoch 1, Batch 20, Loss: 0.48482745885849\n",
      "Epoch 1, Batch 30, Loss: 0.3281605541706085\n",
      "Epoch 1, Batch 40, Loss: 0.3156566619873047\n",
      "Epoch 1, Batch 50, Loss: 0.2060951590538025\n",
      "Epoch 1, Batch 60, Loss: 0.172569140791893\n",
      "Epoch 1, Batch 70, Loss: 0.13567519187927246\n",
      "Epoch 1, Batch 80, Loss: 0.13006511330604553\n",
      "Epoch 1, Batch 90, Loss: 0.12423089891672134\n",
      "Epoch 1, Batch 100, Loss: 0.11439726501703262\n",
      "Epoch 2, Batch 0, Loss: 0.14987245202064514\n",
      "Epoch 2, Batch 10, Loss: 0.13569727540016174\n",
      "Epoch 2, Batch 20, Loss: 0.10323677211999893\n",
      "Epoch 2, Batch 30, Loss: 0.10557287931442261\n",
      "Epoch 2, Batch 40, Loss: 0.10819407552480698\n",
      "Epoch 2, Batch 50, Loss: 0.08923318982124329\n",
      "Epoch 2, Batch 60, Loss: 0.1206265315413475\n",
      "Epoch 2, Batch 70, Loss: 0.09624399989843369\n",
      "Epoch 2, Batch 80, Loss: 0.07959055155515671\n",
      "Epoch 2, Batch 90, Loss: 0.06395889818668365\n",
      "Epoch 2, Batch 100, Loss: 0.05886582285165787\n",
      "Epoch 3, Batch 0, Loss: 0.07813447713851929\n",
      "Epoch 3, Batch 10, Loss: 0.044519949704408646\n",
      "Epoch 3, Batch 20, Loss: 0.05337243899703026\n",
      "Epoch 3, Batch 30, Loss: 0.11074364930391312\n",
      "Epoch 3, Batch 40, Loss: 0.0918750986456871\n",
      "Epoch 3, Batch 50, Loss: 0.07332276552915573\n",
      "Epoch 3, Batch 60, Loss: 0.09141412377357483\n",
      "Epoch 3, Batch 70, Loss: 0.06347279995679855\n",
      "Epoch 3, Batch 80, Loss: 0.07506982982158661\n",
      "Epoch 3, Batch 90, Loss: 0.0535113550722599\n",
      "Epoch 3, Batch 100, Loss: 0.09811294078826904\n",
      "Epoch 4, Batch 0, Loss: 0.039951831102371216\n",
      "Epoch 4, Batch 10, Loss: 0.049329787492752075\n",
      "Epoch 4, Batch 20, Loss: 0.03849981352686882\n",
      "Epoch 4, Batch 30, Loss: 0.08948074281215668\n",
      "Epoch 4, Batch 40, Loss: 0.07763232290744781\n",
      "Epoch 4, Batch 50, Loss: 0.07900619506835938\n",
      "Epoch 4, Batch 60, Loss: 0.0571020171046257\n",
      "Epoch 4, Batch 70, Loss: 0.043400298804044724\n",
      "Epoch 4, Batch 80, Loss: 0.036863651126623154\n",
      "Epoch 4, Batch 90, Loss: 0.07922932505607605\n",
      "Epoch 4, Batch 100, Loss: 0.033536508679389954\n",
      "Epoch 5, Batch 0, Loss: 0.043786339461803436\n",
      "Epoch 5, Batch 10, Loss: 0.0620272159576416\n",
      "Epoch 5, Batch 20, Loss: 0.025141969323158264\n",
      "Epoch 5, Batch 30, Loss: 0.05450132116675377\n",
      "Epoch 5, Batch 40, Loss: 0.03908904269337654\n",
      "Epoch 5, Batch 50, Loss: 0.05921205133199692\n",
      "Epoch 5, Batch 60, Loss: 0.03259379789233208\n",
      "Epoch 5, Batch 70, Loss: 0.02945876121520996\n",
      "Epoch 5, Batch 80, Loss: 0.05753455311059952\n",
      "Epoch 5, Batch 90, Loss: 0.08766620606184006\n",
      "Epoch 5, Batch 100, Loss: 0.04565136134624481\n",
      "Epoch 6, Batch 0, Loss: 0.04734533280134201\n",
      "Epoch 6, Batch 10, Loss: 0.04048239439725876\n",
      "Epoch 6, Batch 20, Loss: 0.03622758761048317\n",
      "Epoch 6, Batch 30, Loss: 0.06984762847423553\n",
      "Epoch 6, Batch 40, Loss: 0.056560155004262924\n",
      "Epoch 6, Batch 50, Loss: 0.04893072322010994\n",
      "Epoch 6, Batch 60, Loss: 0.05236108973622322\n",
      "Epoch 6, Batch 70, Loss: 0.027574731037020683\n",
      "Epoch 6, Batch 80, Loss: 0.05944312736392021\n",
      "Epoch 6, Batch 90, Loss: 0.03186303749680519\n",
      "Epoch 6, Batch 100, Loss: 0.042801905423402786\n",
      "Epoch 7, Batch 0, Loss: 0.03603549674153328\n",
      "Epoch 7, Batch 10, Loss: 0.043573129922151566\n",
      "Epoch 7, Batch 20, Loss: 0.030964266508817673\n",
      "Epoch 7, Batch 30, Loss: 0.02709008939564228\n",
      "Epoch 7, Batch 40, Loss: 0.03063766472041607\n",
      "Epoch 7, Batch 50, Loss: 0.06316517293453217\n",
      "Epoch 7, Batch 60, Loss: 0.03872670233249664\n",
      "Epoch 7, Batch 70, Loss: 0.026943497359752655\n",
      "Epoch 7, Batch 80, Loss: 0.04004959017038345\n",
      "Epoch 7, Batch 90, Loss: 0.03878238424658775\n",
      "Epoch 7, Batch 100, Loss: 0.037129368633031845\n",
      "Epoch 8, Batch 0, Loss: 0.04904337227344513\n",
      "Epoch 8, Batch 10, Loss: 0.04760411009192467\n",
      "Epoch 8, Batch 20, Loss: 0.040328726172447205\n",
      "Epoch 8, Batch 30, Loss: 0.040543753653764725\n",
      "Epoch 8, Batch 40, Loss: 0.0214109905064106\n",
      "Epoch 8, Batch 50, Loss: 0.02629121206700802\n",
      "Epoch 8, Batch 60, Loss: 0.03358619287610054\n",
      "Epoch 8, Batch 70, Loss: 0.023764921352267265\n",
      "Epoch 8, Batch 80, Loss: 0.046361953020095825\n",
      "Epoch 8, Batch 90, Loss: 0.04838740453124046\n",
      "Epoch 8, Batch 100, Loss: 0.021496722474694252\n",
      "Epoch 9, Batch 0, Loss: 0.041011955589056015\n",
      "Epoch 9, Batch 10, Loss: 0.04253564774990082\n",
      "Epoch 9, Batch 20, Loss: 0.03194054216146469\n",
      "Epoch 9, Batch 30, Loss: 0.022428663447499275\n",
      "Epoch 9, Batch 40, Loss: 0.02948978915810585\n",
      "Epoch 9, Batch 50, Loss: 0.026580601930618286\n",
      "Epoch 9, Batch 60, Loss: 0.026417510583996773\n",
      "Epoch 9, Batch 70, Loss: 0.024828171357512474\n",
      "Epoch 9, Batch 80, Loss: 0.05151292309165001\n",
      "Epoch 9, Batch 90, Loss: 0.03157314658164978\n",
      "Epoch 9, Batch 100, Loss: 0.036424387246370316\n",
      "Epoch 10, Batch 0, Loss: 0.031212013214826584\n",
      "Epoch 10, Batch 10, Loss: 0.03910203278064728\n",
      "Epoch 10, Batch 20, Loss: 0.026011742651462555\n",
      "Epoch 10, Batch 30, Loss: 0.029981080442667007\n",
      "Epoch 10, Batch 40, Loss: 0.021357540041208267\n",
      "Epoch 10, Batch 50, Loss: 0.043009720742702484\n",
      "Epoch 10, Batch 60, Loss: 0.02654360607266426\n",
      "Epoch 10, Batch 70, Loss: 0.028946807608008385\n",
      "Epoch 10, Batch 80, Loss: 0.038917120546102524\n",
      "Epoch 10, Batch 90, Loss: 0.04121030494570732\n",
      "Epoch 10, Batch 100, Loss: 0.03456908091902733\n",
      "Epoch 11, Batch 0, Loss: 0.03226510435342789\n",
      "Epoch 11, Batch 10, Loss: 0.029485130682587624\n",
      "Epoch 11, Batch 20, Loss: 0.025442536920309067\n",
      "Epoch 11, Batch 30, Loss: 0.02898361347615719\n",
      "Epoch 11, Batch 40, Loss: 0.030652370303869247\n",
      "Epoch 11, Batch 50, Loss: 0.027451135218143463\n",
      "Epoch 11, Batch 60, Loss: 0.028616728261113167\n",
      "Epoch 11, Batch 70, Loss: 0.03699420392513275\n",
      "Epoch 11, Batch 80, Loss: 0.02163088694214821\n",
      "Epoch 11, Batch 90, Loss: 0.03128088265657425\n",
      "Epoch 11, Batch 100, Loss: 0.024132363498210907\n",
      "Epoch 12, Batch 0, Loss: 0.021081145852804184\n",
      "Epoch 12, Batch 10, Loss: 0.032585568726062775\n",
      "Epoch 12, Batch 20, Loss: 0.03946562111377716\n",
      "Epoch 12, Batch 30, Loss: 0.030625808984041214\n",
      "Epoch 12, Batch 40, Loss: 0.04140971601009369\n",
      "Epoch 12, Batch 50, Loss: 0.0282540675252676\n",
      "Epoch 12, Batch 60, Loss: 0.03561611473560333\n",
      "Epoch 12, Batch 70, Loss: 0.027447804808616638\n",
      "Epoch 12, Batch 80, Loss: 0.022727828472852707\n",
      "Epoch 12, Batch 90, Loss: 0.03027360886335373\n",
      "Epoch 12, Batch 100, Loss: 0.03969376161694527\n",
      "Epoch 13, Batch 0, Loss: 0.024116061627864838\n",
      "Epoch 13, Batch 10, Loss: 0.03690216317772865\n",
      "Epoch 13, Batch 20, Loss: 0.019906185567378998\n",
      "Epoch 13, Batch 30, Loss: 0.029880305752158165\n",
      "Epoch 13, Batch 40, Loss: 0.02266150340437889\n",
      "Epoch 13, Batch 50, Loss: 0.02856714278459549\n",
      "Epoch 13, Batch 60, Loss: 0.025840410962700844\n",
      "Epoch 13, Batch 70, Loss: 0.04885714501142502\n",
      "Epoch 13, Batch 80, Loss: 0.024271953850984573\n",
      "Epoch 13, Batch 90, Loss: 0.02721020020544529\n",
      "Epoch 13, Batch 100, Loss: 0.02986142784357071\n",
      "Epoch 14, Batch 0, Loss: 0.027705412358045578\n",
      "Epoch 14, Batch 10, Loss: 0.03951369225978851\n",
      "Epoch 14, Batch 20, Loss: 0.03183652088046074\n",
      "Epoch 14, Batch 30, Loss: 0.027345143258571625\n",
      "Epoch 14, Batch 40, Loss: 0.019288398325443268\n",
      "Epoch 14, Batch 50, Loss: 0.025479787960648537\n",
      "Epoch 14, Batch 60, Loss: 0.030611539259552956\n",
      "Epoch 14, Batch 70, Loss: 0.0289269108325243\n",
      "Epoch 14, Batch 80, Loss: 0.030564548447728157\n",
      "Epoch 14, Batch 90, Loss: 0.022524559870362282\n",
      "Epoch 14, Batch 100, Loss: 0.033526819199323654\n",
      "Epoch 15, Batch 0, Loss: 0.03474188223481178\n",
      "Epoch 15, Batch 10, Loss: 0.02609126642346382\n",
      "Epoch 15, Batch 20, Loss: 0.0435890331864357\n",
      "Epoch 15, Batch 30, Loss: 0.030280040577054024\n",
      "Epoch 15, Batch 40, Loss: 0.030703922733664513\n",
      "Epoch 15, Batch 50, Loss: 0.020304428413510323\n",
      "Epoch 15, Batch 60, Loss: 0.03710990399122238\n",
      "Epoch 15, Batch 70, Loss: 0.01775488071143627\n",
      "Epoch 15, Batch 80, Loss: 0.033489156514406204\n",
      "Epoch 15, Batch 90, Loss: 0.026136502623558044\n",
      "Epoch 15, Batch 100, Loss: 0.03681502863764763\n",
      "Epoch 16, Batch 0, Loss: 0.03147241100668907\n",
      "Epoch 16, Batch 10, Loss: 0.026244252920150757\n",
      "Epoch 16, Batch 20, Loss: 0.026010261848568916\n",
      "Epoch 16, Batch 30, Loss: 0.02913188561797142\n",
      "Epoch 16, Batch 40, Loss: 0.030578799545764923\n",
      "Epoch 16, Batch 50, Loss: 0.02350454591214657\n",
      "Epoch 16, Batch 60, Loss: 0.030966274440288544\n",
      "Epoch 16, Batch 70, Loss: 0.027319248765707016\n",
      "Epoch 16, Batch 80, Loss: 0.022031201049685478\n",
      "Epoch 16, Batch 90, Loss: 0.03395090252161026\n",
      "Epoch 16, Batch 100, Loss: 0.02496209926903248\n",
      "Epoch 17, Batch 0, Loss: 0.028111383318901062\n",
      "Epoch 17, Batch 10, Loss: 0.036107998341321945\n",
      "Epoch 17, Batch 20, Loss: 0.031063862144947052\n",
      "Epoch 17, Batch 30, Loss: 0.025363106280565262\n",
      "Epoch 17, Batch 40, Loss: 0.030017627403140068\n",
      "Epoch 17, Batch 50, Loss: 0.030893485993146896\n",
      "Epoch 17, Batch 60, Loss: 0.03813062608242035\n",
      "Epoch 17, Batch 70, Loss: 0.022023160010576248\n",
      "Epoch 17, Batch 80, Loss: 0.02978646010160446\n",
      "Epoch 17, Batch 90, Loss: 0.023123566061258316\n",
      "Epoch 17, Batch 100, Loss: 0.029545722529292107\n",
      "Epoch 18, Batch 0, Loss: 0.04525785520672798\n",
      "Epoch 18, Batch 10, Loss: 0.03443588688969612\n",
      "Epoch 18, Batch 20, Loss: 0.03167439252138138\n",
      "Epoch 18, Batch 30, Loss: 0.025670139119029045\n",
      "Epoch 18, Batch 40, Loss: 0.02111596241593361\n",
      "Epoch 18, Batch 50, Loss: 0.03663210570812225\n",
      "Epoch 18, Batch 60, Loss: 0.022287502884864807\n",
      "Epoch 18, Batch 70, Loss: 0.021563095971941948\n",
      "Epoch 18, Batch 80, Loss: 0.01853756792843342\n",
      "Epoch 18, Batch 90, Loss: 0.03442095220088959\n",
      "Epoch 18, Batch 100, Loss: 0.022060880437493324\n",
      "Epoch 19, Batch 0, Loss: 0.02766292542219162\n",
      "Epoch 19, Batch 10, Loss: 0.02144772559404373\n",
      "Epoch 19, Batch 20, Loss: 0.038033463060855865\n",
      "Epoch 19, Batch 30, Loss: 0.02698555961251259\n",
      "Epoch 19, Batch 40, Loss: 0.03177335113286972\n",
      "Epoch 19, Batch 50, Loss: 0.032277800142765045\n",
      "Epoch 19, Batch 60, Loss: 0.024729955941438675\n",
      "Epoch 19, Batch 70, Loss: 0.0215799268335104\n",
      "Epoch 19, Batch 80, Loss: 0.032257918268442154\n",
      "Epoch 19, Batch 90, Loss: 0.030325721949338913\n",
      "Epoch 19, Batch 100, Loss: 0.027854938060045242\n",
      "Epoch 20, Batch 0, Loss: 0.03190402314066887\n",
      "Epoch 20, Batch 10, Loss: 0.02054613083600998\n",
      "Epoch 20, Batch 20, Loss: 0.02827116847038269\n",
      "Epoch 20, Batch 30, Loss: 0.024598967283964157\n",
      "Epoch 20, Batch 40, Loss: 0.02282649651169777\n",
      "Epoch 20, Batch 50, Loss: 0.022051488980650902\n",
      "Epoch 20, Batch 60, Loss: 0.038029707968235016\n",
      "Epoch 20, Batch 70, Loss: 0.03087110072374344\n",
      "Epoch 20, Batch 80, Loss: 0.025805242359638214\n",
      "Epoch 20, Batch 90, Loss: 0.022351685911417007\n",
      "Epoch 20, Batch 100, Loss: 0.019761916249990463\n",
      "Epoch 21, Batch 0, Loss: 0.028418369591236115\n",
      "Epoch 21, Batch 10, Loss: 0.0314040407538414\n",
      "Epoch 21, Batch 20, Loss: 0.030310964211821556\n",
      "Epoch 21, Batch 30, Loss: 0.023741312325000763\n",
      "Epoch 21, Batch 40, Loss: 0.03437086567282677\n",
      "Epoch 21, Batch 50, Loss: 0.01991744339466095\n",
      "Epoch 21, Batch 60, Loss: 0.030869975686073303\n",
      "Epoch 21, Batch 70, Loss: 0.023442449048161507\n",
      "Epoch 21, Batch 80, Loss: 0.026683896780014038\n",
      "Epoch 21, Batch 90, Loss: 0.032202672213315964\n",
      "Epoch 21, Batch 100, Loss: 0.01986199989914894\n",
      "Epoch 22, Batch 0, Loss: 0.022165600210428238\n",
      "Epoch 22, Batch 10, Loss: 0.03256382793188095\n",
      "Epoch 22, Batch 20, Loss: 0.02684040740132332\n",
      "Epoch 22, Batch 30, Loss: 0.02795187570154667\n",
      "Epoch 22, Batch 40, Loss: 0.030702117830514908\n",
      "Epoch 22, Batch 50, Loss: 0.028474383056163788\n",
      "Epoch 22, Batch 60, Loss: 0.015134467743337154\n",
      "Epoch 22, Batch 70, Loss: 0.023690540343523026\n",
      "Epoch 22, Batch 80, Loss: 0.03305868059396744\n",
      "Epoch 22, Batch 90, Loss: 0.029780376702547073\n",
      "Epoch 22, Batch 100, Loss: 0.030940968543291092\n",
      "Epoch 23, Batch 0, Loss: 0.034744732081890106\n",
      "Epoch 23, Batch 10, Loss: 0.02155173011124134\n",
      "Epoch 23, Batch 20, Loss: 0.02458701655268669\n",
      "Epoch 23, Batch 30, Loss: 0.026499515399336815\n",
      "Epoch 23, Batch 40, Loss: 0.02194724790751934\n",
      "Epoch 23, Batch 50, Loss: 0.01959363929927349\n",
      "Epoch 23, Batch 60, Loss: 0.02897847816348076\n",
      "Epoch 23, Batch 70, Loss: 0.02015216089785099\n",
      "Epoch 23, Batch 80, Loss: 0.022027140483260155\n",
      "Epoch 23, Batch 90, Loss: 0.028677472844719887\n",
      "Epoch 23, Batch 100, Loss: 0.03231162205338478\n",
      "Epoch 24, Batch 0, Loss: 0.024955730885267258\n",
      "Epoch 24, Batch 10, Loss: 0.02576235681772232\n",
      "Epoch 24, Batch 20, Loss: 0.01987050473690033\n",
      "Epoch 24, Batch 30, Loss: 0.021031687036156654\n",
      "Epoch 24, Batch 40, Loss: 0.021846119314432144\n",
      "Epoch 24, Batch 50, Loss: 0.025135710835456848\n",
      "Epoch 24, Batch 60, Loss: 0.0182950496673584\n",
      "Epoch 24, Batch 70, Loss: 0.025562327355146408\n",
      "Epoch 24, Batch 80, Loss: 0.02700158581137657\n",
      "Epoch 24, Batch 90, Loss: 0.028592867776751518\n",
      "Epoch 24, Batch 100, Loss: 0.028854668140411377\n",
      "Epoch 25, Batch 0, Loss: 0.027747653424739838\n",
      "Epoch 25, Batch 10, Loss: 0.02386656031012535\n",
      "Epoch 25, Batch 20, Loss: 0.024890106171369553\n",
      "Epoch 25, Batch 30, Loss: 0.023351166397333145\n",
      "Epoch 25, Batch 40, Loss: 0.023569071665406227\n",
      "Epoch 25, Batch 50, Loss: 0.024090545251965523\n",
      "Epoch 25, Batch 60, Loss: 0.021524876356124878\n",
      "Epoch 25, Batch 70, Loss: 0.02730490267276764\n",
      "Epoch 25, Batch 80, Loss: 0.025333629921078682\n",
      "Epoch 25, Batch 90, Loss: 0.024399492889642715\n",
      "Epoch 25, Batch 100, Loss: 0.024242892861366272\n",
      "Epoch 26, Batch 0, Loss: 0.030321117490530014\n",
      "Epoch 26, Batch 10, Loss: 0.019592976197600365\n",
      "Epoch 26, Batch 20, Loss: 0.026096269488334656\n",
      "Epoch 26, Batch 30, Loss: 0.024371838197112083\n",
      "Epoch 26, Batch 40, Loss: 0.02138148806989193\n",
      "Epoch 26, Batch 50, Loss: 0.027549749240279198\n",
      "Epoch 26, Batch 60, Loss: 0.025862978771328926\n",
      "Epoch 26, Batch 70, Loss: 0.02785438485443592\n",
      "Epoch 26, Batch 80, Loss: 0.020193632692098618\n",
      "Epoch 26, Batch 90, Loss: 0.028421130031347275\n",
      "Epoch 26, Batch 100, Loss: 0.035430729389190674\n",
      "Epoch 27, Batch 0, Loss: 0.02265927754342556\n",
      "Epoch 27, Batch 10, Loss: 0.028578756377100945\n",
      "Epoch 27, Batch 20, Loss: 0.031215958297252655\n",
      "Epoch 27, Batch 30, Loss: 0.027370722964406013\n",
      "Epoch 27, Batch 40, Loss: 0.016877975314855576\n",
      "Epoch 27, Batch 50, Loss: 0.03307376429438591\n",
      "Epoch 27, Batch 60, Loss: 0.024339715018868446\n",
      "Epoch 27, Batch 70, Loss: 0.025825556367635727\n",
      "Epoch 27, Batch 80, Loss: 0.024479277431964874\n",
      "Epoch 27, Batch 90, Loss: 0.02932138741016388\n",
      "Epoch 27, Batch 100, Loss: 0.024306565523147583\n",
      "Epoch 28, Batch 0, Loss: 0.0189772117882967\n",
      "Epoch 28, Batch 10, Loss: 0.029727380722761154\n",
      "Epoch 28, Batch 20, Loss: 0.02233952470123768\n",
      "Epoch 28, Batch 30, Loss: 0.020960239693522453\n",
      "Epoch 28, Batch 40, Loss: 0.026700830087065697\n",
      "Epoch 28, Batch 50, Loss: 0.021389033645391464\n",
      "Epoch 28, Batch 60, Loss: 0.034024134278297424\n",
      "Epoch 28, Batch 70, Loss: 0.036785319447517395\n",
      "Epoch 28, Batch 80, Loss: 0.02307557314634323\n",
      "Epoch 28, Batch 90, Loss: 0.02298711985349655\n",
      "Epoch 28, Batch 100, Loss: 0.025556307286024094\n",
      "Epoch 29, Batch 0, Loss: 0.024686528369784355\n",
      "Epoch 29, Batch 10, Loss: 0.021319007501006126\n",
      "Epoch 29, Batch 20, Loss: 0.03511831909418106\n",
      "Epoch 29, Batch 30, Loss: 0.03544028103351593\n",
      "Epoch 29, Batch 40, Loss: 0.02228826843202114\n",
      "Epoch 29, Batch 50, Loss: 0.022952906787395477\n",
      "Epoch 29, Batch 60, Loss: 0.02120136097073555\n",
      "Epoch 29, Batch 70, Loss: 0.03288803622126579\n",
      "Epoch 29, Batch 80, Loss: 0.029849112033843994\n",
      "Epoch 29, Batch 90, Loss: 0.03088977560400963\n",
      "Epoch 29, Batch 100, Loss: 0.025446582585573196\n",
      "Epoch 30, Batch 0, Loss: 0.021114952862262726\n",
      "Epoch 30, Batch 10, Loss: 0.021670453250408173\n",
      "Epoch 30, Batch 20, Loss: 0.017174655571579933\n",
      "Epoch 30, Batch 30, Loss: 0.02549012005329132\n",
      "Epoch 30, Batch 40, Loss: 0.02530004270374775\n",
      "Epoch 30, Batch 50, Loss: 0.023424066603183746\n",
      "Epoch 30, Batch 60, Loss: 0.02520780637860298\n",
      "Epoch 30, Batch 70, Loss: 0.028007864952087402\n",
      "Epoch 30, Batch 80, Loss: 0.028873160481452942\n",
      "Epoch 30, Batch 90, Loss: 0.023273887112736702\n",
      "Epoch 30, Batch 100, Loss: 0.02975304052233696\n",
      "Epoch 31, Batch 0, Loss: 0.0240328311920166\n",
      "Epoch 31, Batch 10, Loss: 0.016697881743311882\n",
      "Epoch 31, Batch 20, Loss: 0.028147375211119652\n",
      "Epoch 31, Batch 30, Loss: 0.028498483821749687\n",
      "Epoch 31, Batch 40, Loss: 0.017972517758607864\n",
      "Epoch 31, Batch 50, Loss: 0.02553384192287922\n",
      "Epoch 31, Batch 60, Loss: 0.028998255729675293\n",
      "Epoch 31, Batch 70, Loss: 0.026092085987329483\n",
      "Epoch 31, Batch 80, Loss: 0.02709287963807583\n",
      "Epoch 31, Batch 90, Loss: 0.023095186799764633\n",
      "Epoch 31, Batch 100, Loss: 0.024015771225094795\n",
      "Epoch 32, Batch 0, Loss: 0.02080347388982773\n",
      "Epoch 32, Batch 10, Loss: 0.02498949132859707\n",
      "Epoch 32, Batch 20, Loss: 0.022539086639881134\n",
      "Epoch 32, Batch 30, Loss: 0.0319557748734951\n",
      "Epoch 32, Batch 40, Loss: 0.028484418988227844\n",
      "Epoch 32, Batch 50, Loss: 0.027807336300611496\n",
      "Epoch 32, Batch 60, Loss: 0.021536706015467644\n",
      "Epoch 32, Batch 70, Loss: 0.020206967368721962\n",
      "Epoch 32, Batch 80, Loss: 0.02613438293337822\n",
      "Epoch 32, Batch 90, Loss: 0.028527898713946342\n",
      "Epoch 32, Batch 100, Loss: 0.02237425372004509\n",
      "Epoch 33, Batch 0, Loss: 0.022392502054572105\n",
      "Epoch 33, Batch 10, Loss: 0.017936570569872856\n",
      "Epoch 33, Batch 20, Loss: 0.022122733294963837\n",
      "Epoch 33, Batch 30, Loss: 0.03034966066479683\n",
      "Epoch 33, Batch 40, Loss: 0.03744681179523468\n",
      "Epoch 33, Batch 50, Loss: 0.02326705865561962\n",
      "Epoch 33, Batch 60, Loss: 0.021683409810066223\n",
      "Epoch 33, Batch 70, Loss: 0.017843622714281082\n",
      "Epoch 33, Batch 80, Loss: 0.023140985518693924\n",
      "Epoch 33, Batch 90, Loss: 0.017858708277344704\n",
      "Epoch 33, Batch 100, Loss: 0.022068466991186142\n",
      "Epoch 34, Batch 0, Loss: 0.02835123799741268\n",
      "Epoch 34, Batch 10, Loss: 0.022702611982822418\n",
      "Epoch 34, Batch 20, Loss: 0.032055411487817764\n",
      "Epoch 34, Batch 30, Loss: 0.020147670060396194\n",
      "Epoch 34, Batch 40, Loss: 0.019051672890782356\n",
      "Epoch 34, Batch 50, Loss: 0.030073028057813644\n",
      "Epoch 34, Batch 60, Loss: 0.02137802727520466\n",
      "Epoch 34, Batch 70, Loss: 0.02729172632098198\n",
      "Epoch 34, Batch 80, Loss: 0.024643007665872574\n",
      "Epoch 34, Batch 90, Loss: 0.01974857784807682\n",
      "Epoch 34, Batch 100, Loss: 0.020331935957074165\n",
      "Epoch 35, Batch 0, Loss: 0.018206842243671417\n",
      "Epoch 35, Batch 10, Loss: 0.02832939475774765\n",
      "Epoch 35, Batch 20, Loss: 0.021765684708952904\n",
      "Epoch 35, Batch 30, Loss: 0.029793161898851395\n",
      "Epoch 35, Batch 40, Loss: 0.039228178560733795\n",
      "Epoch 35, Batch 50, Loss: 0.02631344646215439\n",
      "Epoch 35, Batch 60, Loss: 0.02305220067501068\n",
      "Epoch 35, Batch 70, Loss: 0.021029166877269745\n",
      "Epoch 35, Batch 80, Loss: 0.028518039733171463\n",
      "Epoch 35, Batch 90, Loss: 0.03337907791137695\n",
      "Epoch 35, Batch 100, Loss: 0.026867998763918877\n",
      "Epoch 36, Batch 0, Loss: 0.02554059959948063\n",
      "Epoch 36, Batch 10, Loss: 0.019926702603697777\n",
      "Epoch 36, Batch 20, Loss: 0.02771878056228161\n",
      "Epoch 36, Batch 30, Loss: 0.02610778622329235\n",
      "Epoch 36, Batch 40, Loss: 0.022375941276550293\n",
      "Epoch 36, Batch 50, Loss: 0.02098321169614792\n",
      "Epoch 36, Batch 60, Loss: 0.02230226621031761\n",
      "Epoch 36, Batch 70, Loss: 0.02164204977452755\n",
      "Epoch 36, Batch 80, Loss: 0.02269884943962097\n",
      "Epoch 36, Batch 90, Loss: 0.02174709178507328\n",
      "Epoch 36, Batch 100, Loss: 0.023142829537391663\n",
      "Epoch 37, Batch 0, Loss: 0.029929501935839653\n",
      "Epoch 37, Batch 10, Loss: 0.02319050207734108\n",
      "Epoch 37, Batch 20, Loss: 0.022500213235616684\n",
      "Epoch 37, Batch 30, Loss: 0.02037234976887703\n",
      "Epoch 37, Batch 40, Loss: 0.02560684271156788\n",
      "Epoch 37, Batch 50, Loss: 0.027103066444396973\n",
      "Epoch 37, Batch 60, Loss: 0.01985918916761875\n",
      "Epoch 37, Batch 70, Loss: 0.036848850548267365\n",
      "Epoch 37, Batch 80, Loss: 0.030974727123975754\n",
      "Epoch 37, Batch 90, Loss: 0.03190553933382034\n",
      "Epoch 37, Batch 100, Loss: 0.018536848947405815\n",
      "Epoch 38, Batch 0, Loss: 0.026085348799824715\n",
      "Epoch 38, Batch 10, Loss: 0.022646348923444748\n",
      "Epoch 38, Batch 20, Loss: 0.021415766328573227\n",
      "Epoch 38, Batch 30, Loss: 0.024544984102249146\n",
      "Epoch 38, Batch 40, Loss: 0.021178800612688065\n",
      "Epoch 38, Batch 50, Loss: 0.028861593455076218\n",
      "Epoch 38, Batch 60, Loss: 0.02130226418375969\n",
      "Epoch 38, Batch 70, Loss: 0.028920713812112808\n",
      "Epoch 38, Batch 80, Loss: 0.019519800320267677\n",
      "Epoch 38, Batch 90, Loss: 0.027445077896118164\n",
      "Epoch 38, Batch 100, Loss: 0.01904379017651081\n",
      "Epoch 39, Batch 0, Loss: 0.017408013343811035\n",
      "Epoch 39, Batch 10, Loss: 0.023740874603390694\n",
      "Epoch 39, Batch 20, Loss: 0.022493265569210052\n",
      "Epoch 39, Batch 30, Loss: 0.02857349067926407\n",
      "Epoch 39, Batch 40, Loss: 0.02680313214659691\n",
      "Epoch 39, Batch 50, Loss: 0.019191259518265724\n",
      "Epoch 39, Batch 60, Loss: 0.026117313653230667\n",
      "Epoch 39, Batch 70, Loss: 0.027915844693779945\n",
      "Epoch 39, Batch 80, Loss: 0.027005497366189957\n",
      "Epoch 39, Batch 90, Loss: 0.026576166972517967\n",
      "Epoch 39, Batch 100, Loss: 0.025920620188117027\n",
      "Epoch 40, Batch 0, Loss: 0.021897386759519577\n",
      "Epoch 40, Batch 10, Loss: 0.020406464114785194\n",
      "Epoch 40, Batch 20, Loss: 0.023873629048466682\n",
      "Epoch 40, Batch 30, Loss: 0.018275516107678413\n",
      "Epoch 40, Batch 40, Loss: 0.03185012564063072\n",
      "Epoch 40, Batch 50, Loss: 0.0254011582583189\n",
      "Epoch 40, Batch 60, Loss: 0.03185587748885155\n",
      "Epoch 40, Batch 70, Loss: 0.019765570759773254\n",
      "Epoch 40, Batch 80, Loss: 0.02518761157989502\n",
      "Epoch 40, Batch 90, Loss: 0.0203280970454216\n",
      "Epoch 40, Batch 100, Loss: 0.023569071665406227\n",
      "Epoch 41, Batch 0, Loss: 0.026431875303387642\n",
      "Epoch 41, Batch 10, Loss: 0.022997817024588585\n",
      "Epoch 41, Batch 20, Loss: 0.028289584442973137\n",
      "Epoch 41, Batch 30, Loss: 0.02144155278801918\n",
      "Epoch 41, Batch 40, Loss: 0.02554943785071373\n",
      "Epoch 41, Batch 50, Loss: 0.028646521270275116\n",
      "Epoch 41, Batch 60, Loss: 0.02673826925456524\n",
      "Epoch 41, Batch 70, Loss: 0.02305375412106514\n",
      "Epoch 41, Batch 80, Loss: 0.021188654005527496\n",
      "Epoch 41, Batch 90, Loss: 0.02028951793909073\n",
      "Epoch 41, Batch 100, Loss: 0.03113085962831974\n",
      "Epoch 42, Batch 0, Loss: 0.027087176218628883\n",
      "Epoch 42, Batch 10, Loss: 0.022150207310914993\n",
      "Epoch 42, Batch 20, Loss: 0.037645354866981506\n",
      "Epoch 42, Batch 30, Loss: 0.02451203763484955\n",
      "Epoch 42, Batch 40, Loss: 0.02218206226825714\n",
      "Epoch 42, Batch 50, Loss: 0.02447451278567314\n",
      "Epoch 42, Batch 60, Loss: 0.022840537130832672\n",
      "Epoch 42, Batch 70, Loss: 0.027102990075945854\n",
      "Epoch 42, Batch 80, Loss: 0.03157675638794899\n",
      "Epoch 42, Batch 90, Loss: 0.026584630832076073\n",
      "Epoch 42, Batch 100, Loss: 0.024744417518377304\n",
      "Epoch 43, Batch 0, Loss: 0.02334827370941639\n",
      "Epoch 43, Batch 10, Loss: 0.022899605333805084\n",
      "Epoch 43, Batch 20, Loss: 0.020858224481344223\n",
      "Epoch 43, Batch 30, Loss: 0.015139762312173843\n",
      "Epoch 43, Batch 40, Loss: 0.035243213176727295\n",
      "Epoch 43, Batch 50, Loss: 0.026561439037322998\n",
      "Epoch 43, Batch 60, Loss: 0.023992814123630524\n",
      "Epoch 43, Batch 70, Loss: 0.02767273411154747\n",
      "Epoch 43, Batch 80, Loss: 0.021591991186141968\n",
      "Epoch 43, Batch 90, Loss: 0.02427908033132553\n",
      "Epoch 43, Batch 100, Loss: 0.022648721933364868\n",
      "Epoch 44, Batch 0, Loss: 0.01779676228761673\n",
      "Epoch 44, Batch 10, Loss: 0.031751472502946854\n",
      "Epoch 44, Batch 20, Loss: 0.030102070420980453\n",
      "Epoch 44, Batch 30, Loss: 0.02964099496603012\n",
      "Epoch 44, Batch 40, Loss: 0.033365871757268906\n",
      "Epoch 44, Batch 50, Loss: 0.024252960458397865\n",
      "Epoch 44, Batch 60, Loss: 0.020586393773555756\n",
      "Epoch 44, Batch 70, Loss: 0.021175043657422066\n",
      "Epoch 44, Batch 80, Loss: 0.020190007984638214\n",
      "Epoch 44, Batch 90, Loss: 0.028548656031489372\n",
      "Epoch 44, Batch 100, Loss: 0.02086075395345688\n",
      "Epoch 45, Batch 0, Loss: 0.032153986394405365\n",
      "Epoch 45, Batch 10, Loss: 0.02353855036199093\n",
      "Epoch 45, Batch 20, Loss: 0.0351979061961174\n",
      "Epoch 45, Batch 30, Loss: 0.03318125754594803\n",
      "Epoch 45, Batch 40, Loss: 0.02222568355500698\n",
      "Epoch 45, Batch 50, Loss: 0.02644464373588562\n",
      "Epoch 45, Batch 60, Loss: 0.019748825579881668\n",
      "Epoch 45, Batch 70, Loss: 0.019062433391809464\n",
      "Epoch 45, Batch 80, Loss: 0.022470850497484207\n",
      "Epoch 45, Batch 90, Loss: 0.028208952397108078\n",
      "Epoch 45, Batch 100, Loss: 0.021851737052202225\n",
      "Epoch 46, Batch 0, Loss: 0.025717953220009804\n",
      "Epoch 46, Batch 10, Loss: 0.02354677952826023\n",
      "Epoch 46, Batch 20, Loss: 0.02152537927031517\n",
      "Epoch 46, Batch 30, Loss: 0.021790677681565285\n",
      "Epoch 46, Batch 40, Loss: 0.021865539252758026\n",
      "Epoch 46, Batch 50, Loss: 0.03254436329007149\n",
      "Epoch 46, Batch 60, Loss: 0.03507872670888901\n",
      "Epoch 46, Batch 70, Loss: 0.022155508399009705\n",
      "Epoch 46, Batch 80, Loss: 0.018034396693110466\n",
      "Epoch 46, Batch 90, Loss: 0.021563705056905746\n",
      "Epoch 46, Batch 100, Loss: 0.02084250934422016\n",
      "Epoch 47, Batch 0, Loss: 0.025029761716723442\n",
      "Epoch 47, Batch 10, Loss: 0.023828042671084404\n",
      "Epoch 47, Batch 20, Loss: 0.024093948304653168\n",
      "Epoch 47, Batch 30, Loss: 0.022568771615624428\n",
      "Epoch 47, Batch 40, Loss: 0.02976691722869873\n",
      "Epoch 47, Batch 50, Loss: 0.023865681141614914\n",
      "Epoch 47, Batch 60, Loss: 0.025423221290111542\n",
      "Epoch 47, Batch 70, Loss: 0.02203221246600151\n",
      "Epoch 47, Batch 80, Loss: 0.02435312233865261\n",
      "Epoch 47, Batch 90, Loss: 0.028079204261302948\n",
      "Epoch 47, Batch 100, Loss: 0.020214136689901352\n",
      "Epoch 48, Batch 0, Loss: 0.02452923357486725\n",
      "Epoch 48, Batch 10, Loss: 0.023295719176530838\n",
      "Epoch 48, Batch 20, Loss: 0.029175598174333572\n",
      "Epoch 48, Batch 30, Loss: 0.02409180998802185\n",
      "Epoch 48, Batch 40, Loss: 0.029384901747107506\n",
      "Epoch 48, Batch 50, Loss: 0.03301148861646652\n",
      "Epoch 48, Batch 60, Loss: 0.02277914620935917\n",
      "Epoch 48, Batch 70, Loss: 0.01964687928557396\n",
      "Epoch 48, Batch 80, Loss: 0.023126626387238503\n",
      "Epoch 48, Batch 90, Loss: 0.023658527061343193\n",
      "Epoch 48, Batch 100, Loss: 0.02931409329175949\n",
      "Epoch 49, Batch 0, Loss: 0.018341323360800743\n",
      "Epoch 49, Batch 10, Loss: 0.01930547133088112\n",
      "Epoch 49, Batch 20, Loss: 0.022289209067821503\n",
      "Epoch 49, Batch 30, Loss: 0.02927352301776409\n",
      "Epoch 49, Batch 40, Loss: 0.028583336621522903\n",
      "Epoch 49, Batch 50, Loss: 0.023738721385598183\n",
      "Epoch 49, Batch 60, Loss: 0.019382618367671967\n",
      "Epoch 49, Batch 70, Loss: 0.022817598655819893\n",
      "Epoch 49, Batch 80, Loss: 0.023054765537381172\n",
      "Epoch 49, Batch 90, Loss: 0.026067480444908142\n",
      "Epoch 49, Batch 100, Loss: 0.019181234762072563\n",
      "Epoch 50, Batch 0, Loss: 0.02025473676621914\n",
      "Epoch 50, Batch 10, Loss: 0.025229591876268387\n",
      "Epoch 50, Batch 20, Loss: 0.03024798445403576\n",
      "Epoch 50, Batch 30, Loss: 0.024878300726413727\n",
      "Epoch 50, Batch 40, Loss: 0.030656246468424797\n",
      "Epoch 50, Batch 50, Loss: 0.02203817293047905\n",
      "Epoch 50, Batch 60, Loss: 0.025867396965622902\n",
      "Epoch 50, Batch 70, Loss: 0.021129125729203224\n",
      "Epoch 50, Batch 80, Loss: 0.026745090261101723\n",
      "Epoch 50, Batch 90, Loss: 0.021187730133533478\n",
      "Epoch 50, Batch 100, Loss: 0.022645290940999985\n",
      "Epoch 51, Batch 0, Loss: 0.02490999549627304\n",
      "Epoch 51, Batch 10, Loss: 0.016978397965431213\n",
      "Epoch 51, Batch 20, Loss: 0.01655818335711956\n",
      "Epoch 51, Batch 30, Loss: 0.028603408485651016\n",
      "Epoch 51, Batch 40, Loss: 0.02689499408006668\n",
      "Epoch 51, Batch 50, Loss: 0.024335172027349472\n",
      "Epoch 51, Batch 60, Loss: 0.02322426065802574\n",
      "Epoch 51, Batch 70, Loss: 0.02384885773062706\n",
      "Epoch 51, Batch 80, Loss: 0.024527542293071747\n",
      "Epoch 51, Batch 90, Loss: 0.022674210369586945\n",
      "Epoch 51, Batch 100, Loss: 0.025262435898184776\n",
      "Epoch 52, Batch 0, Loss: 0.02234472706913948\n",
      "Epoch 52, Batch 10, Loss: 0.02227957919239998\n",
      "Epoch 52, Batch 20, Loss: 0.023029617965221405\n",
      "Epoch 52, Batch 30, Loss: 0.023462381213903427\n",
      "Epoch 52, Batch 40, Loss: 0.028888070955872536\n",
      "Epoch 52, Batch 50, Loss: 0.02174319326877594\n",
      "Epoch 52, Batch 60, Loss: 0.020163245499134064\n",
      "Epoch 52, Batch 70, Loss: 0.027040280401706696\n",
      "Epoch 52, Batch 80, Loss: 0.03113878145813942\n",
      "Epoch 52, Batch 90, Loss: 0.028105581179261208\n",
      "Epoch 52, Batch 100, Loss: 0.022842492908239365\n",
      "Epoch 53, Batch 0, Loss: 0.025839708745479584\n",
      "Epoch 53, Batch 10, Loss: 0.025769434869289398\n",
      "Epoch 53, Batch 20, Loss: 0.022486524656414986\n",
      "Epoch 53, Batch 30, Loss: 0.02272479049861431\n",
      "Epoch 53, Batch 40, Loss: 0.020242754369974136\n",
      "Epoch 53, Batch 50, Loss: 0.02510913833975792\n",
      "Epoch 53, Batch 60, Loss: 0.021376991644501686\n",
      "Epoch 53, Batch 70, Loss: 0.023389453068375587\n",
      "Epoch 53, Batch 80, Loss: 0.021394725888967514\n",
      "Epoch 53, Batch 90, Loss: 0.02228565141558647\n",
      "Epoch 53, Batch 100, Loss: 0.020714757964015007\n",
      "Epoch 54, Batch 0, Loss: 0.021473869681358337\n",
      "Epoch 54, Batch 10, Loss: 0.022911878302693367\n",
      "Epoch 54, Batch 20, Loss: 0.024604767560958862\n",
      "Epoch 54, Batch 30, Loss: 0.02521795779466629\n",
      "Epoch 54, Batch 40, Loss: 0.01806974597275257\n",
      "Epoch 54, Batch 50, Loss: 0.019056759774684906\n",
      "Epoch 54, Batch 60, Loss: 0.02831104025244713\n",
      "Epoch 54, Batch 70, Loss: 0.023333603516221046\n",
      "Epoch 54, Batch 80, Loss: 0.026311302557587624\n",
      "Epoch 54, Batch 90, Loss: 0.026117637753486633\n",
      "Epoch 54, Batch 100, Loss: 0.02294827252626419\n",
      "Epoch 55, Batch 0, Loss: 0.021165791898965836\n",
      "Epoch 55, Batch 10, Loss: 0.026783160865306854\n",
      "Epoch 55, Batch 20, Loss: 0.0205937959253788\n",
      "Epoch 55, Batch 30, Loss: 0.019092630594968796\n",
      "Epoch 55, Batch 40, Loss: 0.021138636395335197\n",
      "Epoch 55, Batch 50, Loss: 0.02270195446908474\n",
      "Epoch 55, Batch 60, Loss: 0.025866497308015823\n",
      "Epoch 55, Batch 70, Loss: 0.02926863729953766\n",
      "Epoch 55, Batch 80, Loss: 0.027701180428266525\n",
      "Epoch 55, Batch 90, Loss: 0.030184928327798843\n",
      "Epoch 55, Batch 100, Loss: 0.025573663413524628\n",
      "Epoch 56, Batch 0, Loss: 0.026791028678417206\n",
      "Epoch 56, Batch 10, Loss: 0.02332000620663166\n",
      "Epoch 56, Batch 20, Loss: 0.024477355182170868\n",
      "Epoch 56, Batch 30, Loss: 0.02132684737443924\n",
      "Epoch 56, Batch 40, Loss: 0.022593852132558823\n",
      "Epoch 56, Batch 50, Loss: 0.0259503535926342\n",
      "Epoch 56, Batch 60, Loss: 0.020785391330718994\n",
      "Epoch 56, Batch 70, Loss: 0.02094719558954239\n",
      "Epoch 56, Batch 80, Loss: 0.03395013511180878\n",
      "Epoch 56, Batch 90, Loss: 0.029714304953813553\n",
      "Epoch 56, Batch 100, Loss: 0.022690296173095703\n",
      "Epoch 57, Batch 0, Loss: 0.026730965822935104\n",
      "Epoch 57, Batch 10, Loss: 0.030979041010141373\n",
      "Epoch 57, Batch 20, Loss: 0.026349099352955818\n",
      "Epoch 57, Batch 30, Loss: 0.024729371070861816\n",
      "Epoch 57, Batch 40, Loss: 0.03124535270035267\n",
      "Epoch 57, Batch 50, Loss: 0.023069720715284348\n",
      "Epoch 57, Batch 60, Loss: 0.02565794810652733\n",
      "Epoch 57, Batch 70, Loss: 0.03989574313163757\n",
      "Epoch 57, Batch 80, Loss: 0.020383473485708237\n",
      "Epoch 57, Batch 90, Loss: 0.024274347350001335\n",
      "Epoch 57, Batch 100, Loss: 0.024322517216205597\n",
      "Epoch 58, Batch 0, Loss: 0.023622920736670494\n",
      "Epoch 58, Batch 10, Loss: 0.02167636901140213\n",
      "Epoch 58, Batch 20, Loss: 0.02162744477391243\n",
      "Epoch 58, Batch 30, Loss: 0.023503106087446213\n",
      "Epoch 58, Batch 40, Loss: 0.017444081604480743\n",
      "Epoch 58, Batch 50, Loss: 0.029011191800236702\n",
      "Epoch 58, Batch 60, Loss: 0.021927379071712494\n",
      "Epoch 58, Batch 70, Loss: 0.023737017065286636\n",
      "Epoch 58, Batch 80, Loss: 0.02948889136314392\n",
      "Epoch 58, Batch 90, Loss: 0.033495113253593445\n",
      "Epoch 58, Batch 100, Loss: 0.023965327069163322\n",
      "Epoch 59, Batch 0, Loss: 0.01771322824060917\n",
      "Epoch 59, Batch 10, Loss: 0.01928315870463848\n",
      "Epoch 59, Batch 20, Loss: 0.03064478561282158\n",
      "Epoch 59, Batch 30, Loss: 0.022573798894882202\n",
      "Epoch 59, Batch 40, Loss: 0.02075793966650963\n",
      "Epoch 59, Batch 50, Loss: 0.023564571514725685\n",
      "Epoch 59, Batch 60, Loss: 0.02175934799015522\n",
      "Epoch 59, Batch 70, Loss: 0.021996043622493744\n",
      "Epoch 59, Batch 80, Loss: 0.021321922540664673\n",
      "Epoch 59, Batch 90, Loss: 0.03803032264113426\n",
      "Epoch 59, Batch 100, Loss: 0.021542884409427643\n",
      "Epoch 60, Batch 0, Loss: 0.01863264851272106\n",
      "Epoch 60, Batch 10, Loss: 0.02420433610677719\n",
      "Epoch 60, Batch 20, Loss: 0.024349361658096313\n",
      "Epoch 60, Batch 30, Loss: 0.023388229310512543\n",
      "Epoch 60, Batch 40, Loss: 0.018663713708519936\n",
      "Epoch 60, Batch 50, Loss: 0.02842198684811592\n",
      "Epoch 60, Batch 60, Loss: 0.021257124841213226\n",
      "Epoch 60, Batch 70, Loss: 0.01448787935078144\n",
      "Epoch 60, Batch 80, Loss: 0.022903865203261375\n",
      "Epoch 60, Batch 90, Loss: 0.018729206174612045\n",
      "Epoch 60, Batch 100, Loss: 0.01986907422542572\n",
      "Epoch 61, Batch 0, Loss: 0.023066751658916473\n",
      "Epoch 61, Batch 10, Loss: 0.031183676794171333\n",
      "Epoch 61, Batch 20, Loss: 0.030486973002552986\n",
      "Epoch 61, Batch 30, Loss: 0.019773703068494797\n",
      "Epoch 61, Batch 40, Loss: 0.025565117597579956\n",
      "Epoch 61, Batch 50, Loss: 0.022407889366149902\n",
      "Epoch 61, Batch 60, Loss: 0.018240677192807198\n",
      "Epoch 61, Batch 70, Loss: 0.01841583289206028\n",
      "Epoch 61, Batch 80, Loss: 0.02899158000946045\n",
      "Epoch 61, Batch 90, Loss: 0.028941573575139046\n",
      "Epoch 61, Batch 100, Loss: 0.028146488592028618\n",
      "Epoch 62, Batch 0, Loss: 0.020511196926236153\n",
      "Epoch 62, Batch 10, Loss: 0.027799159288406372\n",
      "Epoch 62, Batch 20, Loss: 0.026775116100907326\n",
      "Epoch 62, Batch 30, Loss: 0.03139945864677429\n",
      "Epoch 62, Batch 40, Loss: 0.017897741869091988\n",
      "Epoch 62, Batch 50, Loss: 0.023944837972521782\n",
      "Epoch 62, Batch 60, Loss: 0.02196197584271431\n",
      "Epoch 62, Batch 70, Loss: 0.028290661051869392\n",
      "Epoch 62, Batch 80, Loss: 0.03269293159246445\n",
      "Epoch 62, Batch 90, Loss: 0.02204069308936596\n",
      "Epoch 62, Batch 100, Loss: 0.023895345628261566\n",
      "Epoch 63, Batch 0, Loss: 0.019355058670043945\n",
      "Epoch 63, Batch 10, Loss: 0.021551521494984627\n",
      "Epoch 63, Batch 20, Loss: 0.020020097494125366\n",
      "Epoch 63, Batch 30, Loss: 0.024239445105195045\n",
      "Epoch 63, Batch 40, Loss: 0.024063026532530785\n",
      "Epoch 63, Batch 50, Loss: 0.023376543074846268\n",
      "Epoch 63, Batch 60, Loss: 0.02879437990486622\n",
      "Epoch 63, Batch 70, Loss: 0.017956580966711044\n",
      "Epoch 63, Batch 80, Loss: 0.030334580689668655\n",
      "Epoch 63, Batch 90, Loss: 0.01915249042212963\n",
      "Epoch 63, Batch 100, Loss: 0.02373376674950123\n",
      "Epoch 64, Batch 0, Loss: 0.028910035267472267\n",
      "Epoch 64, Batch 10, Loss: 0.026714354753494263\n",
      "Epoch 64, Batch 20, Loss: 0.024843579158186913\n",
      "Epoch 64, Batch 30, Loss: 0.030468160286545753\n",
      "Epoch 64, Batch 40, Loss: 0.025662463158369064\n",
      "Epoch 64, Batch 50, Loss: 0.03461531549692154\n",
      "Epoch 64, Batch 60, Loss: 0.027012845501303673\n",
      "Epoch 64, Batch 70, Loss: 0.02576318383216858\n",
      "Epoch 64, Batch 80, Loss: 0.019916146993637085\n",
      "Epoch 64, Batch 90, Loss: 0.0195242241024971\n",
      "Epoch 64, Batch 100, Loss: 0.017888886854052544\n",
      "Epoch 65, Batch 0, Loss: 0.02324867993593216\n",
      "Epoch 65, Batch 10, Loss: 0.02433381788432598\n",
      "Epoch 65, Batch 20, Loss: 0.018332242965698242\n",
      "Epoch 65, Batch 30, Loss: 0.031686943024396896\n",
      "Epoch 65, Batch 40, Loss: 0.029118236154317856\n",
      "Epoch 65, Batch 50, Loss: 0.02012384682893753\n",
      "Epoch 65, Batch 60, Loss: 0.01989874616265297\n",
      "Epoch 65, Batch 70, Loss: 0.019252192229032516\n",
      "Epoch 65, Batch 80, Loss: 0.018773213028907776\n",
      "Epoch 65, Batch 90, Loss: 0.02322838269174099\n",
      "Epoch 65, Batch 100, Loss: 0.02400563843548298\n",
      "Epoch 66, Batch 0, Loss: 0.01931600086390972\n",
      "Epoch 66, Batch 10, Loss: 0.02464745007455349\n",
      "Epoch 66, Batch 20, Loss: 0.018121087923645973\n",
      "Epoch 66, Batch 30, Loss: 0.025595013052225113\n",
      "Epoch 66, Batch 40, Loss: 0.026419352740049362\n",
      "Epoch 66, Batch 50, Loss: 0.02365301363170147\n",
      "Epoch 66, Batch 60, Loss: 0.019732315093278885\n",
      "Epoch 66, Batch 70, Loss: 0.027236243709921837\n",
      "Epoch 66, Batch 80, Loss: 0.018357323482632637\n",
      "Epoch 66, Batch 90, Loss: 0.01404842920601368\n",
      "Epoch 66, Batch 100, Loss: 0.028496993705630302\n",
      "Epoch 67, Batch 0, Loss: 0.019258592277765274\n",
      "Epoch 67, Batch 10, Loss: 0.020010055974125862\n",
      "Epoch 67, Batch 20, Loss: 0.02085449919104576\n",
      "Epoch 67, Batch 30, Loss: 0.0220334529876709\n",
      "Epoch 67, Batch 40, Loss: 0.026969032362103462\n",
      "Epoch 67, Batch 50, Loss: 0.024314764887094498\n",
      "Epoch 67, Batch 60, Loss: 0.02504097856581211\n",
      "Epoch 67, Batch 70, Loss: 0.020770268514752388\n",
      "Epoch 67, Batch 80, Loss: 0.020086243748664856\n",
      "Epoch 67, Batch 90, Loss: 0.026003289967775345\n",
      "Epoch 67, Batch 100, Loss: 0.026992471888661385\n",
      "Epoch 68, Batch 0, Loss: 0.020152775570750237\n",
      "Epoch 68, Batch 10, Loss: 0.0177261084318161\n",
      "Epoch 68, Batch 20, Loss: 0.016715725883841515\n",
      "Epoch 68, Batch 30, Loss: 0.03158671781420708\n",
      "Epoch 68, Batch 40, Loss: 0.022061791270971298\n",
      "Epoch 68, Batch 50, Loss: 0.027698196470737457\n",
      "Epoch 68, Batch 60, Loss: 0.021964184939861298\n",
      "Epoch 68, Batch 70, Loss: 0.02438027784228325\n",
      "Epoch 68, Batch 80, Loss: 0.022590113803744316\n",
      "Epoch 68, Batch 90, Loss: 0.018446728587150574\n",
      "Epoch 68, Batch 100, Loss: 0.031541697680950165\n",
      "Epoch 69, Batch 0, Loss: 0.018570175394415855\n",
      "Epoch 69, Batch 10, Loss: 0.01877550035715103\n",
      "Epoch 69, Batch 20, Loss: 0.022847970947623253\n",
      "Epoch 69, Batch 30, Loss: 0.025960557162761688\n",
      "Epoch 69, Batch 40, Loss: 0.029934929683804512\n",
      "Epoch 69, Batch 50, Loss: 0.022618919610977173\n",
      "Epoch 69, Batch 60, Loss: 0.02802325412631035\n",
      "Epoch 69, Batch 70, Loss: 0.018671592697501183\n",
      "Epoch 69, Batch 80, Loss: 0.020846284925937653\n",
      "Epoch 69, Batch 90, Loss: 0.020931798964738846\n",
      "Epoch 69, Batch 100, Loss: 0.02235066331923008\n",
      "Epoch 70, Batch 0, Loss: 0.026013825088739395\n",
      "Epoch 70, Batch 10, Loss: 0.01901851035654545\n",
      "Epoch 70, Batch 20, Loss: 0.023443007841706276\n",
      "Epoch 70, Batch 30, Loss: 0.023816216737031937\n",
      "Epoch 70, Batch 40, Loss: 0.01993899792432785\n",
      "Epoch 70, Batch 50, Loss: 0.02982483059167862\n",
      "Epoch 70, Batch 60, Loss: 0.030925441533327103\n",
      "Epoch 70, Batch 70, Loss: 0.031219929456710815\n",
      "Epoch 70, Batch 80, Loss: 0.027934465557336807\n",
      "Epoch 70, Batch 90, Loss: 0.020192209631204605\n",
      "Epoch 70, Batch 100, Loss: 0.03204016014933586\n",
      "Epoch 71, Batch 0, Loss: 0.02547948807477951\n",
      "Epoch 71, Batch 10, Loss: 0.02826748415827751\n",
      "Epoch 71, Batch 20, Loss: 0.01829325221478939\n",
      "Epoch 71, Batch 30, Loss: 0.019136378541588783\n",
      "Epoch 71, Batch 40, Loss: 0.027900077402591705\n",
      "Epoch 71, Batch 50, Loss: 0.024355106055736542\n",
      "Epoch 71, Batch 60, Loss: 0.020398402586579323\n",
      "Epoch 71, Batch 70, Loss: 0.021914182230830193\n",
      "Epoch 71, Batch 80, Loss: 0.023647218942642212\n",
      "Epoch 71, Batch 90, Loss: 0.023363027721643448\n",
      "Epoch 71, Batch 100, Loss: 0.032450709491968155\n",
      "Epoch 72, Batch 0, Loss: 0.02217140980064869\n",
      "Epoch 72, Batch 10, Loss: 0.02221374213695526\n",
      "Epoch 72, Batch 20, Loss: 0.024036452174186707\n",
      "Epoch 72, Batch 30, Loss: 0.024928158149123192\n",
      "Epoch 72, Batch 40, Loss: 0.02369762398302555\n",
      "Epoch 72, Batch 50, Loss: 0.0257935281842947\n",
      "Epoch 72, Batch 60, Loss: 0.024602508172392845\n",
      "Epoch 72, Batch 70, Loss: 0.022578148171305656\n",
      "Epoch 72, Batch 80, Loss: 0.026024630293250084\n",
      "Epoch 72, Batch 90, Loss: 0.019121509045362473\n",
      "Epoch 72, Batch 100, Loss: 0.019883999601006508\n",
      "Epoch 73, Batch 0, Loss: 0.0241059772670269\n",
      "Epoch 73, Batch 10, Loss: 0.02136414870619774\n",
      "Epoch 73, Batch 20, Loss: 0.021352112293243408\n",
      "Epoch 73, Batch 30, Loss: 0.017876461148262024\n",
      "Epoch 73, Batch 40, Loss: 0.027246661484241486\n",
      "Epoch 73, Batch 50, Loss: 0.023450402542948723\n",
      "Epoch 73, Batch 60, Loss: 0.030090870335698128\n",
      "Epoch 73, Batch 70, Loss: 0.025286296382546425\n",
      "Epoch 73, Batch 80, Loss: 0.01866229996085167\n",
      "Epoch 73, Batch 90, Loss: 0.02500440552830696\n",
      "Epoch 73, Batch 100, Loss: 0.031715650111436844\n",
      "Epoch 74, Batch 0, Loss: 0.0212448388338089\n",
      "Epoch 74, Batch 10, Loss: 0.032565973699092865\n",
      "Epoch 74, Batch 20, Loss: 0.018689671531319618\n",
      "Epoch 74, Batch 30, Loss: 0.021686170250177383\n",
      "Epoch 74, Batch 40, Loss: 0.02438066154718399\n",
      "Epoch 74, Batch 50, Loss: 0.023249797523021698\n",
      "Epoch 74, Batch 60, Loss: 0.020359396934509277\n",
      "Epoch 74, Batch 70, Loss: 0.025530796498060226\n",
      "Epoch 74, Batch 80, Loss: 0.02982986532151699\n",
      "Epoch 74, Batch 90, Loss: 0.020685561001300812\n",
      "Epoch 74, Batch 100, Loss: 0.02011893130838871\n",
      "Epoch 75, Batch 0, Loss: 0.02861986681818962\n",
      "Epoch 75, Batch 10, Loss: 0.01924460008740425\n",
      "Epoch 75, Batch 20, Loss: 0.0317971333861351\n",
      "Epoch 75, Batch 30, Loss: 0.02033565565943718\n",
      "Epoch 75, Batch 40, Loss: 0.025454871356487274\n",
      "Epoch 75, Batch 50, Loss: 0.017253365367650986\n",
      "Epoch 75, Batch 60, Loss: 0.030033204704523087\n",
      "Epoch 75, Batch 70, Loss: 0.019600944593548775\n",
      "Epoch 75, Batch 80, Loss: 0.02265155501663685\n",
      "Epoch 75, Batch 90, Loss: 0.026011478155851364\n",
      "Epoch 75, Batch 100, Loss: 0.02008635550737381\n",
      "Epoch 76, Batch 0, Loss: 0.025376489385962486\n",
      "Epoch 76, Batch 10, Loss: 0.026338685303926468\n",
      "Epoch 76, Batch 20, Loss: 0.02459084801375866\n",
      "Epoch 76, Batch 30, Loss: 0.02245124615728855\n",
      "Epoch 76, Batch 40, Loss: 0.01859499141573906\n",
      "Epoch 76, Batch 50, Loss: 0.02838754653930664\n",
      "Epoch 76, Batch 60, Loss: 0.022235630080103874\n",
      "Epoch 76, Batch 70, Loss: 0.02694476768374443\n",
      "Epoch 76, Batch 80, Loss: 0.016662759706377983\n",
      "Epoch 76, Batch 90, Loss: 0.028881240636110306\n",
      "Epoch 76, Batch 100, Loss: 0.035252831876277924\n",
      "Epoch 77, Batch 0, Loss: 0.024219373241066933\n",
      "Epoch 77, Batch 10, Loss: 0.031016848981380463\n",
      "Epoch 77, Batch 20, Loss: 0.020300820469856262\n",
      "Epoch 77, Batch 30, Loss: 0.022103382274508476\n",
      "Epoch 77, Batch 40, Loss: 0.016562022268772125\n",
      "Epoch 77, Batch 50, Loss: 0.017122158780694008\n",
      "Epoch 77, Batch 60, Loss: 0.025178195908665657\n",
      "Epoch 77, Batch 70, Loss: 0.021372593939304352\n",
      "Epoch 77, Batch 80, Loss: 0.025763876736164093\n",
      "Epoch 77, Batch 90, Loss: 0.02583126723766327\n",
      "Epoch 77, Batch 100, Loss: 0.023937948048114777\n",
      "Epoch 78, Batch 0, Loss: 0.03134242072701454\n",
      "Epoch 78, Batch 10, Loss: 0.022378448396921158\n",
      "Epoch 78, Batch 20, Loss: 0.023309145122766495\n",
      "Epoch 78, Batch 30, Loss: 0.020574741065502167\n",
      "Epoch 78, Batch 40, Loss: 0.025447936728596687\n",
      "Epoch 78, Batch 50, Loss: 0.02269749902188778\n",
      "Epoch 78, Batch 60, Loss: 0.024506492540240288\n",
      "Epoch 78, Batch 70, Loss: 0.025250140577554703\n",
      "Epoch 78, Batch 80, Loss: 0.030693570151925087\n",
      "Epoch 78, Batch 90, Loss: 0.020922120660543442\n",
      "Epoch 78, Batch 100, Loss: 0.01900951936841011\n",
      "Epoch 79, Batch 0, Loss: 0.029387574642896652\n",
      "Epoch 79, Batch 10, Loss: 0.02233673445880413\n",
      "Epoch 79, Batch 20, Loss: 0.026450898498296738\n",
      "Epoch 79, Batch 30, Loss: 0.022237960249185562\n",
      "Epoch 79, Batch 40, Loss: 0.022960413247346878\n",
      "Epoch 79, Batch 50, Loss: 0.016564901918172836\n",
      "Epoch 79, Batch 60, Loss: 0.022468499839305878\n",
      "Epoch 79, Batch 70, Loss: 0.021082762628793716\n",
      "Epoch 79, Batch 80, Loss: 0.022856853902339935\n",
      "Epoch 79, Batch 90, Loss: 0.024349769577383995\n",
      "Epoch 79, Batch 100, Loss: 0.023897575214505196\n",
      "Epoch 80, Batch 0, Loss: 0.02340133860707283\n",
      "Epoch 80, Batch 10, Loss: 0.03198890760540962\n",
      "Epoch 80, Batch 20, Loss: 0.0176281426101923\n",
      "Epoch 80, Batch 30, Loss: 0.025955593213438988\n",
      "Epoch 80, Batch 40, Loss: 0.028382865712046623\n",
      "Epoch 80, Batch 50, Loss: 0.02102864347398281\n",
      "Epoch 80, Batch 60, Loss: 0.021535411477088928\n",
      "Epoch 80, Batch 70, Loss: 0.02093488723039627\n",
      "Epoch 80, Batch 80, Loss: 0.020903056487441063\n",
      "Epoch 80, Batch 90, Loss: 0.022354671731591225\n",
      "Epoch 80, Batch 100, Loss: 0.029767943546175957\n",
      "Epoch 81, Batch 0, Loss: 0.020382003858685493\n",
      "Epoch 81, Batch 10, Loss: 0.02570434659719467\n",
      "Epoch 81, Batch 20, Loss: 0.027320826426148415\n",
      "Epoch 81, Batch 30, Loss: 0.029512854292988777\n",
      "Epoch 81, Batch 40, Loss: 0.03060189262032509\n",
      "Epoch 81, Batch 50, Loss: 0.024063214659690857\n",
      "Epoch 81, Batch 60, Loss: 0.026472551748156548\n",
      "Epoch 81, Batch 70, Loss: 0.028656860813498497\n",
      "Epoch 81, Batch 80, Loss: 0.0258379764854908\n",
      "Epoch 81, Batch 90, Loss: 0.02974880114197731\n",
      "Epoch 81, Batch 100, Loss: 0.027690501883625984\n",
      "Epoch 82, Batch 0, Loss: 0.03068801388144493\n",
      "Epoch 82, Batch 10, Loss: 0.0208442285656929\n",
      "Epoch 82, Batch 20, Loss: 0.024674342945218086\n",
      "Epoch 82, Batch 30, Loss: 0.023270420730113983\n",
      "Epoch 82, Batch 40, Loss: 0.028068603947758675\n",
      "Epoch 82, Batch 50, Loss: 0.027175065129995346\n",
      "Epoch 82, Batch 60, Loss: 0.02599584124982357\n",
      "Epoch 82, Batch 70, Loss: 0.019650189206004143\n",
      "Epoch 82, Batch 80, Loss: 0.01814005710184574\n",
      "Epoch 82, Batch 90, Loss: 0.022399190813302994\n",
      "Epoch 82, Batch 100, Loss: 0.02387920208275318\n",
      "Epoch 83, Batch 0, Loss: 0.021069835871458054\n",
      "Epoch 83, Batch 10, Loss: 0.020358795300126076\n",
      "Epoch 83, Batch 20, Loss: 0.01878507435321808\n",
      "Epoch 83, Batch 30, Loss: 0.027958931401371956\n",
      "Epoch 83, Batch 40, Loss: 0.02839682251214981\n",
      "Epoch 83, Batch 50, Loss: 0.024122655391693115\n",
      "Epoch 83, Batch 60, Loss: 0.025674622505903244\n",
      "Epoch 83, Batch 70, Loss: 0.026896923780441284\n",
      "Epoch 83, Batch 80, Loss: 0.023941177874803543\n",
      "Epoch 83, Batch 90, Loss: 0.026278270408511162\n",
      "Epoch 83, Batch 100, Loss: 0.018946511670947075\n",
      "Epoch 84, Batch 0, Loss: 0.024924803525209427\n",
      "Epoch 84, Batch 10, Loss: 0.022162603214383125\n",
      "Epoch 84, Batch 20, Loss: 0.029027491807937622\n",
      "Epoch 84, Batch 30, Loss: 0.02244742214679718\n",
      "Epoch 84, Batch 40, Loss: 0.02314164489507675\n",
      "Epoch 84, Batch 50, Loss: 0.022138264030218124\n",
      "Epoch 84, Batch 60, Loss: 0.02241906151175499\n",
      "Epoch 84, Batch 70, Loss: 0.023485353216528893\n",
      "Epoch 84, Batch 80, Loss: 0.027096334844827652\n",
      "Epoch 84, Batch 90, Loss: 0.019192880019545555\n",
      "Epoch 84, Batch 100, Loss: 0.022989168763160706\n",
      "Epoch 85, Batch 0, Loss: 0.026545247063040733\n",
      "Epoch 85, Batch 10, Loss: 0.020241491496562958\n",
      "Epoch 85, Batch 20, Loss: 0.024037355557084084\n",
      "Epoch 85, Batch 30, Loss: 0.0227874256670475\n",
      "Epoch 85, Batch 40, Loss: 0.02288280427455902\n",
      "Epoch 85, Batch 50, Loss: 0.02397068217396736\n",
      "Epoch 85, Batch 60, Loss: 0.027504488825798035\n",
      "Epoch 85, Batch 70, Loss: 0.030067309737205505\n",
      "Epoch 85, Batch 80, Loss: 0.01874513365328312\n",
      "Epoch 85, Batch 90, Loss: 0.020014435052871704\n",
      "Epoch 85, Batch 100, Loss: 0.019946187734603882\n",
      "Epoch 86, Batch 0, Loss: 0.02020862139761448\n",
      "Epoch 86, Batch 10, Loss: 0.02116275206208229\n",
      "Epoch 86, Batch 20, Loss: 0.027870625257492065\n",
      "Epoch 86, Batch 30, Loss: 0.019105851650238037\n",
      "Epoch 86, Batch 40, Loss: 0.02240092307329178\n",
      "Epoch 86, Batch 50, Loss: 0.029767781496047974\n",
      "Epoch 86, Batch 60, Loss: 0.025157775729894638\n",
      "Epoch 86, Batch 70, Loss: 0.028106335550546646\n",
      "Epoch 86, Batch 80, Loss: 0.025692088529467583\n",
      "Epoch 86, Batch 90, Loss: 0.02005002461373806\n",
      "Epoch 86, Batch 100, Loss: 0.029555222019553185\n",
      "Epoch 87, Batch 0, Loss: 0.022204870358109474\n",
      "Epoch 87, Batch 10, Loss: 0.024690009653568268\n",
      "Epoch 87, Batch 20, Loss: 0.02395695261657238\n",
      "Epoch 87, Batch 30, Loss: 0.025694141164422035\n",
      "Epoch 87, Batch 40, Loss: 0.01567169651389122\n",
      "Epoch 87, Batch 50, Loss: 0.02525453083217144\n",
      "Epoch 87, Batch 60, Loss: 0.026507750153541565\n",
      "Epoch 87, Batch 70, Loss: 0.0267166830599308\n",
      "Epoch 87, Batch 80, Loss: 0.0136309415102005\n",
      "Epoch 87, Batch 90, Loss: 0.02345895580947399\n",
      "Epoch 87, Batch 100, Loss: 0.021010231226682663\n",
      "Epoch 88, Batch 0, Loss: 0.02198227494955063\n",
      "Epoch 88, Batch 10, Loss: 0.02634323574602604\n",
      "Epoch 88, Batch 20, Loss: 0.022304877638816833\n",
      "Epoch 88, Batch 30, Loss: 0.027409076690673828\n",
      "Epoch 88, Batch 40, Loss: 0.018093710765242577\n",
      "Epoch 88, Batch 50, Loss: 0.025011394172906876\n",
      "Epoch 88, Batch 60, Loss: 0.01862867921590805\n",
      "Epoch 88, Batch 70, Loss: 0.019163990393280983\n",
      "Epoch 88, Batch 80, Loss: 0.029377982020378113\n",
      "Epoch 88, Batch 90, Loss: 0.02311518043279648\n",
      "Epoch 88, Batch 100, Loss: 0.025368789210915565\n",
      "Epoch 89, Batch 0, Loss: 0.02427930384874344\n",
      "Epoch 89, Batch 10, Loss: 0.02532782591879368\n",
      "Epoch 89, Batch 20, Loss: 0.028316671028733253\n",
      "Epoch 89, Batch 30, Loss: 0.01718440093100071\n",
      "Epoch 89, Batch 40, Loss: 0.020717071369290352\n",
      "Epoch 89, Batch 50, Loss: 0.02032429724931717\n",
      "Epoch 89, Batch 60, Loss: 0.02423400618135929\n",
      "Epoch 89, Batch 70, Loss: 0.021846065297722816\n",
      "Epoch 89, Batch 80, Loss: 0.021325506269931793\n",
      "Epoch 89, Batch 90, Loss: 0.030237983912229538\n",
      "Epoch 89, Batch 100, Loss: 0.024910753592848778\n",
      "Epoch 90, Batch 0, Loss: 0.02604123204946518\n",
      "Epoch 90, Batch 10, Loss: 0.025226358324289322\n",
      "Epoch 90, Batch 20, Loss: 0.025658024474978447\n",
      "Epoch 90, Batch 30, Loss: 0.031484734266996384\n",
      "Epoch 90, Batch 40, Loss: 0.019901758059859276\n",
      "Epoch 90, Batch 50, Loss: 0.02073846384882927\n",
      "Epoch 90, Batch 60, Loss: 0.01741679757833481\n",
      "Epoch 90, Batch 70, Loss: 0.023325804620981216\n",
      "Epoch 90, Batch 80, Loss: 0.023994628340005875\n",
      "Epoch 90, Batch 90, Loss: 0.029368016868829727\n",
      "Epoch 90, Batch 100, Loss: 0.017226142808794975\n",
      "Epoch 91, Batch 0, Loss: 0.021820038557052612\n",
      "Epoch 91, Batch 10, Loss: 0.0247366763651371\n",
      "Epoch 91, Batch 20, Loss: 0.022148871794342995\n",
      "Epoch 91, Batch 30, Loss: 0.020350586622953415\n",
      "Epoch 91, Batch 40, Loss: 0.023966308683156967\n",
      "Epoch 91, Batch 50, Loss: 0.03101726993918419\n",
      "Epoch 91, Batch 60, Loss: 0.021077347919344902\n",
      "Epoch 91, Batch 70, Loss: 0.02696315571665764\n",
      "Epoch 91, Batch 80, Loss: 0.018450023606419563\n",
      "Epoch 91, Batch 90, Loss: 0.02915060706436634\n",
      "Epoch 91, Batch 100, Loss: 0.024148300290107727\n",
      "Epoch 92, Batch 0, Loss: 0.03448393940925598\n",
      "Epoch 92, Batch 10, Loss: 0.02168724313378334\n",
      "Epoch 92, Batch 20, Loss: 0.025583267211914062\n",
      "Epoch 92, Batch 30, Loss: 0.030761998146772385\n",
      "Epoch 92, Batch 40, Loss: 0.020664047449827194\n",
      "Epoch 92, Batch 50, Loss: 0.026142267510294914\n",
      "Epoch 92, Batch 60, Loss: 0.017886284738779068\n",
      "Epoch 92, Batch 70, Loss: 0.03661748766899109\n",
      "Epoch 92, Batch 80, Loss: 0.02072448842227459\n",
      "Epoch 92, Batch 90, Loss: 0.02450687810778618\n",
      "Epoch 92, Batch 100, Loss: 0.019227290526032448\n",
      "Epoch 93, Batch 0, Loss: 0.0250894445925951\n",
      "Epoch 93, Batch 10, Loss: 0.015935072675347328\n",
      "Epoch 93, Batch 20, Loss: 0.02068500779569149\n",
      "Epoch 93, Batch 30, Loss: 0.025385405868291855\n",
      "Epoch 93, Batch 40, Loss: 0.022730689495801926\n",
      "Epoch 93, Batch 50, Loss: 0.029634438455104828\n",
      "Epoch 93, Batch 60, Loss: 0.019838770851492882\n",
      "Epoch 93, Batch 70, Loss: 0.028857560828328133\n",
      "Epoch 93, Batch 80, Loss: 0.026475299149751663\n",
      "Epoch 93, Batch 90, Loss: 0.024015288800001144\n",
      "Epoch 93, Batch 100, Loss: 0.0191047303378582\n",
      "Epoch 94, Batch 0, Loss: 0.022252025082707405\n",
      "Epoch 94, Batch 10, Loss: 0.019557790830731392\n",
      "Epoch 94, Batch 20, Loss: 0.023247050121426582\n",
      "Epoch 94, Batch 30, Loss: 0.020715264603495598\n",
      "Epoch 94, Batch 40, Loss: 0.02592570334672928\n",
      "Epoch 94, Batch 50, Loss: 0.03314440697431564\n",
      "Epoch 94, Batch 60, Loss: 0.03206998482346535\n",
      "Epoch 94, Batch 70, Loss: 0.01880609057843685\n",
      "Epoch 94, Batch 80, Loss: 0.01647677645087242\n",
      "Epoch 94, Batch 90, Loss: 0.037821318954229355\n",
      "Epoch 94, Batch 100, Loss: 0.02150990441441536\n",
      "Epoch 95, Batch 0, Loss: 0.019851019605994225\n",
      "Epoch 95, Batch 10, Loss: 0.0228278748691082\n",
      "Epoch 95, Batch 20, Loss: 0.019091889262199402\n",
      "Epoch 95, Batch 30, Loss: 0.02705570124089718\n",
      "Epoch 95, Batch 40, Loss: 0.02692178264260292\n",
      "Epoch 95, Batch 50, Loss: 0.019728166982531548\n",
      "Epoch 95, Batch 60, Loss: 0.020849473774433136\n",
      "Epoch 95, Batch 70, Loss: 0.03000504896044731\n",
      "Epoch 95, Batch 80, Loss: 0.02379348687827587\n",
      "Epoch 95, Batch 90, Loss: 0.02981290966272354\n",
      "Epoch 95, Batch 100, Loss: 0.026374679058790207\n",
      "Epoch 96, Batch 0, Loss: 0.02311023697257042\n",
      "Epoch 96, Batch 10, Loss: 0.020772434771060944\n",
      "Epoch 96, Batch 20, Loss: 0.023448988795280457\n",
      "Epoch 96, Batch 30, Loss: 0.020338086411356926\n",
      "Epoch 96, Batch 40, Loss: 0.0295607540756464\n",
      "Epoch 96, Batch 50, Loss: 0.02007988467812538\n",
      "Epoch 96, Batch 60, Loss: 0.022944141179323196\n",
      "Epoch 96, Batch 70, Loss: 0.02366519533097744\n",
      "Epoch 96, Batch 80, Loss: 0.025659121572971344\n",
      "Epoch 96, Batch 90, Loss: 0.02419142611324787\n",
      "Epoch 96, Batch 100, Loss: 0.019319919869303703\n",
      "Epoch 97, Batch 0, Loss: 0.024017460644245148\n",
      "Epoch 97, Batch 10, Loss: 0.016169216483831406\n",
      "Epoch 97, Batch 20, Loss: 0.024119561538100243\n",
      "Epoch 97, Batch 30, Loss: 0.016374778002500534\n",
      "Epoch 97, Batch 40, Loss: 0.023427853360772133\n",
      "Epoch 97, Batch 50, Loss: 0.025067616254091263\n",
      "Epoch 97, Batch 60, Loss: 0.028250493109226227\n",
      "Epoch 97, Batch 70, Loss: 0.01786160282790661\n",
      "Epoch 97, Batch 80, Loss: 0.025653529912233353\n",
      "Epoch 97, Batch 90, Loss: 0.023370906710624695\n",
      "Epoch 97, Batch 100, Loss: 0.02230650931596756\n",
      "Epoch 98, Batch 0, Loss: 0.015895478427410126\n",
      "Epoch 98, Batch 10, Loss: 0.0213871318846941\n",
      "Epoch 98, Batch 20, Loss: 0.027464214712381363\n",
      "Epoch 98, Batch 30, Loss: 0.030726347118616104\n",
      "Epoch 98, Batch 40, Loss: 0.02130592241883278\n",
      "Epoch 98, Batch 50, Loss: 0.015142296440899372\n",
      "Epoch 98, Batch 60, Loss: 0.021797602996230125\n",
      "Epoch 98, Batch 70, Loss: 0.019522178918123245\n",
      "Epoch 98, Batch 80, Loss: 0.020067734643816948\n",
      "Epoch 98, Batch 90, Loss: 0.023422447964549065\n",
      "Epoch 98, Batch 100, Loss: 0.01700681261718273\n",
      "Epoch 99, Batch 0, Loss: 0.017613407224416733\n",
      "Epoch 99, Batch 10, Loss: 0.016803139820694923\n",
      "Epoch 99, Batch 20, Loss: 0.025458987802267075\n",
      "Epoch 99, Batch 30, Loss: 0.01758693903684616\n",
      "Epoch 99, Batch 40, Loss: 0.02043922059237957\n",
      "Epoch 99, Batch 50, Loss: 0.01979108527302742\n",
      "Epoch 99, Batch 60, Loss: 0.021935977041721344\n",
      "Epoch 99, Batch 70, Loss: 0.02359023690223694\n",
      "Epoch 99, Batch 80, Loss: 0.02660478837788105\n",
      "Epoch 99, Batch 90, Loss: 0.02318090945482254\n",
      "Epoch 99, Batch 100, Loss: 0.019675495103001595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_LSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True)\n",
       "    (fc1): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.040634; Test RMSE 43.239197\n",
      "\n",
      "Train  MAE: 0.022905; Test  MAE 31.198692\n",
      "\n",
      "Train  R^2: 0.998357; Test  R^2 0.959767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BILSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_BILSTM, self).__init__()\n",
    "        self.cnn = CNN()\n",
    "        self.bilstm = BILSTM(input_size=32)  # Assuming the CNN output has 32 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  #[64, 10, 8]\n",
    "        # CNN expects the channels in the second dimension\n",
    "        x = x.permute(0, 2, 1)  #[64, 8, 10]\n",
    "        x = self.cnn(x)\n",
    "        # Permute back to (batch, seq_len, features) for the LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Reshape x to fit BiLSTM input requirements\n",
    "        # x = x.contiguous().view(batch_size, seq_len, -1)\n",
    "        x = self.bilstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "batch_size = 64 #? None?\n",
    "time_steps = 10\n",
    "\n",
    "\n",
    "# input_size = 8\n",
    "# lstm_num_layers = 1\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_dropout = 0.2\n",
    "# fc1_output_size = 16\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 100\n",
    "\n",
    "model = CNN_BILSTM().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss() # Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.7793314456939697\n",
      "Epoch 0, Batch 10, Loss: 0.7929213643074036\n",
      "Epoch 0, Batch 20, Loss: 0.8718426823616028\n",
      "Epoch 0, Batch 30, Loss: 0.8045697808265686\n",
      "Epoch 0, Batch 40, Loss: 0.7431604862213135\n",
      "Epoch 0, Batch 50, Loss: 0.6652891039848328\n",
      "Epoch 0, Batch 60, Loss: 0.6235947608947754\n",
      "Epoch 0, Batch 70, Loss: 0.6872910261154175\n",
      "Epoch 0, Batch 80, Loss: 0.714838981628418\n",
      "Epoch 0, Batch 90, Loss: 0.6941305994987488\n",
      "Epoch 0, Batch 100, Loss: 0.4842958450317383\n",
      "Epoch 1, Batch 0, Loss: 0.6399528980255127\n",
      "Epoch 1, Batch 10, Loss: 0.44477778673171997\n",
      "Epoch 1, Batch 20, Loss: 0.5268522500991821\n",
      "Epoch 1, Batch 30, Loss: 0.5145187377929688\n",
      "Epoch 1, Batch 40, Loss: 0.45475220680236816\n",
      "Epoch 1, Batch 50, Loss: 0.2933398187160492\n",
      "Epoch 1, Batch 60, Loss: 0.23742803931236267\n",
      "Epoch 1, Batch 70, Loss: 0.20982234179973602\n",
      "Epoch 1, Batch 80, Loss: 0.29928478598594666\n",
      "Epoch 1, Batch 90, Loss: 0.19146794080734253\n",
      "Epoch 1, Batch 100, Loss: 0.18411894142627716\n",
      "Epoch 2, Batch 0, Loss: 0.16804085671901703\n",
      "Epoch 2, Batch 10, Loss: 0.1351923942565918\n",
      "Epoch 2, Batch 20, Loss: 0.11645331233739853\n",
      "Epoch 2, Batch 30, Loss: 0.15278899669647217\n",
      "Epoch 2, Batch 40, Loss: 0.18028023838996887\n",
      "Epoch 2, Batch 50, Loss: 0.07243607193231583\n",
      "Epoch 2, Batch 60, Loss: 0.14095069468021393\n",
      "Epoch 2, Batch 70, Loss: 0.05766554921865463\n",
      "Epoch 2, Batch 80, Loss: 0.06708915531635284\n",
      "Epoch 2, Batch 90, Loss: 0.11089282482862473\n",
      "Epoch 2, Batch 100, Loss: 0.08897711336612701\n",
      "Epoch 3, Batch 0, Loss: 0.04540739953517914\n",
      "Epoch 3, Batch 10, Loss: 0.06441890448331833\n",
      "Epoch 3, Batch 20, Loss: 0.07569897919893265\n",
      "Epoch 3, Batch 30, Loss: 0.08311060070991516\n",
      "Epoch 3, Batch 40, Loss: 0.07526826858520508\n",
      "Epoch 3, Batch 50, Loss: 0.08013992756605148\n",
      "Epoch 3, Batch 60, Loss: 0.08212719112634659\n",
      "Epoch 3, Batch 70, Loss: 0.04646690934896469\n",
      "Epoch 3, Batch 80, Loss: 0.04519565403461456\n",
      "Epoch 3, Batch 90, Loss: 0.0359247587621212\n",
      "Epoch 3, Batch 100, Loss: 0.04232970252633095\n",
      "Epoch 4, Batch 0, Loss: 0.05046295374631882\n",
      "Epoch 4, Batch 10, Loss: 0.03373434767127037\n",
      "Epoch 4, Batch 20, Loss: 0.06960280984640121\n",
      "Epoch 4, Batch 30, Loss: 0.041997894644737244\n",
      "Epoch 4, Batch 40, Loss: 0.05933060124516487\n",
      "Epoch 4, Batch 50, Loss: 0.05854172259569168\n",
      "Epoch 4, Batch 60, Loss: 0.06973224878311157\n",
      "Epoch 4, Batch 70, Loss: 0.0366474911570549\n",
      "Epoch 4, Batch 80, Loss: 0.05351525917649269\n",
      "Epoch 4, Batch 90, Loss: 0.05122668668627739\n",
      "Epoch 4, Batch 100, Loss: 0.02799363061785698\n",
      "Epoch 5, Batch 0, Loss: 0.04317568987607956\n",
      "Epoch 5, Batch 10, Loss: 0.048338379710912704\n",
      "Epoch 5, Batch 20, Loss: 0.04876010864973068\n",
      "Epoch 5, Batch 30, Loss: 0.04297206178307533\n",
      "Epoch 5, Batch 40, Loss: 0.03829526528716087\n",
      "Epoch 5, Batch 50, Loss: 0.04355379194021225\n",
      "Epoch 5, Batch 60, Loss: 0.04552600532770157\n",
      "Epoch 5, Batch 70, Loss: 0.04626120626926422\n",
      "Epoch 5, Batch 80, Loss: 0.045366186648607254\n",
      "Epoch 5, Batch 90, Loss: 0.037184394896030426\n",
      "Epoch 5, Batch 100, Loss: 0.04167916625738144\n",
      "Epoch 6, Batch 0, Loss: 0.05956101417541504\n",
      "Epoch 6, Batch 10, Loss: 0.028701361268758774\n",
      "Epoch 6, Batch 20, Loss: 0.04081428796052933\n",
      "Epoch 6, Batch 30, Loss: 0.04533906653523445\n",
      "Epoch 6, Batch 40, Loss: 0.05610502511262894\n",
      "Epoch 6, Batch 50, Loss: 0.04273989796638489\n",
      "Epoch 6, Batch 60, Loss: 0.04078671336174011\n",
      "Epoch 6, Batch 70, Loss: 0.040204040706157684\n",
      "Epoch 6, Batch 80, Loss: 0.03313493728637695\n",
      "Epoch 6, Batch 90, Loss: 0.05787098407745361\n",
      "Epoch 6, Batch 100, Loss: 0.04783064126968384\n",
      "Epoch 7, Batch 0, Loss: 0.032707393169403076\n",
      "Epoch 7, Batch 10, Loss: 0.04428477957844734\n",
      "Epoch 7, Batch 20, Loss: 0.03766833245754242\n",
      "Epoch 7, Batch 30, Loss: 0.06038527563214302\n",
      "Epoch 7, Batch 40, Loss: 0.04820942133665085\n",
      "Epoch 7, Batch 50, Loss: 0.038279276341199875\n",
      "Epoch 7, Batch 60, Loss: 0.03518955036997795\n",
      "Epoch 7, Batch 70, Loss: 0.03893786296248436\n",
      "Epoch 7, Batch 80, Loss: 0.028474336490035057\n",
      "Epoch 7, Batch 90, Loss: 0.030497796833515167\n",
      "Epoch 7, Batch 100, Loss: 0.033180322498083115\n",
      "Epoch 8, Batch 0, Loss: 0.034444667398929596\n",
      "Epoch 8, Batch 10, Loss: 0.04622597247362137\n",
      "Epoch 8, Batch 20, Loss: 0.03635333478450775\n",
      "Epoch 8, Batch 30, Loss: 0.03247320279479027\n",
      "Epoch 8, Batch 40, Loss: 0.03787340223789215\n",
      "Epoch 8, Batch 50, Loss: 0.03446429595351219\n",
      "Epoch 8, Batch 60, Loss: 0.03016636148095131\n",
      "Epoch 8, Batch 70, Loss: 0.03132955729961395\n",
      "Epoch 8, Batch 80, Loss: 0.03777293860912323\n",
      "Epoch 8, Batch 90, Loss: 0.03498733043670654\n",
      "Epoch 8, Batch 100, Loss: 0.02886909805238247\n",
      "Epoch 9, Batch 0, Loss: 0.027335407212376595\n",
      "Epoch 9, Batch 10, Loss: 0.03880196437239647\n",
      "Epoch 9, Batch 20, Loss: 0.020771214738488197\n",
      "Epoch 9, Batch 30, Loss: 0.03281416371464729\n",
      "Epoch 9, Batch 40, Loss: 0.026236997917294502\n",
      "Epoch 9, Batch 50, Loss: 0.02525242231786251\n",
      "Epoch 9, Batch 60, Loss: 0.048238370567560196\n",
      "Epoch 9, Batch 70, Loss: 0.026040606200695038\n",
      "Epoch 9, Batch 80, Loss: 0.03184385970234871\n",
      "Epoch 9, Batch 90, Loss: 0.04118786007165909\n",
      "Epoch 9, Batch 100, Loss: 0.03841815143823624\n",
      "Epoch 10, Batch 0, Loss: 0.01858111470937729\n",
      "Epoch 10, Batch 10, Loss: 0.02811211720108986\n",
      "Epoch 10, Batch 20, Loss: 0.038462452590465546\n",
      "Epoch 10, Batch 30, Loss: 0.027169153094291687\n",
      "Epoch 10, Batch 40, Loss: 0.02375160902738571\n",
      "Epoch 10, Batch 50, Loss: 0.02635837532579899\n",
      "Epoch 10, Batch 60, Loss: 0.027714218944311142\n",
      "Epoch 10, Batch 70, Loss: 0.03502907231450081\n",
      "Epoch 10, Batch 80, Loss: 0.028396259993314743\n",
      "Epoch 10, Batch 90, Loss: 0.02782137505710125\n",
      "Epoch 10, Batch 100, Loss: 0.027667902410030365\n",
      "Epoch 11, Batch 0, Loss: 0.03588550537824631\n",
      "Epoch 11, Batch 10, Loss: 0.03743733838200569\n",
      "Epoch 11, Batch 20, Loss: 0.022733982652425766\n",
      "Epoch 11, Batch 30, Loss: 0.030192047357559204\n",
      "Epoch 11, Batch 40, Loss: 0.02568155899643898\n",
      "Epoch 11, Batch 50, Loss: 0.04334138333797455\n",
      "Epoch 11, Batch 60, Loss: 0.03822430223226547\n",
      "Epoch 11, Batch 70, Loss: 0.04021690785884857\n",
      "Epoch 11, Batch 80, Loss: 0.026151327416300774\n",
      "Epoch 11, Batch 90, Loss: 0.0349443294107914\n",
      "Epoch 11, Batch 100, Loss: 0.027295149862766266\n",
      "Epoch 12, Batch 0, Loss: 0.022404147312045097\n",
      "Epoch 12, Batch 10, Loss: 0.04230103641748428\n",
      "Epoch 12, Batch 20, Loss: 0.026902424171566963\n",
      "Epoch 12, Batch 30, Loss: 0.02789914794266224\n",
      "Epoch 12, Batch 40, Loss: 0.028104640543460846\n",
      "Epoch 12, Batch 50, Loss: 0.029362443834543228\n",
      "Epoch 12, Batch 60, Loss: 0.02747257798910141\n",
      "Epoch 12, Batch 70, Loss: 0.035997506231069565\n",
      "Epoch 12, Batch 80, Loss: 0.03393526002764702\n",
      "Epoch 12, Batch 90, Loss: 0.02950574830174446\n",
      "Epoch 12, Batch 100, Loss: 0.017088811844587326\n",
      "Epoch 13, Batch 0, Loss: 0.025403983891010284\n",
      "Epoch 13, Batch 10, Loss: 0.024050554260611534\n",
      "Epoch 13, Batch 20, Loss: 0.025398610159754753\n",
      "Epoch 13, Batch 30, Loss: 0.03392805531620979\n",
      "Epoch 13, Batch 40, Loss: 0.03036859817802906\n",
      "Epoch 13, Batch 50, Loss: 0.022494807839393616\n",
      "Epoch 13, Batch 60, Loss: 0.03687898814678192\n",
      "Epoch 13, Batch 70, Loss: 0.03062237612903118\n",
      "Epoch 13, Batch 80, Loss: 0.02839525230228901\n",
      "Epoch 13, Batch 90, Loss: 0.030013862997293472\n",
      "Epoch 13, Batch 100, Loss: 0.02539750747382641\n",
      "Epoch 14, Batch 0, Loss: 0.03694227337837219\n",
      "Epoch 14, Batch 10, Loss: 0.025906486436724663\n",
      "Epoch 14, Batch 20, Loss: 0.032380640506744385\n",
      "Epoch 14, Batch 30, Loss: 0.03134055435657501\n",
      "Epoch 14, Batch 40, Loss: 0.03018222376704216\n",
      "Epoch 14, Batch 50, Loss: 0.031170789152383804\n",
      "Epoch 14, Batch 60, Loss: 0.0269736647605896\n",
      "Epoch 14, Batch 70, Loss: 0.02907838299870491\n",
      "Epoch 14, Batch 80, Loss: 0.03372430056333542\n",
      "Epoch 14, Batch 90, Loss: 0.02549055404961109\n",
      "Epoch 14, Batch 100, Loss: 0.027901992201805115\n",
      "Epoch 15, Batch 0, Loss: 0.03068818897008896\n",
      "Epoch 15, Batch 10, Loss: 0.020218513906002045\n",
      "Epoch 15, Batch 20, Loss: 0.023539002984762192\n",
      "Epoch 15, Batch 30, Loss: 0.021932832896709442\n",
      "Epoch 15, Batch 40, Loss: 0.02680838480591774\n",
      "Epoch 15, Batch 50, Loss: 0.02649552933871746\n",
      "Epoch 15, Batch 60, Loss: 0.03034083917737007\n",
      "Epoch 15, Batch 70, Loss: 0.03305075690150261\n",
      "Epoch 15, Batch 80, Loss: 0.02677908167243004\n",
      "Epoch 15, Batch 90, Loss: 0.025558143854141235\n",
      "Epoch 15, Batch 100, Loss: 0.038428887724876404\n",
      "Epoch 16, Batch 0, Loss: 0.02274475246667862\n",
      "Epoch 16, Batch 10, Loss: 0.0257689468562603\n",
      "Epoch 16, Batch 20, Loss: 0.025529853999614716\n",
      "Epoch 16, Batch 30, Loss: 0.024780845269560814\n",
      "Epoch 16, Batch 40, Loss: 0.027279650792479515\n",
      "Epoch 16, Batch 50, Loss: 0.033785343170166016\n",
      "Epoch 16, Batch 60, Loss: 0.03237520530819893\n",
      "Epoch 16, Batch 70, Loss: 0.030844779685139656\n",
      "Epoch 16, Batch 80, Loss: 0.03015706315636635\n",
      "Epoch 16, Batch 90, Loss: 0.018760457634925842\n",
      "Epoch 16, Batch 100, Loss: 0.027307504788041115\n",
      "Epoch 17, Batch 0, Loss: 0.024384401738643646\n",
      "Epoch 17, Batch 10, Loss: 0.02791300043463707\n",
      "Epoch 17, Batch 20, Loss: 0.036925964057445526\n",
      "Epoch 17, Batch 30, Loss: 0.02961081638932228\n",
      "Epoch 17, Batch 40, Loss: 0.018667669966816902\n",
      "Epoch 17, Batch 50, Loss: 0.026040272787213326\n",
      "Epoch 17, Batch 60, Loss: 0.03518025949597359\n",
      "Epoch 17, Batch 70, Loss: 0.019165175035595894\n",
      "Epoch 17, Batch 80, Loss: 0.033659256994724274\n",
      "Epoch 17, Batch 90, Loss: 0.027423353865742683\n",
      "Epoch 17, Batch 100, Loss: 0.023434631526470184\n",
      "Epoch 18, Batch 0, Loss: 0.027794407680630684\n",
      "Epoch 18, Batch 10, Loss: 0.031272877007722855\n",
      "Epoch 18, Batch 20, Loss: 0.03910481184720993\n",
      "Epoch 18, Batch 30, Loss: 0.02895287796854973\n",
      "Epoch 18, Batch 40, Loss: 0.02584647573530674\n",
      "Epoch 18, Batch 50, Loss: 0.02973497286438942\n",
      "Epoch 18, Batch 60, Loss: 0.017842404544353485\n",
      "Epoch 18, Batch 70, Loss: 0.024315010756254196\n",
      "Epoch 18, Batch 80, Loss: 0.025584019720554352\n",
      "Epoch 18, Batch 90, Loss: 0.02262643165886402\n",
      "Epoch 18, Batch 100, Loss: 0.022084256634116173\n",
      "Epoch 19, Batch 0, Loss: 0.027168704196810722\n",
      "Epoch 19, Batch 10, Loss: 0.0222935751080513\n",
      "Epoch 19, Batch 20, Loss: 0.022329481318593025\n",
      "Epoch 19, Batch 30, Loss: 0.02434557117521763\n",
      "Epoch 19, Batch 40, Loss: 0.020855236798524857\n",
      "Epoch 19, Batch 50, Loss: 0.025691967457532883\n",
      "Epoch 19, Batch 60, Loss: 0.03619583323597908\n",
      "Epoch 19, Batch 70, Loss: 0.025654707103967667\n",
      "Epoch 19, Batch 80, Loss: 0.026343543082475662\n",
      "Epoch 19, Batch 90, Loss: 0.025255519896745682\n",
      "Epoch 19, Batch 100, Loss: 0.028257163241505623\n",
      "Epoch 20, Batch 0, Loss: 0.026460370048880577\n",
      "Epoch 20, Batch 10, Loss: 0.02292863093316555\n",
      "Epoch 20, Batch 20, Loss: 0.023875676095485687\n",
      "Epoch 20, Batch 30, Loss: 0.03214571252465248\n",
      "Epoch 20, Batch 40, Loss: 0.029447510838508606\n",
      "Epoch 20, Batch 50, Loss: 0.03165893256664276\n",
      "Epoch 20, Batch 60, Loss: 0.01564868539571762\n",
      "Epoch 20, Batch 70, Loss: 0.024583183228969574\n",
      "Epoch 20, Batch 80, Loss: 0.0338679775595665\n",
      "Epoch 20, Batch 90, Loss: 0.02289874665439129\n",
      "Epoch 20, Batch 100, Loss: 0.021210139617323875\n",
      "Epoch 21, Batch 0, Loss: 0.027135629206895828\n",
      "Epoch 21, Batch 10, Loss: 0.032063886523246765\n",
      "Epoch 21, Batch 20, Loss: 0.04400419816374779\n",
      "Epoch 21, Batch 30, Loss: 0.02867431752383709\n",
      "Epoch 21, Batch 40, Loss: 0.02828562632203102\n",
      "Epoch 21, Batch 50, Loss: 0.03534037247300148\n",
      "Epoch 21, Batch 60, Loss: 0.02574482560157776\n",
      "Epoch 21, Batch 70, Loss: 0.024790624156594276\n",
      "Epoch 21, Batch 80, Loss: 0.02471325732767582\n",
      "Epoch 21, Batch 90, Loss: 0.031078657135367393\n",
      "Epoch 21, Batch 100, Loss: 0.02348214015364647\n",
      "Epoch 22, Batch 0, Loss: 0.020717501640319824\n",
      "Epoch 22, Batch 10, Loss: 0.02888529561460018\n",
      "Epoch 22, Batch 20, Loss: 0.0320686437189579\n",
      "Epoch 22, Batch 30, Loss: 0.03331378102302551\n",
      "Epoch 22, Batch 40, Loss: 0.024801436811685562\n",
      "Epoch 22, Batch 50, Loss: 0.018703926354646683\n",
      "Epoch 22, Batch 60, Loss: 0.030467933043837547\n",
      "Epoch 22, Batch 70, Loss: 0.02333778701722622\n",
      "Epoch 22, Batch 80, Loss: 0.02692246064543724\n",
      "Epoch 22, Batch 90, Loss: 0.027102654799818993\n",
      "Epoch 22, Batch 100, Loss: 0.03455398604273796\n",
      "Epoch 23, Batch 0, Loss: 0.025418557226657867\n",
      "Epoch 23, Batch 10, Loss: 0.024076776579022408\n",
      "Epoch 23, Batch 20, Loss: 0.029685411602258682\n",
      "Epoch 23, Batch 30, Loss: 0.02211594767868519\n",
      "Epoch 23, Batch 40, Loss: 0.023363614454865456\n",
      "Epoch 23, Batch 50, Loss: 0.023174764588475227\n",
      "Epoch 23, Batch 60, Loss: 0.027528144419193268\n",
      "Epoch 23, Batch 70, Loss: 0.028253110125660896\n",
      "Epoch 23, Batch 80, Loss: 0.02658950909972191\n",
      "Epoch 23, Batch 90, Loss: 0.0284119863063097\n",
      "Epoch 23, Batch 100, Loss: 0.02485477738082409\n",
      "Epoch 24, Batch 0, Loss: 0.023972099646925926\n",
      "Epoch 24, Batch 10, Loss: 0.03131931275129318\n",
      "Epoch 24, Batch 20, Loss: 0.029952272772789\n",
      "Epoch 24, Batch 30, Loss: 0.03182453662157059\n",
      "Epoch 24, Batch 40, Loss: 0.03162304311990738\n",
      "Epoch 24, Batch 50, Loss: 0.021596405655145645\n",
      "Epoch 24, Batch 60, Loss: 0.016873102635145187\n",
      "Epoch 24, Batch 70, Loss: 0.019409675151109695\n",
      "Epoch 24, Batch 80, Loss: 0.028387073427438736\n",
      "Epoch 24, Batch 90, Loss: 0.019045762717723846\n",
      "Epoch 24, Batch 100, Loss: 0.029318340122699738\n",
      "Epoch 25, Batch 0, Loss: 0.031156081706285477\n",
      "Epoch 25, Batch 10, Loss: 0.021309515461325645\n",
      "Epoch 25, Batch 20, Loss: 0.027368538081645966\n",
      "Epoch 25, Batch 30, Loss: 0.03015006147325039\n",
      "Epoch 25, Batch 40, Loss: 0.028940865769982338\n",
      "Epoch 25, Batch 50, Loss: 0.025043144822120667\n",
      "Epoch 25, Batch 60, Loss: 0.030976423993706703\n",
      "Epoch 25, Batch 70, Loss: 0.0233010184019804\n",
      "Epoch 25, Batch 80, Loss: 0.023071181029081345\n",
      "Epoch 25, Batch 90, Loss: 0.026525361463427544\n",
      "Epoch 25, Batch 100, Loss: 0.031744495034217834\n",
      "Epoch 26, Batch 0, Loss: 0.023758525028824806\n",
      "Epoch 26, Batch 10, Loss: 0.024417582899332047\n",
      "Epoch 26, Batch 20, Loss: 0.025259580463171005\n",
      "Epoch 26, Batch 30, Loss: 0.03037552535533905\n",
      "Epoch 26, Batch 40, Loss: 0.027528854086995125\n",
      "Epoch 26, Batch 50, Loss: 0.027112377807497978\n",
      "Epoch 26, Batch 60, Loss: 0.023677345365285873\n",
      "Epoch 26, Batch 70, Loss: 0.03136279806494713\n",
      "Epoch 26, Batch 80, Loss: 0.027536144480109215\n",
      "Epoch 26, Batch 90, Loss: 0.026886077597737312\n",
      "Epoch 26, Batch 100, Loss: 0.026371784508228302\n",
      "Epoch 27, Batch 0, Loss: 0.02622993104159832\n",
      "Epoch 27, Batch 10, Loss: 0.029445838183164597\n",
      "Epoch 27, Batch 20, Loss: 0.026361793279647827\n",
      "Epoch 27, Batch 30, Loss: 0.01987636275589466\n",
      "Epoch 27, Batch 40, Loss: 0.031174011528491974\n",
      "Epoch 27, Batch 50, Loss: 0.0308469720184803\n",
      "Epoch 27, Batch 60, Loss: 0.028680788353085518\n",
      "Epoch 27, Batch 70, Loss: 0.026962555944919586\n",
      "Epoch 27, Batch 80, Loss: 0.02432732656598091\n",
      "Epoch 27, Batch 90, Loss: 0.028302587568759918\n",
      "Epoch 27, Batch 100, Loss: 0.022100988775491714\n",
      "Epoch 28, Batch 0, Loss: 0.027648795396089554\n",
      "Epoch 28, Batch 10, Loss: 0.026562122628092766\n",
      "Epoch 28, Batch 20, Loss: 0.03308022394776344\n",
      "Epoch 28, Batch 30, Loss: 0.022717557847499847\n",
      "Epoch 28, Batch 40, Loss: 0.027588607743382454\n",
      "Epoch 28, Batch 50, Loss: 0.025131497532129288\n",
      "Epoch 28, Batch 60, Loss: 0.027693066745996475\n",
      "Epoch 28, Batch 70, Loss: 0.024722103029489517\n",
      "Epoch 28, Batch 80, Loss: 0.036226555705070496\n",
      "Epoch 28, Batch 90, Loss: 0.028298677876591682\n",
      "Epoch 28, Batch 100, Loss: 0.030627617612481117\n",
      "Epoch 29, Batch 0, Loss: 0.023378251120448112\n",
      "Epoch 29, Batch 10, Loss: 0.027682313695549965\n",
      "Epoch 29, Batch 20, Loss: 0.02437846176326275\n",
      "Epoch 29, Batch 30, Loss: 0.025199294090270996\n",
      "Epoch 29, Batch 40, Loss: 0.026155898347496986\n",
      "Epoch 29, Batch 50, Loss: 0.029327817261219025\n",
      "Epoch 29, Batch 60, Loss: 0.018886413425207138\n",
      "Epoch 29, Batch 70, Loss: 0.022397546097636223\n",
      "Epoch 29, Batch 80, Loss: 0.02728075161576271\n",
      "Epoch 29, Batch 90, Loss: 0.02919662743806839\n",
      "Epoch 29, Batch 100, Loss: 0.0308469720184803\n",
      "Epoch 30, Batch 0, Loss: 0.02092076651751995\n",
      "Epoch 30, Batch 10, Loss: 0.028010593727231026\n",
      "Epoch 30, Batch 20, Loss: 0.021698636934161186\n",
      "Epoch 30, Batch 30, Loss: 0.025753425434231758\n",
      "Epoch 30, Batch 40, Loss: 0.03125083073973656\n",
      "Epoch 30, Batch 50, Loss: 0.026353944092988968\n",
      "Epoch 30, Batch 60, Loss: 0.031687330454587936\n",
      "Epoch 30, Batch 70, Loss: 0.030355162918567657\n",
      "Epoch 30, Batch 80, Loss: 0.020393600687384605\n",
      "Epoch 30, Batch 90, Loss: 0.024875178933143616\n",
      "Epoch 30, Batch 100, Loss: 0.020658545196056366\n",
      "Epoch 31, Batch 0, Loss: 0.026038145646452904\n",
      "Epoch 31, Batch 10, Loss: 0.023588914424180984\n",
      "Epoch 31, Batch 20, Loss: 0.03564639762043953\n",
      "Epoch 31, Batch 30, Loss: 0.025313865393400192\n",
      "Epoch 31, Batch 40, Loss: 0.021242747083306313\n",
      "Epoch 31, Batch 50, Loss: 0.03160089999437332\n",
      "Epoch 31, Batch 60, Loss: 0.02772083505988121\n",
      "Epoch 31, Batch 70, Loss: 0.02883889712393284\n",
      "Epoch 31, Batch 80, Loss: 0.035829849541187286\n",
      "Epoch 31, Batch 90, Loss: 0.03150084614753723\n",
      "Epoch 31, Batch 100, Loss: 0.029442911967635155\n",
      "Epoch 32, Batch 0, Loss: 0.03903381526470184\n",
      "Epoch 32, Batch 10, Loss: 0.02669491432607174\n",
      "Epoch 32, Batch 20, Loss: 0.02523954212665558\n",
      "Epoch 32, Batch 30, Loss: 0.024627549573779106\n",
      "Epoch 32, Batch 40, Loss: 0.023038726300001144\n",
      "Epoch 32, Batch 50, Loss: 0.030914517119526863\n",
      "Epoch 32, Batch 60, Loss: 0.027038250118494034\n",
      "Epoch 32, Batch 70, Loss: 0.02756962552666664\n",
      "Epoch 32, Batch 80, Loss: 0.022520771250128746\n",
      "Epoch 32, Batch 90, Loss: 0.025199156254529953\n",
      "Epoch 32, Batch 100, Loss: 0.024038713425397873\n",
      "Epoch 33, Batch 0, Loss: 0.033601582050323486\n",
      "Epoch 33, Batch 10, Loss: 0.02384699508547783\n",
      "Epoch 33, Batch 20, Loss: 0.029186587780714035\n",
      "Epoch 33, Batch 30, Loss: 0.023160891607403755\n",
      "Epoch 33, Batch 40, Loss: 0.022787941619753838\n",
      "Epoch 33, Batch 50, Loss: 0.021765567362308502\n",
      "Epoch 33, Batch 60, Loss: 0.029574129730463028\n",
      "Epoch 33, Batch 70, Loss: 0.03092038258910179\n",
      "Epoch 33, Batch 80, Loss: 0.030005766078829765\n",
      "Epoch 33, Batch 90, Loss: 0.02381892316043377\n",
      "Epoch 33, Batch 100, Loss: 0.025418182834982872\n",
      "Epoch 34, Batch 0, Loss: 0.03071017377078533\n",
      "Epoch 34, Batch 10, Loss: 0.02098829485476017\n",
      "Epoch 34, Batch 20, Loss: 0.020746003836393356\n",
      "Epoch 34, Batch 30, Loss: 0.022970860823988914\n",
      "Epoch 34, Batch 40, Loss: 0.022826828062534332\n",
      "Epoch 34, Batch 50, Loss: 0.01699213683605194\n",
      "Epoch 34, Batch 60, Loss: 0.02320200577378273\n",
      "Epoch 34, Batch 70, Loss: 0.026181736961007118\n",
      "Epoch 34, Batch 80, Loss: 0.02884596586227417\n",
      "Epoch 34, Batch 90, Loss: 0.020710572600364685\n",
      "Epoch 34, Batch 100, Loss: 0.029983779415488243\n",
      "Epoch 35, Batch 0, Loss: 0.02561240643262863\n",
      "Epoch 35, Batch 10, Loss: 0.019421163946390152\n",
      "Epoch 35, Batch 20, Loss: 0.02276490069925785\n",
      "Epoch 35, Batch 30, Loss: 0.024799969047307968\n",
      "Epoch 35, Batch 40, Loss: 0.019084881991147995\n",
      "Epoch 35, Batch 50, Loss: 0.02443194016814232\n",
      "Epoch 35, Batch 60, Loss: 0.02538307011127472\n",
      "Epoch 35, Batch 70, Loss: 0.029007302597165108\n",
      "Epoch 35, Batch 80, Loss: 0.026808351278305054\n",
      "Epoch 35, Batch 90, Loss: 0.022464491426944733\n",
      "Epoch 35, Batch 100, Loss: 0.027327945455908775\n",
      "Epoch 36, Batch 0, Loss: 0.01857778988778591\n",
      "Epoch 36, Batch 10, Loss: 0.02012653462588787\n",
      "Epoch 36, Batch 20, Loss: 0.026447083801031113\n",
      "Epoch 36, Batch 30, Loss: 0.028866242617368698\n",
      "Epoch 36, Batch 40, Loss: 0.02601572312414646\n",
      "Epoch 36, Batch 50, Loss: 0.029268428683280945\n",
      "Epoch 36, Batch 60, Loss: 0.02452862076461315\n",
      "Epoch 36, Batch 70, Loss: 0.02789265476167202\n",
      "Epoch 36, Batch 80, Loss: 0.025830170139670372\n",
      "Epoch 36, Batch 90, Loss: 0.029955586418509483\n",
      "Epoch 36, Batch 100, Loss: 0.020525719970464706\n",
      "Epoch 37, Batch 0, Loss: 0.022862819954752922\n",
      "Epoch 37, Batch 10, Loss: 0.021682986989617348\n",
      "Epoch 37, Batch 20, Loss: 0.02224617637693882\n",
      "Epoch 37, Batch 30, Loss: 0.02377302013337612\n",
      "Epoch 37, Batch 40, Loss: 0.026261163875460625\n",
      "Epoch 37, Batch 50, Loss: 0.02950877696275711\n",
      "Epoch 37, Batch 60, Loss: 0.02384677529335022\n",
      "Epoch 37, Batch 70, Loss: 0.031615741550922394\n",
      "Epoch 37, Batch 80, Loss: 0.025211039930582047\n",
      "Epoch 37, Batch 90, Loss: 0.025740083307027817\n",
      "Epoch 37, Batch 100, Loss: 0.026479756459593773\n",
      "Epoch 38, Batch 0, Loss: 0.01715472713112831\n",
      "Epoch 38, Batch 10, Loss: 0.03146718069911003\n",
      "Epoch 38, Batch 20, Loss: 0.029858339577913284\n",
      "Epoch 38, Batch 30, Loss: 0.02655630186200142\n",
      "Epoch 38, Batch 40, Loss: 0.020134510472416878\n",
      "Epoch 38, Batch 50, Loss: 0.02347463183104992\n",
      "Epoch 38, Batch 60, Loss: 0.022829031571745872\n",
      "Epoch 38, Batch 70, Loss: 0.02927231416106224\n",
      "Epoch 38, Batch 80, Loss: 0.018932154402136803\n",
      "Epoch 38, Batch 90, Loss: 0.030635327100753784\n",
      "Epoch 38, Batch 100, Loss: 0.023030119016766548\n",
      "Epoch 39, Batch 0, Loss: 0.03018602728843689\n",
      "Epoch 39, Batch 10, Loss: 0.026371916756033897\n",
      "Epoch 39, Batch 20, Loss: 0.02440425008535385\n",
      "Epoch 39, Batch 30, Loss: 0.024362564086914062\n",
      "Epoch 39, Batch 40, Loss: 0.019890105351805687\n",
      "Epoch 39, Batch 50, Loss: 0.023202700540423393\n",
      "Epoch 39, Batch 60, Loss: 0.017658637836575508\n",
      "Epoch 39, Batch 70, Loss: 0.0341840460896492\n",
      "Epoch 39, Batch 80, Loss: 0.025751594454050064\n",
      "Epoch 39, Batch 90, Loss: 0.02525615692138672\n",
      "Epoch 39, Batch 100, Loss: 0.029672961682081223\n",
      "Epoch 40, Batch 0, Loss: 0.02009756676852703\n",
      "Epoch 40, Batch 10, Loss: 0.01971328631043434\n",
      "Epoch 40, Batch 20, Loss: 0.02892739325761795\n",
      "Epoch 40, Batch 30, Loss: 0.023116841912269592\n",
      "Epoch 40, Batch 40, Loss: 0.03412993624806404\n",
      "Epoch 40, Batch 50, Loss: 0.021779395639896393\n",
      "Epoch 40, Batch 60, Loss: 0.029118141159415245\n",
      "Epoch 40, Batch 70, Loss: 0.024156760424375534\n",
      "Epoch 40, Batch 80, Loss: 0.018790850415825844\n",
      "Epoch 40, Batch 90, Loss: 0.02111331932246685\n",
      "Epoch 40, Batch 100, Loss: 0.021660374477505684\n",
      "Epoch 41, Batch 0, Loss: 0.01966160535812378\n",
      "Epoch 41, Batch 10, Loss: 0.027151811867952347\n",
      "Epoch 41, Batch 20, Loss: 0.029978808015584946\n",
      "Epoch 41, Batch 30, Loss: 0.02645116299390793\n",
      "Epoch 41, Batch 40, Loss: 0.022212058305740356\n",
      "Epoch 41, Batch 50, Loss: 0.026222866028547287\n",
      "Epoch 41, Batch 60, Loss: 0.026790505275130272\n",
      "Epoch 41, Batch 70, Loss: 0.024082237854599953\n",
      "Epoch 41, Batch 80, Loss: 0.03102058544754982\n",
      "Epoch 41, Batch 90, Loss: 0.02545086294412613\n",
      "Epoch 41, Batch 100, Loss: 0.021303247660398483\n",
      "Epoch 42, Batch 0, Loss: 0.02049834653735161\n",
      "Epoch 42, Batch 10, Loss: 0.026602989062666893\n",
      "Epoch 42, Batch 20, Loss: 0.026666875928640366\n",
      "Epoch 42, Batch 30, Loss: 0.02820969931781292\n",
      "Epoch 42, Batch 40, Loss: 0.030236078426241875\n",
      "Epoch 42, Batch 50, Loss: 0.026005297899246216\n",
      "Epoch 42, Batch 60, Loss: 0.0264614075422287\n",
      "Epoch 42, Batch 70, Loss: 0.025232696905732155\n",
      "Epoch 42, Batch 80, Loss: 0.027963880449533463\n",
      "Epoch 42, Batch 90, Loss: 0.032508984208106995\n",
      "Epoch 42, Batch 100, Loss: 0.02578289993107319\n",
      "Epoch 43, Batch 0, Loss: 0.02903282642364502\n",
      "Epoch 43, Batch 10, Loss: 0.021328896284103394\n",
      "Epoch 43, Batch 20, Loss: 0.026245389133691788\n",
      "Epoch 43, Batch 30, Loss: 0.02659701555967331\n",
      "Epoch 43, Batch 40, Loss: 0.029742054641246796\n",
      "Epoch 43, Batch 50, Loss: 0.03142716735601425\n",
      "Epoch 43, Batch 60, Loss: 0.024723347276449203\n",
      "Epoch 43, Batch 70, Loss: 0.01936573162674904\n",
      "Epoch 43, Batch 80, Loss: 0.022733349353075027\n",
      "Epoch 43, Batch 90, Loss: 0.022639956325292587\n",
      "Epoch 43, Batch 100, Loss: 0.029014984145760536\n",
      "Epoch 44, Batch 0, Loss: 0.025447893887758255\n",
      "Epoch 44, Batch 10, Loss: 0.021075960248708725\n",
      "Epoch 44, Batch 20, Loss: 0.02247558906674385\n",
      "Epoch 44, Batch 30, Loss: 0.026964280754327774\n",
      "Epoch 44, Batch 40, Loss: 0.01781616359949112\n",
      "Epoch 44, Batch 50, Loss: 0.03006652742624283\n",
      "Epoch 44, Batch 60, Loss: 0.02517564408481121\n",
      "Epoch 44, Batch 70, Loss: 0.026138294488191605\n",
      "Epoch 44, Batch 80, Loss: 0.03193189203739166\n",
      "Epoch 44, Batch 90, Loss: 0.03317642584443092\n",
      "Epoch 44, Batch 100, Loss: 0.02021818235516548\n",
      "Epoch 45, Batch 0, Loss: 0.022936346009373665\n",
      "Epoch 45, Batch 10, Loss: 0.021062184125185013\n",
      "Epoch 45, Batch 20, Loss: 0.02512241154909134\n",
      "Epoch 45, Batch 30, Loss: 0.02713187225162983\n",
      "Epoch 45, Batch 40, Loss: 0.020538805052638054\n",
      "Epoch 45, Batch 50, Loss: 0.02470671944320202\n",
      "Epoch 45, Batch 60, Loss: 0.019689228385686874\n",
      "Epoch 45, Batch 70, Loss: 0.022444818168878555\n",
      "Epoch 45, Batch 80, Loss: 0.027851527556777\n",
      "Epoch 45, Batch 90, Loss: 0.02536306343972683\n",
      "Epoch 45, Batch 100, Loss: 0.023990783840417862\n",
      "Epoch 46, Batch 0, Loss: 0.030185556039214134\n",
      "Epoch 46, Batch 10, Loss: 0.02569497562944889\n",
      "Epoch 46, Batch 20, Loss: 0.025127381086349487\n",
      "Epoch 46, Batch 30, Loss: 0.028367767110466957\n",
      "Epoch 46, Batch 40, Loss: 0.021965041756629944\n",
      "Epoch 46, Batch 50, Loss: 0.029711829498410225\n",
      "Epoch 46, Batch 60, Loss: 0.037067607045173645\n",
      "Epoch 46, Batch 70, Loss: 0.026286981999874115\n",
      "Epoch 46, Batch 80, Loss: 0.020283138379454613\n",
      "Epoch 46, Batch 90, Loss: 0.023415854200720787\n",
      "Epoch 46, Batch 100, Loss: 0.028130725026130676\n",
      "Epoch 47, Batch 0, Loss: 0.026873355731368065\n",
      "Epoch 47, Batch 10, Loss: 0.02329735830426216\n",
      "Epoch 47, Batch 20, Loss: 0.02998402900993824\n",
      "Epoch 47, Batch 30, Loss: 0.024494152516126633\n",
      "Epoch 47, Batch 40, Loss: 0.027722209692001343\n",
      "Epoch 47, Batch 50, Loss: 0.027672428637742996\n",
      "Epoch 47, Batch 60, Loss: 0.027155086398124695\n",
      "Epoch 47, Batch 70, Loss: 0.02638774737715721\n",
      "Epoch 47, Batch 80, Loss: 0.03293243795633316\n",
      "Epoch 47, Batch 90, Loss: 0.01557270810008049\n",
      "Epoch 47, Batch 100, Loss: 0.031771138310432434\n",
      "Epoch 48, Batch 0, Loss: 0.029720865190029144\n",
      "Epoch 48, Batch 10, Loss: 0.02632482536137104\n",
      "Epoch 48, Batch 20, Loss: 0.029728753492236137\n",
      "Epoch 48, Batch 30, Loss: 0.024716082960367203\n",
      "Epoch 48, Batch 40, Loss: 0.021707788109779358\n",
      "Epoch 48, Batch 50, Loss: 0.02170470915734768\n",
      "Epoch 48, Batch 60, Loss: 0.0300302691757679\n",
      "Epoch 48, Batch 70, Loss: 0.020118216052651405\n",
      "Epoch 48, Batch 80, Loss: 0.018242983147501945\n",
      "Epoch 48, Batch 90, Loss: 0.03075190633535385\n",
      "Epoch 48, Batch 100, Loss: 0.032403793185949326\n",
      "Epoch 49, Batch 0, Loss: 0.03752690181136131\n",
      "Epoch 49, Batch 10, Loss: 0.01710917055606842\n",
      "Epoch 49, Batch 20, Loss: 0.02096538059413433\n",
      "Epoch 49, Batch 30, Loss: 0.021968519315123558\n",
      "Epoch 49, Batch 40, Loss: 0.023018673062324524\n",
      "Epoch 49, Batch 50, Loss: 0.026145312935113907\n",
      "Epoch 49, Batch 60, Loss: 0.02522433176636696\n",
      "Epoch 49, Batch 70, Loss: 0.029866959899663925\n",
      "Epoch 49, Batch 80, Loss: 0.026907775551080704\n",
      "Epoch 49, Batch 90, Loss: 0.017867086455225945\n",
      "Epoch 49, Batch 100, Loss: 0.02268395759165287\n",
      "Epoch 50, Batch 0, Loss: 0.028346657752990723\n",
      "Epoch 50, Batch 10, Loss: 0.023472774773836136\n",
      "Epoch 50, Batch 20, Loss: 0.02645091339945793\n",
      "Epoch 50, Batch 30, Loss: 0.02088928408920765\n",
      "Epoch 50, Batch 40, Loss: 0.02799663133919239\n",
      "Epoch 50, Batch 50, Loss: 0.022280512377619743\n",
      "Epoch 50, Batch 60, Loss: 0.02727731503546238\n",
      "Epoch 50, Batch 70, Loss: 0.02215738222002983\n",
      "Epoch 50, Batch 80, Loss: 0.022718967869877815\n",
      "Epoch 50, Batch 90, Loss: 0.02445068396627903\n",
      "Epoch 50, Batch 100, Loss: 0.02523988112807274\n",
      "Epoch 51, Batch 0, Loss: 0.022308556362986565\n",
      "Epoch 51, Batch 10, Loss: 0.025797244161367416\n",
      "Epoch 51, Batch 20, Loss: 0.02240905351936817\n",
      "Epoch 51, Batch 30, Loss: 0.021539464592933655\n",
      "Epoch 51, Batch 40, Loss: 0.02277766913175583\n",
      "Epoch 51, Batch 50, Loss: 0.028575686737895012\n",
      "Epoch 51, Batch 60, Loss: 0.026886126026511192\n",
      "Epoch 51, Batch 70, Loss: 0.0211480762809515\n",
      "Epoch 51, Batch 80, Loss: 0.028741752728819847\n",
      "Epoch 51, Batch 90, Loss: 0.030455678701400757\n",
      "Epoch 51, Batch 100, Loss: 0.02023734524846077\n",
      "Epoch 52, Batch 0, Loss: 0.03205966204404831\n",
      "Epoch 52, Batch 10, Loss: 0.021165601909160614\n",
      "Epoch 52, Batch 20, Loss: 0.02999405935406685\n",
      "Epoch 52, Batch 30, Loss: 0.01958341896533966\n",
      "Epoch 52, Batch 40, Loss: 0.032795291393995285\n",
      "Epoch 52, Batch 50, Loss: 0.0332954078912735\n",
      "Epoch 52, Batch 60, Loss: 0.026302805170416832\n",
      "Epoch 52, Batch 70, Loss: 0.022294888272881508\n",
      "Epoch 52, Batch 80, Loss: 0.020796652883291245\n",
      "Epoch 52, Batch 90, Loss: 0.023314816877245903\n",
      "Epoch 52, Batch 100, Loss: 0.02308025397360325\n",
      "Epoch 53, Batch 0, Loss: 0.031491611152887344\n",
      "Epoch 53, Batch 10, Loss: 0.026678962633013725\n",
      "Epoch 53, Batch 20, Loss: 0.030157266184687614\n",
      "Epoch 53, Batch 30, Loss: 0.028330091387033463\n",
      "Epoch 53, Batch 40, Loss: 0.02733834832906723\n",
      "Epoch 53, Batch 50, Loss: 0.026394754648208618\n",
      "Epoch 53, Batch 60, Loss: 0.01778484508395195\n",
      "Epoch 53, Batch 70, Loss: 0.019022522494196892\n",
      "Epoch 53, Batch 80, Loss: 0.026397738605737686\n",
      "Epoch 53, Batch 90, Loss: 0.026642823591828346\n",
      "Epoch 53, Batch 100, Loss: 0.024629691615700722\n",
      "Epoch 54, Batch 0, Loss: 0.027772357687354088\n",
      "Epoch 54, Batch 10, Loss: 0.023867115378379822\n",
      "Epoch 54, Batch 20, Loss: 0.028061186894774437\n",
      "Epoch 54, Batch 30, Loss: 0.02920287661254406\n",
      "Epoch 54, Batch 40, Loss: 0.028177505359053612\n",
      "Epoch 54, Batch 50, Loss: 0.022154029458761215\n",
      "Epoch 54, Batch 60, Loss: 0.030151044949889183\n",
      "Epoch 54, Batch 70, Loss: 0.021155662834644318\n",
      "Epoch 54, Batch 80, Loss: 0.020994169637560844\n",
      "Epoch 54, Batch 90, Loss: 0.020896242931485176\n",
      "Epoch 54, Batch 100, Loss: 0.024105824530124664\n",
      "Epoch 55, Batch 0, Loss: 0.023910533636808395\n",
      "Epoch 55, Batch 10, Loss: 0.022576551884412766\n",
      "Epoch 55, Batch 20, Loss: 0.026882339268922806\n",
      "Epoch 55, Batch 30, Loss: 0.02171313762664795\n",
      "Epoch 55, Batch 40, Loss: 0.02344811148941517\n",
      "Epoch 55, Batch 50, Loss: 0.0253475122153759\n",
      "Epoch 55, Batch 60, Loss: 0.03009548783302307\n",
      "Epoch 55, Batch 70, Loss: 0.018911568447947502\n",
      "Epoch 55, Batch 80, Loss: 0.020159518346190453\n",
      "Epoch 55, Batch 90, Loss: 0.020942043513059616\n",
      "Epoch 55, Batch 100, Loss: 0.026281211525201797\n",
      "Epoch 56, Batch 0, Loss: 0.019584208726882935\n",
      "Epoch 56, Batch 10, Loss: 0.026500437408685684\n",
      "Epoch 56, Batch 20, Loss: 0.022359732538461685\n",
      "Epoch 56, Batch 30, Loss: 0.029675684869289398\n",
      "Epoch 56, Batch 40, Loss: 0.024651899933815002\n",
      "Epoch 56, Batch 50, Loss: 0.026798291131854057\n",
      "Epoch 56, Batch 60, Loss: 0.03185313940048218\n",
      "Epoch 56, Batch 70, Loss: 0.02503250166773796\n",
      "Epoch 56, Batch 80, Loss: 0.025214286521077156\n",
      "Epoch 56, Batch 90, Loss: 0.02516779489815235\n",
      "Epoch 56, Batch 100, Loss: 0.019581757485866547\n",
      "Epoch 57, Batch 0, Loss: 0.01704278215765953\n",
      "Epoch 57, Batch 10, Loss: 0.027777064591646194\n",
      "Epoch 57, Batch 20, Loss: 0.022012393921613693\n",
      "Epoch 57, Batch 30, Loss: 0.021241022273898125\n",
      "Epoch 57, Batch 40, Loss: 0.02196270041167736\n",
      "Epoch 57, Batch 50, Loss: 0.02927386201918125\n",
      "Epoch 57, Batch 60, Loss: 0.024819210171699524\n",
      "Epoch 57, Batch 70, Loss: 0.02722243033349514\n",
      "Epoch 57, Batch 80, Loss: 0.026145774871110916\n",
      "Epoch 57, Batch 90, Loss: 0.020988214761018753\n",
      "Epoch 57, Batch 100, Loss: 0.029764816164970398\n",
      "Epoch 58, Batch 0, Loss: 0.02957153506577015\n",
      "Epoch 58, Batch 10, Loss: 0.028353184461593628\n",
      "Epoch 58, Batch 20, Loss: 0.03236732259392738\n",
      "Epoch 58, Batch 30, Loss: 0.02066131681203842\n",
      "Epoch 58, Batch 40, Loss: 0.021215595304965973\n",
      "Epoch 58, Batch 50, Loss: 0.02748006395995617\n",
      "Epoch 58, Batch 60, Loss: 0.02616514451801777\n",
      "Epoch 58, Batch 70, Loss: 0.02150949463248253\n",
      "Epoch 58, Batch 80, Loss: 0.026501063257455826\n",
      "Epoch 58, Batch 90, Loss: 0.023078035563230515\n",
      "Epoch 58, Batch 100, Loss: 0.03392180800437927\n",
      "Epoch 59, Batch 0, Loss: 0.02019977755844593\n",
      "Epoch 59, Batch 10, Loss: 0.024264946579933167\n",
      "Epoch 59, Batch 20, Loss: 0.023863401263952255\n",
      "Epoch 59, Batch 30, Loss: 0.023524390533566475\n",
      "Epoch 59, Batch 40, Loss: 0.019865447655320168\n",
      "Epoch 59, Batch 50, Loss: 0.020988989621400833\n",
      "Epoch 59, Batch 60, Loss: 0.024173714220523834\n",
      "Epoch 59, Batch 70, Loss: 0.025549769401550293\n",
      "Epoch 59, Batch 80, Loss: 0.025355374440550804\n",
      "Epoch 59, Batch 90, Loss: 0.03412516415119171\n",
      "Epoch 59, Batch 100, Loss: 0.0284908264875412\n",
      "Epoch 60, Batch 0, Loss: 0.027598615735769272\n",
      "Epoch 60, Batch 10, Loss: 0.022678466513752937\n",
      "Epoch 60, Batch 20, Loss: 0.02574986033141613\n",
      "Epoch 60, Batch 30, Loss: 0.022815091535449028\n",
      "Epoch 60, Batch 40, Loss: 0.02693277597427368\n",
      "Epoch 60, Batch 50, Loss: 0.026568934321403503\n",
      "Epoch 60, Batch 60, Loss: 0.031222103163599968\n",
      "Epoch 60, Batch 70, Loss: 0.022399207577109337\n",
      "Epoch 60, Batch 80, Loss: 0.021117258816957474\n",
      "Epoch 60, Batch 90, Loss: 0.027482621371746063\n",
      "Epoch 60, Batch 100, Loss: 0.021699903532862663\n",
      "Epoch 61, Batch 0, Loss: 0.019912730902433395\n",
      "Epoch 61, Batch 10, Loss: 0.01632799208164215\n",
      "Epoch 61, Batch 20, Loss: 0.02013937383890152\n",
      "Epoch 61, Batch 30, Loss: 0.02078261598944664\n",
      "Epoch 61, Batch 40, Loss: 0.03075108304619789\n",
      "Epoch 61, Batch 50, Loss: 0.027663279324769974\n",
      "Epoch 61, Batch 60, Loss: 0.017762674018740654\n",
      "Epoch 61, Batch 70, Loss: 0.030666975304484367\n",
      "Epoch 61, Batch 80, Loss: 0.01936672069132328\n",
      "Epoch 61, Batch 90, Loss: 0.024369683116674423\n",
      "Epoch 61, Batch 100, Loss: 0.030180463567376137\n",
      "Epoch 62, Batch 0, Loss: 0.01719190739095211\n",
      "Epoch 62, Batch 10, Loss: 0.02272159978747368\n",
      "Epoch 62, Batch 20, Loss: 0.03252805024385452\n",
      "Epoch 62, Batch 30, Loss: 0.016955358907580376\n",
      "Epoch 62, Batch 40, Loss: 0.029200246557593346\n",
      "Epoch 62, Batch 50, Loss: 0.024549124762415886\n",
      "Epoch 62, Batch 60, Loss: 0.03148207440972328\n",
      "Epoch 62, Batch 70, Loss: 0.02145807445049286\n",
      "Epoch 62, Batch 80, Loss: 0.025458894670009613\n",
      "Epoch 62, Batch 90, Loss: 0.024526167660951614\n",
      "Epoch 62, Batch 100, Loss: 0.017575975507497787\n",
      "Epoch 63, Batch 0, Loss: 0.021157488226890564\n",
      "Epoch 63, Batch 10, Loss: 0.02044633962213993\n",
      "Epoch 63, Batch 20, Loss: 0.023691238835453987\n",
      "Epoch 63, Batch 30, Loss: 0.02195846475660801\n",
      "Epoch 63, Batch 40, Loss: 0.017343634739518166\n",
      "Epoch 63, Batch 50, Loss: 0.031212544068694115\n",
      "Epoch 63, Batch 60, Loss: 0.025142338126897812\n",
      "Epoch 63, Batch 70, Loss: 0.026870453730225563\n",
      "Epoch 63, Batch 80, Loss: 0.02941347286105156\n",
      "Epoch 63, Batch 90, Loss: 0.026272349059581757\n",
      "Epoch 63, Batch 100, Loss: 0.02679343894124031\n",
      "Epoch 64, Batch 0, Loss: 0.02825228124856949\n",
      "Epoch 64, Batch 10, Loss: 0.02134104073047638\n",
      "Epoch 64, Batch 20, Loss: 0.030922217294573784\n",
      "Epoch 64, Batch 30, Loss: 0.027211345732212067\n",
      "Epoch 64, Batch 40, Loss: 0.02589215710759163\n",
      "Epoch 64, Batch 50, Loss: 0.03176623582839966\n",
      "Epoch 64, Batch 60, Loss: 0.019564904272556305\n",
      "Epoch 64, Batch 70, Loss: 0.02390487678349018\n",
      "Epoch 64, Batch 80, Loss: 0.026273716241121292\n",
      "Epoch 64, Batch 90, Loss: 0.028729168698191643\n",
      "Epoch 64, Batch 100, Loss: 0.02100088633596897\n",
      "Epoch 65, Batch 0, Loss: 0.020691649988293648\n",
      "Epoch 65, Batch 10, Loss: 0.02000366523861885\n",
      "Epoch 65, Batch 20, Loss: 0.027742812409996986\n",
      "Epoch 65, Batch 30, Loss: 0.023221541196107864\n",
      "Epoch 65, Batch 40, Loss: 0.025253433734178543\n",
      "Epoch 65, Batch 50, Loss: 0.021070823073387146\n",
      "Epoch 65, Batch 60, Loss: 0.02270645461976528\n",
      "Epoch 65, Batch 70, Loss: 0.024618729948997498\n",
      "Epoch 65, Batch 80, Loss: 0.018647998571395874\n",
      "Epoch 65, Batch 90, Loss: 0.020507989451289177\n",
      "Epoch 65, Batch 100, Loss: 0.027729202061891556\n",
      "Epoch 66, Batch 0, Loss: 0.02697008289396763\n",
      "Epoch 66, Batch 10, Loss: 0.025007829070091248\n",
      "Epoch 66, Batch 20, Loss: 0.023028206080198288\n",
      "Epoch 66, Batch 30, Loss: 0.020665768533945084\n",
      "Epoch 66, Batch 40, Loss: 0.025531183928251266\n",
      "Epoch 66, Batch 50, Loss: 0.028753552585840225\n",
      "Epoch 66, Batch 60, Loss: 0.028834078460931778\n",
      "Epoch 66, Batch 70, Loss: 0.02781734988093376\n",
      "Epoch 66, Batch 80, Loss: 0.0249351616948843\n",
      "Epoch 66, Batch 90, Loss: 0.015855863690376282\n",
      "Epoch 66, Batch 100, Loss: 0.024232886731624603\n",
      "Epoch 67, Batch 0, Loss: 0.022758515551686287\n",
      "Epoch 67, Batch 10, Loss: 0.027117948979139328\n",
      "Epoch 67, Batch 20, Loss: 0.027611877769231796\n",
      "Epoch 67, Batch 30, Loss: 0.026671327650547028\n",
      "Epoch 67, Batch 40, Loss: 0.030665474012494087\n",
      "Epoch 67, Batch 50, Loss: 0.022712744772434235\n",
      "Epoch 67, Batch 60, Loss: 0.022920796647667885\n",
      "Epoch 67, Batch 70, Loss: 0.025758711621165276\n",
      "Epoch 67, Batch 80, Loss: 0.023099549114704132\n",
      "Epoch 67, Batch 90, Loss: 0.021934106945991516\n",
      "Epoch 67, Batch 100, Loss: 0.022204207256436348\n",
      "Epoch 68, Batch 0, Loss: 0.020078251138329506\n",
      "Epoch 68, Batch 10, Loss: 0.025246931239962578\n",
      "Epoch 68, Batch 20, Loss: 0.02528292126953602\n",
      "Epoch 68, Batch 30, Loss: 0.020005177706480026\n",
      "Epoch 68, Batch 40, Loss: 0.02315470017492771\n",
      "Epoch 68, Batch 50, Loss: 0.024544626474380493\n",
      "Epoch 68, Batch 60, Loss: 0.026466790586709976\n",
      "Epoch 68, Batch 70, Loss: 0.03450879454612732\n",
      "Epoch 68, Batch 80, Loss: 0.03056381084024906\n",
      "Epoch 68, Batch 90, Loss: 0.018152428790926933\n",
      "Epoch 68, Batch 100, Loss: 0.02419014647603035\n",
      "Epoch 69, Batch 0, Loss: 0.02522905543446541\n",
      "Epoch 69, Batch 10, Loss: 0.022226573899388313\n",
      "Epoch 69, Batch 20, Loss: 0.018575483933091164\n",
      "Epoch 69, Batch 30, Loss: 0.019993042573332787\n",
      "Epoch 69, Batch 40, Loss: 0.01948370411992073\n",
      "Epoch 69, Batch 50, Loss: 0.024145301431417465\n",
      "Epoch 69, Batch 60, Loss: 0.021179644390940666\n",
      "Epoch 69, Batch 70, Loss: 0.03022588975727558\n",
      "Epoch 69, Batch 80, Loss: 0.02207755111157894\n",
      "Epoch 69, Batch 90, Loss: 0.02537728101015091\n",
      "Epoch 69, Batch 100, Loss: 0.029075903818011284\n",
      "Epoch 70, Batch 0, Loss: 0.02682337909936905\n",
      "Epoch 70, Batch 10, Loss: 0.026698501780629158\n",
      "Epoch 70, Batch 20, Loss: 0.028667692095041275\n",
      "Epoch 70, Batch 30, Loss: 0.02559344656765461\n",
      "Epoch 70, Batch 40, Loss: 0.027042893692851067\n",
      "Epoch 70, Batch 50, Loss: 0.027627188712358475\n",
      "Epoch 70, Batch 60, Loss: 0.019702693447470665\n",
      "Epoch 70, Batch 70, Loss: 0.02220340073108673\n",
      "Epoch 70, Batch 80, Loss: 0.017158709466457367\n",
      "Epoch 70, Batch 90, Loss: 0.032499391585588455\n",
      "Epoch 70, Batch 100, Loss: 0.023980211466550827\n",
      "Epoch 71, Batch 0, Loss: 0.0185953788459301\n",
      "Epoch 71, Batch 10, Loss: 0.022696562111377716\n",
      "Epoch 71, Batch 20, Loss: 0.029954561963677406\n",
      "Epoch 71, Batch 30, Loss: 0.025805458426475525\n",
      "Epoch 71, Batch 40, Loss: 0.018434571102261543\n",
      "Epoch 71, Batch 50, Loss: 0.022297721356153488\n",
      "Epoch 71, Batch 60, Loss: 0.029527317732572556\n",
      "Epoch 71, Batch 70, Loss: 0.017110217362642288\n",
      "Epoch 71, Batch 80, Loss: 0.02843450754880905\n",
      "Epoch 71, Batch 90, Loss: 0.024553196504712105\n",
      "Epoch 71, Batch 100, Loss: 0.024272626265883446\n",
      "Epoch 72, Batch 0, Loss: 0.017319776117801666\n",
      "Epoch 72, Batch 10, Loss: 0.035930752754211426\n",
      "Epoch 72, Batch 20, Loss: 0.02692914567887783\n",
      "Epoch 72, Batch 30, Loss: 0.022626714780926704\n",
      "Epoch 72, Batch 40, Loss: 0.02571577951312065\n",
      "Epoch 72, Batch 50, Loss: 0.03339097648859024\n",
      "Epoch 72, Batch 60, Loss: 0.029697222635149956\n",
      "Epoch 72, Batch 70, Loss: 0.021086975932121277\n",
      "Epoch 72, Batch 80, Loss: 0.031955912709236145\n",
      "Epoch 72, Batch 90, Loss: 0.024626167491078377\n",
      "Epoch 72, Batch 100, Loss: 0.027741486206650734\n",
      "Epoch 73, Batch 0, Loss: 0.02059607394039631\n",
      "Epoch 73, Batch 10, Loss: 0.027992727234959602\n",
      "Epoch 73, Batch 20, Loss: 0.021549275144934654\n",
      "Epoch 73, Batch 30, Loss: 0.022136038169264793\n",
      "Epoch 73, Batch 40, Loss: 0.025582974776625633\n",
      "Epoch 73, Batch 50, Loss: 0.024311700835824013\n",
      "Epoch 73, Batch 60, Loss: 0.02225586026906967\n",
      "Epoch 73, Batch 70, Loss: 0.030757375061511993\n",
      "Epoch 73, Batch 80, Loss: 0.02090592123568058\n",
      "Epoch 73, Batch 90, Loss: 0.020376116037368774\n",
      "Epoch 73, Batch 100, Loss: 0.029644779860973358\n",
      "Epoch 74, Batch 0, Loss: 0.023517141118645668\n",
      "Epoch 74, Batch 10, Loss: 0.025473395362496376\n",
      "Epoch 74, Batch 20, Loss: 0.030837219208478928\n",
      "Epoch 74, Batch 30, Loss: 0.0209125317633152\n",
      "Epoch 74, Batch 40, Loss: 0.025831134989857674\n",
      "Epoch 74, Batch 50, Loss: 0.03556236997246742\n",
      "Epoch 74, Batch 60, Loss: 0.027743663638830185\n",
      "Epoch 74, Batch 70, Loss: 0.02458898350596428\n",
      "Epoch 74, Batch 80, Loss: 0.023637188598513603\n",
      "Epoch 74, Batch 90, Loss: 0.022075247019529343\n",
      "Epoch 74, Batch 100, Loss: 0.02140958234667778\n",
      "Epoch 75, Batch 0, Loss: 0.021414291113615036\n",
      "Epoch 75, Batch 10, Loss: 0.023088717833161354\n",
      "Epoch 75, Batch 20, Loss: 0.025606831535696983\n",
      "Epoch 75, Batch 30, Loss: 0.0216766856610775\n",
      "Epoch 75, Batch 40, Loss: 0.020223895087838173\n",
      "Epoch 75, Batch 50, Loss: 0.025564320385456085\n",
      "Epoch 75, Batch 60, Loss: 0.028109336271882057\n",
      "Epoch 75, Batch 70, Loss: 0.024020403623580933\n",
      "Epoch 75, Batch 80, Loss: 0.019416121765971184\n",
      "Epoch 75, Batch 90, Loss: 0.022881973534822464\n",
      "Epoch 75, Batch 100, Loss: 0.03192486613988876\n",
      "Epoch 76, Batch 0, Loss: 0.021826764568686485\n",
      "Epoch 76, Batch 10, Loss: 0.02194352261722088\n",
      "Epoch 76, Batch 20, Loss: 0.02468058466911316\n",
      "Epoch 76, Batch 30, Loss: 0.02904086373746395\n",
      "Epoch 76, Batch 40, Loss: 0.020727910101413727\n",
      "Epoch 76, Batch 50, Loss: 0.02338520810008049\n",
      "Epoch 76, Batch 60, Loss: 0.02817564085125923\n",
      "Epoch 76, Batch 70, Loss: 0.021838197484612465\n",
      "Epoch 76, Batch 80, Loss: 0.022193171083927155\n",
      "Epoch 76, Batch 90, Loss: 0.01729753240942955\n",
      "Epoch 76, Batch 100, Loss: 0.02112499624490738\n",
      "Epoch 77, Batch 0, Loss: 0.02920473739504814\n",
      "Epoch 77, Batch 10, Loss: 0.032483987510204315\n",
      "Epoch 77, Batch 20, Loss: 0.023358767852187157\n",
      "Epoch 77, Batch 30, Loss: 0.022876765578985214\n",
      "Epoch 77, Batch 40, Loss: 0.02061666175723076\n",
      "Epoch 77, Batch 50, Loss: 0.025447221472859383\n",
      "Epoch 77, Batch 60, Loss: 0.02729937620460987\n",
      "Epoch 77, Batch 70, Loss: 0.022710060700774193\n",
      "Epoch 77, Batch 80, Loss: 0.033769406378269196\n",
      "Epoch 77, Batch 90, Loss: 0.021917548030614853\n",
      "Epoch 77, Batch 100, Loss: 0.026286393404006958\n",
      "Epoch 78, Batch 0, Loss: 0.016972700133919716\n",
      "Epoch 78, Batch 10, Loss: 0.02419818565249443\n",
      "Epoch 78, Batch 20, Loss: 0.02537676692008972\n",
      "Epoch 78, Batch 30, Loss: 0.027671312913298607\n",
      "Epoch 78, Batch 40, Loss: 0.024885015562176704\n",
      "Epoch 78, Batch 50, Loss: 0.028235051780939102\n",
      "Epoch 78, Batch 60, Loss: 0.031040864065289497\n",
      "Epoch 78, Batch 70, Loss: 0.025743845850229263\n",
      "Epoch 78, Batch 80, Loss: 0.02755402773618698\n",
      "Epoch 78, Batch 90, Loss: 0.027898520231246948\n",
      "Epoch 78, Batch 100, Loss: 0.02107429876923561\n",
      "Epoch 79, Batch 0, Loss: 0.018034279346466064\n",
      "Epoch 79, Batch 10, Loss: 0.027714582160115242\n",
      "Epoch 79, Batch 20, Loss: 0.02589961141347885\n",
      "Epoch 79, Batch 30, Loss: 0.02001904882490635\n",
      "Epoch 79, Batch 40, Loss: 0.01958771049976349\n",
      "Epoch 79, Batch 50, Loss: 0.02589176408946514\n",
      "Epoch 79, Batch 60, Loss: 0.02208482287824154\n",
      "Epoch 79, Batch 70, Loss: 0.02596832439303398\n",
      "Epoch 79, Batch 80, Loss: 0.02535528875887394\n",
      "Epoch 79, Batch 90, Loss: 0.02270578406751156\n",
      "Epoch 79, Batch 100, Loss: 0.024406673386693\n",
      "Epoch 80, Batch 0, Loss: 0.03023126907646656\n",
      "Epoch 80, Batch 10, Loss: 0.02335681952536106\n",
      "Epoch 80, Batch 20, Loss: 0.022000810131430626\n",
      "Epoch 80, Batch 30, Loss: 0.03895873576402664\n",
      "Epoch 80, Batch 40, Loss: 0.02536299079656601\n",
      "Epoch 80, Batch 50, Loss: 0.025576140731573105\n",
      "Epoch 80, Batch 60, Loss: 0.02108234539628029\n",
      "Epoch 80, Batch 70, Loss: 0.025076834484934807\n",
      "Epoch 80, Batch 80, Loss: 0.02728215418756008\n",
      "Epoch 80, Batch 90, Loss: 0.024081028997898102\n",
      "Epoch 80, Batch 100, Loss: 0.021735606715083122\n",
      "Epoch 81, Batch 0, Loss: 0.025659693405032158\n",
      "Epoch 81, Batch 10, Loss: 0.02388937771320343\n",
      "Epoch 81, Batch 20, Loss: 0.020701570436358452\n",
      "Epoch 81, Batch 30, Loss: 0.025785736739635468\n",
      "Epoch 81, Batch 40, Loss: 0.03770805895328522\n",
      "Epoch 81, Batch 50, Loss: 0.02129860781133175\n",
      "Epoch 81, Batch 60, Loss: 0.02129342034459114\n",
      "Epoch 81, Batch 70, Loss: 0.026369508355855942\n",
      "Epoch 81, Batch 80, Loss: 0.02428818680346012\n",
      "Epoch 81, Batch 90, Loss: 0.033551983535289764\n",
      "Epoch 81, Batch 100, Loss: 0.027541056275367737\n",
      "Epoch 82, Batch 0, Loss: 0.02155045047402382\n",
      "Epoch 82, Batch 10, Loss: 0.01949675753712654\n",
      "Epoch 82, Batch 20, Loss: 0.02084829844534397\n",
      "Epoch 82, Batch 30, Loss: 0.019462652504444122\n",
      "Epoch 82, Batch 40, Loss: 0.021200573071837425\n",
      "Epoch 82, Batch 50, Loss: 0.02805253118276596\n",
      "Epoch 82, Batch 60, Loss: 0.01882382296025753\n",
      "Epoch 82, Batch 70, Loss: 0.025413844734430313\n",
      "Epoch 82, Batch 80, Loss: 0.024534812197089195\n",
      "Epoch 82, Batch 90, Loss: 0.023776747286319733\n",
      "Epoch 82, Batch 100, Loss: 0.02704562619328499\n",
      "Epoch 83, Batch 0, Loss: 0.03580894321203232\n",
      "Epoch 83, Batch 10, Loss: 0.03143085166811943\n",
      "Epoch 83, Batch 20, Loss: 0.02378685027360916\n",
      "Epoch 83, Batch 30, Loss: 0.022431958466768265\n",
      "Epoch 83, Batch 40, Loss: 0.03000362031161785\n",
      "Epoch 83, Batch 50, Loss: 0.023976370692253113\n",
      "Epoch 83, Batch 60, Loss: 0.022648632526397705\n",
      "Epoch 83, Batch 70, Loss: 0.027227893471717834\n",
      "Epoch 83, Batch 80, Loss: 0.02159683033823967\n",
      "Epoch 83, Batch 90, Loss: 0.02513512223958969\n",
      "Epoch 83, Batch 100, Loss: 0.02896450273692608\n",
      "Epoch 84, Batch 0, Loss: 0.02886226214468479\n",
      "Epoch 84, Batch 10, Loss: 0.025216374546289444\n",
      "Epoch 84, Batch 20, Loss: 0.02125682681798935\n",
      "Epoch 84, Batch 30, Loss: 0.022266371175646782\n",
      "Epoch 84, Batch 40, Loss: 0.02930465154349804\n",
      "Epoch 84, Batch 50, Loss: 0.02499961666762829\n",
      "Epoch 84, Batch 60, Loss: 0.026089990511536598\n",
      "Epoch 84, Batch 70, Loss: 0.02549658715724945\n",
      "Epoch 84, Batch 80, Loss: 0.02838130295276642\n",
      "Epoch 84, Batch 90, Loss: 0.03070804849267006\n",
      "Epoch 84, Batch 100, Loss: 0.018009481951594353\n",
      "Epoch 85, Batch 0, Loss: 0.023341147229075432\n",
      "Epoch 85, Batch 10, Loss: 0.020817380398511887\n",
      "Epoch 85, Batch 20, Loss: 0.02410985715687275\n",
      "Epoch 85, Batch 30, Loss: 0.024788130074739456\n",
      "Epoch 85, Batch 40, Loss: 0.01879987306892872\n",
      "Epoch 85, Batch 50, Loss: 0.019423549994826317\n",
      "Epoch 85, Batch 60, Loss: 0.01859092526137829\n",
      "Epoch 85, Batch 70, Loss: 0.021197322756052017\n",
      "Epoch 85, Batch 80, Loss: 0.027150021865963936\n",
      "Epoch 85, Batch 90, Loss: 0.021039124578237534\n",
      "Epoch 85, Batch 100, Loss: 0.02670145593583584\n",
      "Epoch 86, Batch 0, Loss: 0.01937164179980755\n",
      "Epoch 86, Batch 10, Loss: 0.022963697090744972\n",
      "Epoch 86, Batch 20, Loss: 0.022782379761338234\n",
      "Epoch 86, Batch 30, Loss: 0.025745032355189323\n",
      "Epoch 86, Batch 40, Loss: 0.014295807108283043\n",
      "Epoch 86, Batch 50, Loss: 0.02260434255003929\n",
      "Epoch 86, Batch 60, Loss: 0.019243011251091957\n",
      "Epoch 86, Batch 70, Loss: 0.024020420387387276\n",
      "Epoch 86, Batch 80, Loss: 0.027546662837266922\n",
      "Epoch 86, Batch 90, Loss: 0.03045636974275112\n",
      "Epoch 86, Batch 100, Loss: 0.027823075652122498\n",
      "Epoch 87, Batch 0, Loss: 0.02475018799304962\n",
      "Epoch 87, Batch 10, Loss: 0.022453490644693375\n",
      "Epoch 87, Batch 20, Loss: 0.038644012063741684\n",
      "Epoch 87, Batch 30, Loss: 0.029748061671853065\n",
      "Epoch 87, Batch 40, Loss: 0.024506809189915657\n",
      "Epoch 87, Batch 50, Loss: 0.025003351271152496\n",
      "Epoch 87, Batch 60, Loss: 0.02404274046421051\n",
      "Epoch 87, Batch 70, Loss: 0.0241398848593235\n",
      "Epoch 87, Batch 80, Loss: 0.021780813112854958\n",
      "Epoch 87, Batch 90, Loss: 0.021748336032032967\n",
      "Epoch 87, Batch 100, Loss: 0.02174491062760353\n",
      "Epoch 88, Batch 0, Loss: 0.02487458847463131\n",
      "Epoch 88, Batch 10, Loss: 0.018950220197439194\n",
      "Epoch 88, Batch 20, Loss: 0.021359581500291824\n",
      "Epoch 88, Batch 30, Loss: 0.025937190279364586\n",
      "Epoch 88, Batch 40, Loss: 0.026851410046219826\n",
      "Epoch 88, Batch 50, Loss: 0.026405004784464836\n",
      "Epoch 88, Batch 60, Loss: 0.027984218671917915\n",
      "Epoch 88, Batch 70, Loss: 0.019032591953873634\n",
      "Epoch 88, Batch 80, Loss: 0.024782324209809303\n",
      "Epoch 88, Batch 90, Loss: 0.021650301292538643\n",
      "Epoch 88, Batch 100, Loss: 0.017785942181944847\n",
      "Epoch 89, Batch 0, Loss: 0.02264558896422386\n",
      "Epoch 89, Batch 10, Loss: 0.017216475680470467\n",
      "Epoch 89, Batch 20, Loss: 0.032217033207416534\n",
      "Epoch 89, Batch 30, Loss: 0.025023961439728737\n",
      "Epoch 89, Batch 40, Loss: 0.02715069055557251\n",
      "Epoch 89, Batch 50, Loss: 0.03324630483984947\n",
      "Epoch 89, Batch 60, Loss: 0.01967877894639969\n",
      "Epoch 89, Batch 70, Loss: 0.02580445073544979\n",
      "Epoch 89, Batch 80, Loss: 0.025631079450249672\n",
      "Epoch 89, Batch 90, Loss: 0.02321883849799633\n",
      "Epoch 89, Batch 100, Loss: 0.024191496893763542\n",
      "Epoch 90, Batch 0, Loss: 0.023273319005966187\n",
      "Epoch 90, Batch 10, Loss: 0.023010578006505966\n",
      "Epoch 90, Batch 20, Loss: 0.017123349010944366\n",
      "Epoch 90, Batch 30, Loss: 0.02750633843243122\n",
      "Epoch 90, Batch 40, Loss: 0.0233711376786232\n",
      "Epoch 90, Batch 50, Loss: 0.031324636191129684\n",
      "Epoch 90, Batch 60, Loss: 0.018227536231279373\n",
      "Epoch 90, Batch 70, Loss: 0.030797556042671204\n",
      "Epoch 90, Batch 80, Loss: 0.02033311314880848\n",
      "Epoch 90, Batch 90, Loss: 0.02343526855111122\n",
      "Epoch 90, Batch 100, Loss: 0.025111239403486252\n",
      "Epoch 91, Batch 0, Loss: 0.02125268243253231\n",
      "Epoch 91, Batch 10, Loss: 0.022661389783024788\n",
      "Epoch 91, Batch 20, Loss: 0.03320453315973282\n",
      "Epoch 91, Batch 30, Loss: 0.01875065453350544\n",
      "Epoch 91, Batch 40, Loss: 0.030587462708353996\n",
      "Epoch 91, Batch 50, Loss: 0.023633792996406555\n",
      "Epoch 91, Batch 60, Loss: 0.028009874746203423\n",
      "Epoch 91, Batch 70, Loss: 0.022535577416419983\n",
      "Epoch 91, Batch 80, Loss: 0.02213476039469242\n",
      "Epoch 91, Batch 90, Loss: 0.03161397576332092\n",
      "Epoch 91, Batch 100, Loss: 0.02008805051445961\n",
      "Epoch 92, Batch 0, Loss: 0.020820461213588715\n",
      "Epoch 92, Batch 10, Loss: 0.020658310502767563\n",
      "Epoch 92, Batch 20, Loss: 0.022664152085781097\n",
      "Epoch 92, Batch 30, Loss: 0.02034010738134384\n",
      "Epoch 92, Batch 40, Loss: 0.022480079904198647\n",
      "Epoch 92, Batch 50, Loss: 0.029040904715657234\n",
      "Epoch 92, Batch 60, Loss: 0.016147881746292114\n",
      "Epoch 92, Batch 70, Loss: 0.024822067469358444\n",
      "Epoch 92, Batch 80, Loss: 0.019937394186854362\n",
      "Epoch 92, Batch 90, Loss: 0.02162124030292034\n",
      "Epoch 92, Batch 100, Loss: 0.016936833038926125\n",
      "Epoch 93, Batch 0, Loss: 0.02453450858592987\n",
      "Epoch 93, Batch 10, Loss: 0.02490391954779625\n",
      "Epoch 93, Batch 20, Loss: 0.027758298441767693\n",
      "Epoch 93, Batch 30, Loss: 0.020526206120848656\n",
      "Epoch 93, Batch 40, Loss: 0.02462100051343441\n",
      "Epoch 93, Batch 50, Loss: 0.025767888873815536\n",
      "Epoch 93, Batch 60, Loss: 0.020214056596159935\n",
      "Epoch 93, Batch 70, Loss: 0.03591872379183769\n",
      "Epoch 93, Batch 80, Loss: 0.025820964947342873\n",
      "Epoch 93, Batch 90, Loss: 0.02568298578262329\n",
      "Epoch 93, Batch 100, Loss: 0.0204642154276371\n",
      "Epoch 94, Batch 0, Loss: 0.02189934253692627\n",
      "Epoch 94, Batch 10, Loss: 0.02256416156888008\n",
      "Epoch 94, Batch 20, Loss: 0.020410247147083282\n",
      "Epoch 94, Batch 30, Loss: 0.016881240531802177\n",
      "Epoch 94, Batch 40, Loss: 0.019278310239315033\n",
      "Epoch 94, Batch 50, Loss: 0.022784382104873657\n",
      "Epoch 94, Batch 60, Loss: 0.029324090108275414\n",
      "Epoch 94, Batch 70, Loss: 0.024094833061099052\n",
      "Epoch 94, Batch 80, Loss: 0.019736705347895622\n",
      "Epoch 94, Batch 90, Loss: 0.03079087845981121\n",
      "Epoch 94, Batch 100, Loss: 0.024683281779289246\n",
      "Epoch 95, Batch 0, Loss: 0.029856346547603607\n",
      "Epoch 95, Batch 10, Loss: 0.028957460075616837\n",
      "Epoch 95, Batch 20, Loss: 0.02542647160589695\n",
      "Epoch 95, Batch 30, Loss: 0.022620001807808876\n",
      "Epoch 95, Batch 40, Loss: 0.033778995275497437\n",
      "Epoch 95, Batch 50, Loss: 0.02085462212562561\n",
      "Epoch 95, Batch 60, Loss: 0.018643125891685486\n",
      "Epoch 95, Batch 70, Loss: 0.024356627836823463\n",
      "Epoch 95, Batch 80, Loss: 0.023536773398518562\n",
      "Epoch 95, Batch 90, Loss: 0.02446899004280567\n",
      "Epoch 95, Batch 100, Loss: 0.024213511496782303\n",
      "Epoch 96, Batch 0, Loss: 0.02015729248523712\n",
      "Epoch 96, Batch 10, Loss: 0.02676941081881523\n",
      "Epoch 96, Batch 20, Loss: 0.021142009645700455\n",
      "Epoch 96, Batch 30, Loss: 0.020928608253598213\n",
      "Epoch 96, Batch 40, Loss: 0.02230571024119854\n",
      "Epoch 96, Batch 50, Loss: 0.017720995470881462\n",
      "Epoch 96, Batch 60, Loss: 0.023434216156601906\n",
      "Epoch 96, Batch 70, Loss: 0.021153822541236877\n",
      "Epoch 96, Batch 80, Loss: 0.023836124688386917\n",
      "Epoch 96, Batch 90, Loss: 0.021226240321993828\n",
      "Epoch 96, Batch 100, Loss: 0.03160451352596283\n",
      "Epoch 97, Batch 0, Loss: 0.028449293226003647\n",
      "Epoch 97, Batch 10, Loss: 0.024319104850292206\n",
      "Epoch 97, Batch 20, Loss: 0.024248741567134857\n",
      "Epoch 97, Batch 30, Loss: 0.02336334064602852\n",
      "Epoch 97, Batch 40, Loss: 0.02024839259684086\n",
      "Epoch 97, Batch 50, Loss: 0.02219690941274166\n",
      "Epoch 97, Batch 60, Loss: 0.024106081575155258\n",
      "Epoch 97, Batch 70, Loss: 0.022507131099700928\n",
      "Epoch 97, Batch 80, Loss: 0.022452564910054207\n",
      "Epoch 97, Batch 90, Loss: 0.02234680950641632\n",
      "Epoch 97, Batch 100, Loss: 0.019282523542642593\n",
      "Epoch 98, Batch 0, Loss: 0.018983742222189903\n",
      "Epoch 98, Batch 10, Loss: 0.019761087372899055\n",
      "Epoch 98, Batch 20, Loss: 0.018348196521401405\n",
      "Epoch 98, Batch 30, Loss: 0.021129194647073746\n",
      "Epoch 98, Batch 40, Loss: 0.028213413432240486\n",
      "Epoch 98, Batch 50, Loss: 0.026105916127562523\n",
      "Epoch 98, Batch 60, Loss: 0.03672463819384575\n",
      "Epoch 98, Batch 70, Loss: 0.02050325646996498\n",
      "Epoch 98, Batch 80, Loss: 0.025297746062278748\n",
      "Epoch 98, Batch 90, Loss: 0.02910572662949562\n",
      "Epoch 98, Batch 100, Loss: 0.02730688825249672\n",
      "Epoch 99, Batch 0, Loss: 0.02613135054707527\n",
      "Epoch 99, Batch 10, Loss: 0.02348722144961357\n",
      "Epoch 99, Batch 20, Loss: 0.0215245820581913\n",
      "Epoch 99, Batch 30, Loss: 0.02160884067416191\n",
      "Epoch 99, Batch 40, Loss: 0.03136057406663895\n",
      "Epoch 99, Batch 50, Loss: 0.018301431089639664\n",
      "Epoch 99, Batch 60, Loss: 0.025447659194469452\n",
      "Epoch 99, Batch 70, Loss: 0.021206218749284744\n",
      "Epoch 99, Batch 80, Loss: 0.02536114677786827\n",
      "Epoch 99, Batch 90, Loss: 0.022030841559171677\n",
      "Epoch 99, Batch 100, Loss: 0.02454439550638199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_BILSTM(\n",
       "  (cnn): CNN(\n",
       "    (conv1d): Conv1d(8, 32, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (tanh): Tanh()\n",
       "    (maxpool1d): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (bilstm): BILSTM(\n",
       "    (lstm): LSTM(32, 64, batch_first=True, bidirectional=True)\n",
       "    (fc1): Linear(in_features=128, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(True)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_index, (X, y_true) in enumerate(train_loader):\n",
    "        X, y_true = X.to(), y_true.to()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
    "            # Assuming you want to clear the output in a Jupyter notebook to avoid clutter\n",
    "            # You would uncomment the next line in a Jupyter notebook environment\n",
    "            # display.clear_output(wait=True)\n",
    "\n",
    "model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.041022; Test RMSE 38.856445\n",
      "\n",
      "Train  MAE: 0.023681; Test  MAE 27.215050\n",
      "\n",
      "Train  R^2: 0.998321; Test  R^2 0.965925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train.to()).to()\n",
    "    y_test_pred = model(X_test.to()).to()\n",
    "\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_pred, y_train, squared=False)\n",
    "test_mse = mean_squared_error(y_test_pred, y_test, squared=False)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_pred, y_train)\n",
    "test_mae = mean_absolute_error(y_test_pred, y_test)\n",
    "\n",
    "train_r2 = r2_score(y_train_pred, y_train)\n",
    "test_r2 = r2_score(y_test_pred, y_test)\n",
    "\n",
    "print(\"Train RMSE: {0:.6f}; Test RMSE {1:.6f}\\n\".format(train_mse, test_mse))\n",
    "print(\"Train  MAE: {0:.6f}; Test  MAE {1:.6f}\\n\".format(train_mae, test_mae))\n",
    "print(\"Train  R^2: {0:.6f}; Test  R^2 {1:.6f}\\n\".format(train_r2, test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
