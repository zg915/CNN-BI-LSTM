{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import akshare as ak \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_index_df = ak.index_zh_a_hist(symbol=\"000001\", period=\"daily\", start_date=\"19910701\", end_date=\"20200831\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns\n",
    "sz_index_df = sz_index_df.drop(['振幅', '换手率'],axis=1)\n",
    "# Then reorder the remaining columns\n",
    "sz_index_df = sz_index_df[['日期', '开盘', '最高','最低','收盘','成交量','成交额','涨跌额','涨跌幅']]\n",
    "# Directly assign the new column names\n",
    "sz_index_df.columns = ['date','Opening price', 'Highest price', 'Lowest price','Closing price', 'Volume(share)', 'Turnover(RMB)', 'Ups and downs', 'Change(%)']\n",
    "# volumn*100\n",
    "sz_index_df['Volume(share)']=sz_index_df['Volume(share)']*100\n",
    "# Set date as index\n",
    "sz_index_df = sz_index_df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "split_point = len(sz_index_df) - 500\n",
    "train_df = sz_index_df.iloc[:split_point]\n",
    "test_df = sz_index_df.iloc[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df)\n",
    "train_set_standardized = scaler.transform(train_df)\n",
    "test_set_standardized = scaler.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_and_targets(data, time_steps=10, n_features=8):\n",
    "    \"\"\"\n",
    "    Create sequences of time_steps length and corresponding targets from standardized data.\n",
    "\n",
    "    Parameters:\n",
    "    data (array-like): Input two-dimensional standardized data (num_samples, num_features).\n",
    "    time_steps (int): The size of the time step sequences.\n",
    "    n_features (int): The number of features in each time step.\n",
    "\n",
    "    Returns:\n",
    "    X, y: Tuple of numpy arrays\n",
    "          X is three-dimensional data of shape (None, time_steps, n_features) for the CNN-LSTM input.\n",
    "          y is a one-dimensional array of targets, which are the next values following each sequence.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps, :n_features])\n",
    "        y.append(data[i + time_steps, :n_features])  # Assuming we're predicting the next full feature set\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Assuming 'train_set_standardized' is your standardized training data as a two-dimensional NumPy array.\n",
    "# Now let's transform 'train_set_standardized' into the required three-dimensional shape and get the targets.\n",
    "train_sequences, train_targets = create_sequences_and_targets(train_set_standardized)\n",
    "test_sequences, test_targets = create_sequences_and_targets(test_set_standardized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
